x# Streetlights and Shadows_ Searching for the Keys to Adaptive Decision Making (MIT Press) - Gary Klein
 [[Decision-making]] 
---
Lesezeichen [[#^5efaa4]]

[[Klein, Gary - Streetlights and shadows_ searching for the keys to adaptive decision making.pdf]]


# Streetlights and Shadows


[[20211041526 Dreyfus Model of Expertise]]


P5: 
> We can’t treat every situation as an emergency; that’s why we depend on standard strategies to let us reach our everyday goals. However, we can become vulnerable if we are too rigid, too locked into our routines to adjust to changing conditions. **We need both mental “gears”: one for using the standard procedures and the other for improvising when situations become unsettled. **

P5
> Our eyes are built for two perspectives. During the daytime we rely on our cone cells, which depend on lots of light and let us see details. At night the cone cells become useless and we depend on rod cells, which are much more sensitive. The rod cells in our eyes are connected together to detect stray light; as a result they don’t register fine details. If we want to see something in bright light, we focus the image on the center of our retina (the fovea), where the cone cells are tightly packed. To see something at night, we must look off to the side of it, because staring directly at it will focus the object on the useless cone cells in the fovea.

---

Notebookexport

Streetlights and Shadows: Searching for the Keys to Adaptive Decision Making (English Edition)

Klein, Gary A.

---
___

## 1   Ten Surprises about How We Handle Ambiguous Situations

### Markierung (gelb) - Seite 5 · Position 265

we rely on our cone cells , which depend on lots of light and let us see details . At night the cone cells become useless and we depend on rod cells , which are much more sensitive . The rod cells in our eyes are connected together to detect stray light ; as a result they don’t register fine details . If we want to see something in bright light , we focus the image on the center of our retina ( the fovea ) , where the cone cells are tightly packed . To see something

### Markierung (gelb) - Seite 12 · Position 397
- make progress when we find regularities in situations that appeared to be highly complex . We should encourage those researchers who look for order in complex situations . Many hygiene and public health procedures are examples of initially complex domains which , after painstaking study , analysis , data gathering , and assessments , evolved over many years to orderly understandings . SARS was complex and initially required complex responses , but over time we have figured out how it works and now have a repertoire of very structured responses to it .
	- [[You have to Look at the Primary Data to spot patterns]]

### Notiz - Seite 12 · Position 401

Rki corona was wenn noch keine daten verfügbar

## Part I   Making Decisions

### Markierung (orange) - 2   A Passion for Procedures > Seite 17 · Position 459

Instructional Systems Design ,

### Markierung (gelb) - 2   A Passion for Procedures > Seite 17 · Position 461

1974 book The Inner Game of Tennis , Timothy Gallwey argued that in tennis following procedures is the opposite of skill . Instead of engaging in the sport , players worry about their form — whether their feet are too far apart , if one elbow is bent at the correct angle , and so forth . Those kinds of rules and procedures are more likely to interfere with performance than to improve it . Putting Gallwey’s ideas together with my discussion with Scotty and

### Markierung (gelb) - 2   A Passion for Procedures > Seite 17 · Position 467

Hubert and Stuart Dreyfus to provide an alternative view . Both of them were Berkeley professors , Bert in philosophy and Stuart in operations research .

### Markierung (gelb) - 2   A Passion for Procedures > Seite 17 · Position 469

- how people develop expertise . According to their model , novices are given simple procedures that don’t depend on context — on what else might be going on . Thus , beginning chess players might be taught numerical values of the pieces and advised to be careful not to lose exchanges . For example , it’s a bad idea to trade a queen for a pawn . 6 Of course , these numerical values are fictions ; the real value of a chess piece depends on what is happening in a position , and will change as the game changes .
	- [[20211041526 Dreyfus Model of Expertise]]


- Dreyfus model of expertise emphasizes intuition and tacit knowledge that can’t be captured in rules and procedures . People might need some rules in order to get started , but they have to move beyond rules in order to achieve mastery . 7 Procedures , including checklists , are tools . Every tool has limitations , and I am not arguing that we should do away with procedures . For example , I admire Peter Pronovost , who advocated for



- accident , there is a good chance that “ procedural violation ” will be trumpeted as one of the contributing factors . I once participated in an aviation accident investigation . The flight data recordings showed pretty clearly that the pilots hadn’t done everything exactly by the book . The senior pilot next to me pointed out that pilots violate some procedures on almost every flight . There are so many procedures that pilots are bound to

### Markierung (gelb) - 2   A Passion for Procedures > Seite 22 · Position 557

- when turning the data into forecasts . They took the “ low road ” illustrated in figure 2.1 . In contrast , the highly skilled forecasters tried to understand what was going on . They foraged for data that helped them build a better understanding , and used their understanding to make predictions . When I presented figure 2.1 at the 21st American Meteorological Society Conference on Weather Analysis and Forecasting , in Washington , in 2005 , an executive from the national forecasting service Accuweather commented that his company was increasingly reliant on the procedures its forecasters needed to take the low road in figure 2.1 . The senior staff referred to these procedural guidelines as “ the great equalizer . ” They permitted the mediocre forecasters to just follow some rules and still put out adequate forecasts . But they tied the hands of the skilled meteorologists and degraded their performance to the point that they were just putting out adequate forecasts , no better than that . The procedures mandated which data to collect and what types of analyses
	- [[You have to Look at the Primary Data to spot patterns]]

### Markierung (gelb) - 2   A Passion for Procedures > Seite 23 · Position 576

- Research supports this idea of eroding expertise . A number of studies have shown that procedures help people handle typical tasks , but people do best in novel situations when they understand the system they need to control . 11 People taught to understand the system develop richter mental models than people taught to follow procedures
	- Morris, N., and Rouse, W. 1985. Review and Evaluation of Empirical Research in Troubleshooting. Human Factors 27: 503–530.
	- Patrick, J., and Haines, B. 1988. Training and Transfer of Fault-Finding Skill. Ergonomics 31: 193–210.
	- Kontogiannis, T. 2000. The Effect of Training Systemic Information on the Acquisition and Transfer of Fault-Finding Skills. International Journal of Cognitive Ergonomics 4: 243–267.
	- Hockey, G., Sauer, J., and Wastell, D. 2007. Adaptability of Training in Simulated Process Control: Knowledge Versus Rule-Based Guidance under Task Changes and Environmental Stress. Human Factors 49: 158–174.

### Markierung (gelb) - 2   A Passion for Procedures > Seite 28 · Position 655

 . For example , public officials in Taiwan grew frustrated by merchants who failed to pay sales taxes . The merchants handled cash transactions off the cash registers , leaving no trail for inspectors to follow . Instead of increasing penalties and warnings , Taiwan set up a lottery in which every entry was required to be accompanied by a sales slip . Suddenly , in that lottery - crazed country , people were demanding

### Markierung (orange) - 2   A Passion for Procedures > Seite 30 · Position 699

- To put procedures into perspective , consider the difference between directions and maps ( Vicente 2002 ) . When we have to travel to an unfamiliar destination , we sometimes get directions — a sequence of actions ( e.g . , turn right , go straight for two blocks , then turn left ) . Other times we get a map showing where we are , where we want to be , and the terrain in between . The directions are easier to follow , but if anything goes wrong ( say , a street is blocked off ) we are stuck . A map demands more of us but makes it easier for us to adapt and can be used for other routes in the same area . For many types of complex work we need both procedures and the judgment to interpret and work around the procedures . Hockey , Sauer , and Wastell ( 2007 ) used a laboratory process control task to compare the value of training rules and procedures against the value of training people to understand the system they had to control . As was expected , people trained to understand the way the system worked were more flexible , and did a better job of spotting and fixing unfamiliar and complex malfunctions , than people trained to follow rules and procedures . However , they also took longer to do the work , and they were more affected by a stressor — noise — than people who had merely been trained to follow procedures .
	-  Vicente, K. 2002. Work Domain Analysis and Task Analysis: A Difference That Matters. In Cognitive Task Analysis, ed. J. Schraagen et al. Erlbaum.
	-  [[#^a3a220]]
	-  [[Maps are more flexible than Directions]]

### Notiz - 2   A Passion for Procedures > Seite 30 · Position 704

\*\*Linux vs mac\*\*

### Markierung (gelb) - 2   A Passion for Procedures > Seite 31 · Position 709

Here is another way to teach procedures : Set up scenarios for various kinds of challenges and let the new workers go through the scenarios . If the procedures make sense , then workers should get to see what happens when they depart from the optimal procedures . When procedures are taught in a scenario format , people can appreciate why the procedures were put into place and can also gain a sense of the limitations of the procedures . This scenario format seems to work better than having people memorize the details of each step . The scenarios provide a good counterpoint for learning the steps of complicated tasks . Moreover , the scenarios can help people acquire some of the tacit knowledge they need in order to apply procedures effectively . ( The topic of tacit knowledge will be taken up in the next chapter . )

### Markierung (orange) - 3   Seeing the Invisible > Seite 33 · Position 732

- [[Tacit knowledge]]
	- is being able to do things without being able to explain how . We can’t learn tacit knowledge from a textbook . 2 We know more than we can tell . 3
	- Here I am paraphrasing Orr (1990), [[#^8b5d9b]]who stated that tacit knowledge is “the ability to do things without being able to explain them completely and also the inability to learn to do them from a theoretical understanding of the task.” 3. Polanyi 1967 [[#^6f0d17]].
	- The interplay between noticing typical cases and anomalous ones is a type of tacit knowledge found in many fields . For example , nurses in intensive - care units build up a sense of typicality that lets them notice when a patient looks atypical . The nurses become early warning systems to catch weak signals that a patient is starting to deteriorate .

### Markierung (gelb) - 3   Seeing the Invisible > Seite 44 · Position 935

- Mental models [^20] are the stories we construct to understand how things work . They mirror the events or system they are modeling , but they capture only a limited aspect of those events or that system . [^21] We form our mental models from the way we understand causes . [[202108070931 Mental Models MOC]]

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 49 · Position 999

^5efaa4

- The anchoring - and - adjustment heuristic When we have to make an estimate and we don’t know the answer , one strategy we use is to find a plausible answer and then adjust it up or down . This mental strategy is called **anchoring and adjustment** [[202108091333 Mental Models Lazyness#^ac3136]] [[201601041258 Information is Beatiful rhetological Fallacies cognitive Bias]]
	- [[Anchoring]]
	- Wann benötigt man solche Strategien? 
		- [[201608061827 Brainteaser im Bewerbungsgesprach - 140 Ub - Stefan Menden squeaker]]
	- [[Northcraft and Neale - 1987 - Experts, amateurs, and real estate An anchoring-a.pdf]]


# Ab hier wieder in Kindle File
### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 52 · Position 1053
- [[Framing Heuristic]]
	- When given the ‘ deny ’ frame , subjects looked for reasons to deny custody , and parent B had stronger reasons
	- As in the preceding example , the frame we use will affect which types of data we notice . We can influence people’s judgments just by the way we frame the problem .
	- “ Would you like to have me give a short talk about some of my findings ? ” But I was pretty sure he would have said no , because he wouldn’t have heard any reasons to grant my request . Instead , I asked “ Do you have any objection if I take a few minutes to describe some findings that bear directly on the issues we’ve been discussing ? ” He thought about it , he couldn’t come up with any strong reasons to turn me down ( we were slightly ahead of schedule ) , and he carved out a slot for me . It’s all in the framing .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 53 · Position 1070

- Representativeness heuristic
	- Tversky and Kahneman ( 1982 ) provided the classical demonstration of this heuristic : Linda is 31 years old , single , outspoken and very bright . She majored in philosophy . As a student , she was deeply concerned with issues of discrimination and social justice , and also participated in antinuclear demonstrations . Please check off the most likely alternative : — Linda is a bank teller . — Linda is a bank teller and is active in the feminist movement . In the original experiment , about 85 percent of the subjects picked the second alternative . But the first alternative is statistically more likely , because it includes the possibility that Linda is active in the feminist movement as well as the possibility that Linda is a bank teller but is not active in the feminist movement . Most subjects pick the second alternative because it seems like a fuller and more accurate description of Linda . It seems like a better representation of the kind of person Linda is . The pull of the second alternative shows the representativeness heuristic at work . Here it prevented subjects from judging which alternative is more likely .

### Notiz - 4   How Biased Is Our Thinking? > Seite 53 · Position 1079

Gigerenzer wird doch auch bein sönke ahrens erwähnt [[202109250929 Sönke Ahrens Das Zettelkastenprinzip]] gut feeling intuition 

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 55 · Position 1109

Fortunately , the fears of decision biases are overblown . The research doesn’t really demonstrate that we are irrational . Rather , we use effective strategies in our thinking but these have some limitations that researchers can exploit .[[Heuristic]]

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 60 · Position 1197


Instead, we learn to use something called the gaze heuristic. If you are running in at just the right speed, the angle of your gaze—directed at the ball—will stay constant.15 That’s a good feeling. If the angle of your gaze shifts upward, then you are running too fast, and you should slow down or back up. If the angle changes so that your gaze shifts downward, you had better speed up. You just need to find a speed that lets the angle of gaze remain constant.16 The gaze heuristic works very well. It takes into account wind, air resistance, and the spin of the ball. Outfielders don’t have to know where to go to catch the ball. They just have to maintain a constant angle of gaze in order to get to the right spot. And they don’t have to calculate anything—the heuristic does that for them. The gaze heuristic will lead the player to the point where the ball is going to land. 


The angle of gaze is the angle between the eye and the ball, relative to the ground. McLeod and Dienes (1996) performed the research demonstrating the gaze heuristic.  
[^16]. Also, players run slowly while they are trying to establish the gaze angle. Their coaches sometimes reprimand them for getting off to a slow start, not understanding that this is part of the strategy.

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 63 · Position 1258

Lovallo and Kahneman suggest a strategy of taking an “ outside view ” — that is , using previous projects to suggest how long tasks will take and how many resources they will consume .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 63 · Position 1261

PreMortem technique .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 66 · Position 1310

claim 2 matters Claim 2 ( that decision biases distort our thinking ) matters because the concept of bias encourages organizations to overreact to failures by enacting excessive procedures and counter - productive restrictions . Biases often get invoked whenever a decision turns out wrong . But preventing these “ biases ” would likely do more harm than good . It could eradicate

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 67 · Position 1322

1954 , **Paul Meehl** published Clinical vs . Statistical Prediction , a very influential book describing 18 studies that showed the limitations of human judgment . These studies compared the judgments of professionals against statistical rule - based predictions about parole violations , success in pilot training , and academic success . In each study , the professionals had access to the data used by the statistical procedures and to additional data that might not have been included in the algorithm . **Despite all their experience , the professionals outperformed the algorithms in only one of the 18 cases** . In a few other cases the professionals and the formulas gave similar results , but in most of the cases the statistical rules were superior to the expert judgments . In one example , academic counselors had access to all the data on a school’s incoming freshmen plus a 45 - minute interview of each one , and still were less accurate in predicting their first - year grades than a statistical analysis based only on the students ’ high school grades and their scores on a standardized test .

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 68 · Position 1341

Tetlock studied a set of 284 experts , most with PhDs and postgraduate training in fields such as political science , economics , international law and diplomacy , public policy , and journalism . Averaging 12 years of relevant work experience , they came from universities , think tanks , government service , and international institutions . **Tetlock’s criterion for experts was that they be professionals earning their living by commenting or offering advice on political and economic trends** . Their average age was 43 . 

**Approximately 61 percent had been interviewed by a major media outlet ; 21 percent had been interviewed more than ten times**
Tetlock collected most of his data between 1988 and 1992 . He presented these experts with sets of questions and asked them to rate the probabilities . The questions dealt with ( among other things ) the likelihood of various countries acquiring the ability to produce weapons of mass destruction ; the possibility of states or terrorist groups using such weapons ; predictions over 3 , 6 , or 12 years about economic reforms in the former Soviet Bloc countries ; adoption of the Euro ; the prospects of former Soviet Bloc countries and Turkey joining the European Union ; the winners of the American presidential elections in 1992 and 2000 and the margins of victory ; the performance of the NASDAQ ; the revenues , earnings , and share prices of Internet and information - technology companies such as CISCO , Oracle , Microsoft , Enron , and IBM ; and whether Quebec would secede from the rest of Canada . Tetlock waited a few years to see whether the world events actually occurred within the time frame in the questions . Then he compiled the actual results , to see how well the experts performed in comparison with a control — the performance of a hypothetical chimpanzee who simply assigned equal probabilities to the different events . The experts barely beat the hypothetical chimp . They were slightly more accurate than chance . It got worse . Tetlock had tested the experts with some questions from their fields of study and some from unfamiliar fields . The experts didn’t do any better on questions from their fields of study than questions on unfamiliar topics . ( They did outperform Berkeley undergraduates who received short descriptions of each topic of interest . ) When Tetlock confronted the experts with evidence for their inaccurate predictions , they were unfazed . They explained away the prediction failures rather than trying to improve their mental models . They showed the typical symptoms of fixation . Experts attributed their successes to their own skilled judgment , whereas they blamed their failures on bad luck or task difficulty .

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 72 · Position 1411

**To make predictions we have to do two things : collect the data and then combine the data** 

Some of the confusion about logic and statistics and intuition arises from blurring this distinction . Meehl and his followers haven’t shown that expert intuitions are useless . They have just shown that statistical formulas for combining the data can be more accurate than the estimates made by experts .

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 72 · Position 1416

Clinicians make these judgments from interviews and observations with prisoners , judging typicality in reference to hundreds of other interviews they have done , many with similar prisoners . **Clinicians use their judgment to identify variables that might be important** , something that mechanical systems aren’t particularly good at . Meehl never doubted the importance of intuition and expertise in making these judgments . 3

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 73 · Position 1442

Most of what differentiates skilled from unskilled chess players is their tacit knowledge , not their ability to calculate move quality . Systematic analysis is important in chess , but intuitive skills and tacit knowledge seem to be more critical . **As players get stronger , their ability to do deliberate search and analysis doesn’t seem to get any better** . 5,6 It is hard to reconcile such findings with the claim that successful decision makers rely on logic and statistics rather than on intuition .

[[202112091755 Quick choices often have a higher Quality than lengthy deliberations]]


### Markierung (orange) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 86 · Position 1680

**The more skilled one is , the fewer options one thinks about .** The only population I studied that compared multiple options for most decisions was the least experienced — tank platoon leaders in their initial training . Novices have to compare options because they don’t have the experience base to recognize what to do . However , comparing options doesn’t substitute for experience . 

[[Satisficing means picking the first option to exit the zone of indifference]]
**A final weakness in these analytical methods is the Zone of Indifference . 6 When one option is clearly better than the others , we need not do any analysis . We immediately know what to choose . The closer the options become , the more the strengths and weaknesses are balanced , the harder the choice . The hardest decisions are those that must be made when the options are just about perfectly balanced . Paradoxically , if the options are perfectly balanced it doesn’t much matter which one we choose . We agonize the most , spend the most time and effort , making choices that are inside this Zone of Indifference , when we might as well flip a coin.
The analytical methods are designed to help us make the toughest choices , but once we realize we are inside the Zone of Indifference we should stop right there , make an arbitrary choice , and move on.**
What if we don’t move on ? Herbert Simon argued that any company that attempted to optimize its returns and make the best decisions would fall into a never - ending quest to find the best decision . Simon ( 1957 ) coined the term **satisficing** to describe what we all do just about all the time — pick the first option that seems to get the job done and not worry about whether it is the best . [[202108091333 Mental Models Lazyness#^9fbba3]]

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 90 · Position 1752

continuing on until they found an adequate course of action . Klein , Calderwood , and Clinton - Cirocco ( 1986 ) called this strategy a Recognition - Primed Decision ( RPD ) model . ( See figure 6.1 . ) The pattern recognition suggested an effective course of action and then the firefighters used a mental simulation to make sure it would work . This RPD strategy combines intuition with analysis . The pattern matching is the intuitive part , and the mental simulation is the

[[202112201312 Recognition-Primed Decision-Making]]

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 91 · Position 1768

Example 6.3 : Miracle on the Hudson On January 15 , 2009 , at 3 : 25 p.m . , US Airways Flight 1529 , an Airbus 320 , took off from LaGuardia Airport in New York on its way to Charlotte , North Carolina . Two minutes after the takeoff the airplane hit a flock of Canada geese and lost thrust in both of its engines . The captain , Chesley B . “ Sully ” Sullen - berger III , and the first officer , Jeffrey Skiles , safely landed the airplane in the Hudson River at 3 : 31 p.m . All 150 passengers plus the five crew members were rescued . Media reports and interviews with Sullenberger allow us to describe his decision strategy after he lost thrust in both engines . Option 1 was to return to LaGuardia Airport . Sullenberger’s initial message to the air traffic controllers was “ Hit birds . We lost thrust in both engines . We’re turning back toward LaGuardia . ” But he quickly realized that the airplane was too low and slow to make it back , so he abandoned that plan . Option 2 was to find another airport . Sullenberger was headed west and thought he might be able to reach Teterboro Airport in New Jersey . The air traffic controllers quickly gained permission for him to land at Teterboro , but Sullenberger judged he wouldn’t get that far . “ We can’t do it , ” he stated . “ We’re gonna be in the Hudson . ” Option 3 was to land in the Hudson River .

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 95 · Position 1835

Research by Raanan Lipshitz and Orit Ben Shaul ( 1997 ) is consistent with what numerous other studies have found : novices tend to deliberate about which option to select , whereas experts deliberate about what is going on in the situation .

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 95 · Position 1848

may have a choice between a few different job applicants , or between a few different jobs , or different colleges that have accepted us , or we may have to figure out which computer to buy . We may have to decide whether to move to another state to be with a spouse .

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 96 · Position 1850

We can learn from skilled chess players . 17 They don’t settle for the first satisfactory option . They really do want to play the best move possible . Their strategy is to conduct mental simulations of each one of the promising moves , imagining how the option would play out deeper and deeper into the future . Then they take stock of their mental and emotional reactions to what they see in this mental review . If they feel that a line of play is going to get them into trouble , they reject that move . Other moves may have potential and are worth thinking about , and some just feel right — they just seem to be more promising than the others .

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 98 · Position 1895

Because of misplaced faith in claim 3 , and because of its advantages for coordinating planning teams , some organizations mandate these methods . For example , the US Army has a Military Decision Making Process that centers around generating three courses of action and evaluating each option on a common set of dimensions , as illustrated in table 6.1 . The Marine Corps has its own version of this process .

### Markierung (gelb) - 7   Experts and Errors > Seite 101 · Position 1934

7 Experts and Errors According to legend , a famous composer was asked how long it took to write one of his operas and replied “ One week , and all my life before that . ” Many of the claims discussed in this book relate to the nature of expertise . These claims rest on

### Markierung (orange) - 7   Experts and Errors > Seite 104 · Position 1998

The experience and the patterns enable us to judge what to pay attention to and what to ignore . That way , we usually reserve our attention for the most important cues and aspects of a situation . However , if the situation is deceptive or is different from what we expect , we may focus our attention on the wrong things and ignore important cues . That’s why the concept of “ mindsets ” creates so much controversy . Our mindsets frame the cues in front of us and the events that are unfolding so we can make sense of everything . Experience and patterns produce mindsets . The more experience we have , the more patterns we have learned , the larger and more varied our mindsets and the more accurate they are . We depend heavily on our mindsets . Yet our mindsets aren’t perfect and can mislead us . With more expertise , we may become more confident in our mindsets , and therefore more easily misled . 2

### Markierung (orange) - 7   Experts and Errors > Seite 105 · Position 2006

Mindsets aren’t good or bad . Their value depends on how well they fit the situation in which we find ourselves . Mindsets help us frame situations and provide anchors for making estimates . With more experience , our frames will be effective and our anchors will permit accurate estimates . When we are in unfamiliar situations , this same use of mindsets won’t work as well . Our frames may distort what is going on , and we may be susceptible to irrelevant anchors .

 .

### Markierung (orange) - 7   Experts and Errors > Seite 108 · Position 2069

Think of the task of controlling a nuclear power plant . If you insert the graphite rods too far into the core , you slow the reaction and the plant doesn’t produce much energy . If you withdraw the graphite rods too far , the reaction will speed up too much and the uranium in the core may overheat and melt . The technicians in the control room have an array of sensors and computer aids with which to determine what is going on inside the reactor .

### Markierung (gelb) - 7   Experts and Errors > Seite 109 · Position 2093

 Everyone has a different metabolism and physiology , so you can’t rely on standard rules . You can’t boil diabetes management into one - size - fits - all procedures . The successful patients learned offsetting strategies , such as going for a walk , to get the levels down . One Air Force pilot explained that controlling his diabetes was like flying an airplane when you couldn’t use autopilot anymore and had to take over manual control . That illustrates his attitude as well as the strategy of learning the handling characteristics of his condition . The successful diabetics had learned what caused their blood glucose to go up and down , and what the time lags were . Diabetics
[[202201030625 Stable Environments are necessary for developing Intuition]]


[[202201030623 learning requires being able to make mistakes]]













### Markierung (gelb) - 8   Automating Decisions > Seite 115 · Position 2162

And yet many decision - support systems are rejected or fail . The developers think that people will welcome decision aids that make it easier to conduct decision analyses . The developers often can’t believe that decision makers are too stupid to appreciate all the benefits of such decision aids . But decision makers are not just rejecting the aids — they are rejecting the mindset that keeps churning out this kind of system . Decision makers are rejecting the mentality that idealizes reflective , analytical thinking and marginalizes automatic , intuitive thinking , instead of blending the two kinds of thinking .

### Markierung (gelb) - 8   Automating Decisions > Seite 116 · Position 2177

The decision maker alone . Kahneman and I think that most judgments and decisions fall into the first category . We don’t see much to be gained by incorporating information technology when people can attain a reasonable level of expertise . Information technology makes even less sense in the face of unstable , shadowy conditions that are heavily dependent on context . We are most comfortable relying on decision makers when they can develop tacit knowledge as a basis for their intuitions . For that to happen , the environment must have some predictability and decision makers must be able to get feedback on their choices and to gain some level of proficiency .

### Markierung (gelb) - 8   Automating Decisions > Seite 116 · Position 2182

The decision maker is helped by a support system or an algorithm . How hard should I exercise ? Many gyms have diagrams showing heart - rate guidelines . By looking at the column for my age , I can see the level I need to get a good workout and the suggested maximum heart rate . What price should I set for my car when I list it for sale ? By entering the year , model , and condition of the car and the geographical area , I can find out what price the car is likely to fetch . Should we drill for oil in a certain region ? Geologists have powerful analytical tools that can provide useful advice , even though the tools aren’t good enough to replace the seasoned veterans .

### Markierung (gelb) - 8   Automating Decisions > Seite 117 · Position 2187

The decision - support system has the final say , with inputs from the operators . Should a bank lend me money ? Intelligent systems now can do a more reliable and unbiased job than most of the people who have this authority . Drivers can tell their GPS systems if they want the fastest or the shortest route , and get a recommendation . Let the humans feed data into the program , but let the program make the final decision or recommendation . This approach means giving up authority to the decision - support system . However , in cases where our judgment is not particularly accurate ( for example , selecting job applicants or finding our way in unfamiliar cities , or guessing future revenues ) , we’ll probably get better decisions by relying on a formula than by using our judgment .

### Markierung (gelb) - 8   Automating Decisions > Seite 118 · Position 2204

The decision - support system makes the entire decision on its own . There is no point in letting experts do a job that a machine can do better . If I’m driving on an icy road , my car’s anti - lock braking system can figure out when to kick in and how to pump the brakes better than I can . My car’s traction - control system can figure out better than I can how to transfer power from wheels that are starting to slip to those that are solidly contacting the road .

### Markierung (gelb) - 8   Automating Decisions > Seite 121 · Position 2273

Example 8.1 : Racking and stacking4 Software developers have to decide what features to include in the release of the next system . To help with this challenge , Carlshamre ( 2002 ) built a prototype planning aid that balanced the costs of the new features and their value to the client . His system would “ rack and stack ” the proposed features , to help the developers decide which ones to select for the next version . But the simple tradeoffs that Carlshamre built into his planning aid didn’t work . Software developers rejected this system . They weren’t just trading off costs and benefits . For instance , a given requirement might depend on an employee who was going on maternity leave , so the requirement had to be delayed until a subsequent version . Carlshamre also discovered that the concept of “ value to the client ” combined the strategic business value for the customer , long - term and short - term value for a range of consumers with differing importance in different markets , compliance with laws and regulations , compatibility with new computing platforms , and internal cost savings . In estimating the resource demands for a new feature , the developers considered factors such as individual employee’s workload and vacation schedules and their company’s recruitment plans . Carlshamre’s system required users to assign values to the requirements but to the software developers these values were arbitrary and unconvincing . The criteria couldn’t be defined in advance , because many essential parameters are never quantified . The developers were always discovering properties as they planned — some criteria were only realized after solutions were presented . And as they worked the developers were continually gaining new insights about the relationship between features that were treated separately by Carlshamre’s program . Carlshamre concluded that a simple tradeoff — calculating the value of a feature against the resource required to field it — was actually a “ wicked ” problem ( Rittel and Webber 1984 ) that doesn’t permit optimal solutions .

### Markierung (gelb) - 8   Automating Decisions > Seite 123 · Position 2311

However , the errors that the system makes usually involve high - impact weather , such as severe storms and extreme temperature events .

### Markierung (gelb) - 8   Automating Decisions > Seite 124 · Position 2317

Decision aids aren’t doing very well in the field of medicine , either . One review of 100 studies of clinical decision - support systems over a six - year period6 found that if we discount evaluations done by the people who built the systems , fewer than 50 percent showed an improvement in performance . Wears and Berg ( 2005 ) argue that we can’t blame the problems on bad programming or poor implementation .

## Part II   Making Sense of Situations

### Markierung (orange) - Seite 128 · Position 2348

Sensemaking is not just a matter of connecting the dots . Sensemaking determines what counts as a dot . Jumping to conclusions is sometimes the right thing to do even before all the dots have been collected . Feedback depends on sensemaking . Our minds are not computers — they don’t connect dots

### Markierung (gelb) - 9   More Is Less > Seite 130 · Position 2369

A useful way to think about uncertainty is to distinguish between puzzles and mysteries . 3 A puzzle is easily solved with the addition of a critical data point . For example , as I write this ( in 2008 ) we don’t know exactly where Osama bin Laden is hiding . That is knowable . He is somewhere . We just don’t know where he is , or even if he is alive . 4 But if an informer were to provide bin Laden’s current location , the puzzle would be solved . A mystery isn’t solved by critical data . It requires more analysis , not more data . If we want to know what the future will bring to Iraq , no data point will give us the answer . No amount of data will eliminate our uncertainties about whether China is a potential business partner of the United States or an inevitable military , political , and commercial threat .

### Markierung (gelb) - 9   More Is Less > Seite 132 · Position 2403

Most experts use fewer than five cues when making judgments . That doesn’t mean we should stop gathering data after we get five cues . Experts know which five cues will matter . However , even experts ask for more than they need . Gathering the extra information is the easy part . Thinking about what the extra information means takes real work . We would be better off thinking more about what we have learned instead of continuing our pursuit of data . Omodei et al . ( 2005 ) came to the same conclusion in a study of firefighters . The researchers presented experienced commanders a set of decision scenarios of simulated forest fires . The teams with incomplete information performed better than the teams with detailed information .

### Markierung (gelb) - 9   More Is Less > Seite 133 · Position 2418

Oskamp ( 1965 ) gave experienced clinical psychologists more information to use in diagnosing a patient’s condition . The additional information didn’t improve the accuracy of their judgments , but their confidence ratings got higher as they received more information . Therefore , their confidence was misplaced . It reflected the amount of data the judges had , not their accuracy . We have to be careful not to overplay these kinds of studies in which the experimenters control which data to feed to their subjects . In practice , we usually decide for ourselves which data we will examine . We decide when we will stop seeking more data . Look at Mauboussin’s ( 2007 ) study of horse - racing handicappers . The more information the handicappers got , the more confident they were , just as Oskamp found with clinicians . Further , the handicappers ’ predictions were less accurate when they got 40 pieces of information than when they got five pieces . Once again , the more information , the worse the performance . But Ceci and Liker’s racing handicappers ( discussed in chapter 7 ) were able to integrate lots and lots of information . In a natural setting , the handicappers were deciding for themselves what information to use , rather than having the information thrust on them . It makes a difference . Therefore , even though I like the studies showing that people reach a saturation point and their performance gets worse when we drown them in too much data , in real - world settings experts usually can protect themselves . They self - select which types of data to seek . For example , in a study of professional auditors and accounting students , the experts primarily relied on a single type of information , whereas the novices tried to use all the data ( Ettenson et al . 1987 ) . The experts , using their single data source , were more accurate and showed greater reliability and consensus than the students .

### Markierung (gelb) - 9   More Is Less > Seite 133 · Position 2431

The biggest danger of claim 4 ( that we can reduce uncertainty by gathering more information ) is that it invites a mindless search for more data . It invites a search for anything that might be relevant . It invites abuse , encouraging us to move past our saturation point . Even when we control our own searches for information , we tend to gather more than we need , and the extra information can get in our way . We do this to ourselves . How many times have we gathered information we didn’t use ? The real reason for gathering extra information isn’t to make a better decision ; usually it is to stall for time .

### Markierung (gelb) - 9   More Is Less > Seite 144 · Position 2624

We will gather more data . The more the uncertainty , the more strenuous the data gathering . We won’t stop until our uncertainty disappears or we become exhausted , whichever comes first . Sometimes we will notice that the information we need is already in our “ in ” boxes . Therefore , we’ll decree that every message must be read in its entirety , to make sure we don’t miss anything . If we can’t personally read every message , we’ll have to hire additional staff members , and somehow work out how all those involved will collaborate . Because of the potential for overload , we will have to invest in information technology to collect and categorize and analyze all the data . And we’ll have to store the data in a way that will let us retrieve what we need instantaneously ( as long as we remember it and the label we used to store it ) . We will also have to develop intelligent filters to figure out what is relevant and what we can ignore . To make sure this all works efficiently , we’ll develop measures and standards for the right amount of data with which to make any type of decision . Does that fill anyone with confidence ?

### Markierung (gelb) - 9   More Is Less > Seite 144 · Position 2638

When dealing with a mystery , instead of a puzzle , we enter the realm of complexity . We have to pick our way through the shadows . What is the future of the relationship between Islamic and Western civilizations ? No data point will solve this mystery . The more books we read , the more lectures we attend , the more we have to think about , the more complicated it all gets . Information will not cure these kinds of uncertainty . And a non - directed search for more information can just add to the confusion . Richards Heuer wrote in Psychology of Intelligence Analysis ( 1999 ) that the intelligence community needs more analysis , not more data . His recommendation applies to many other communities as well . The replacement for that claim is that in complex environments , what we need isn’t the right information but the right way to understand the information we have . Further , under complex conditions , we need to manage uncertainty more than we need to reduce it . To manage uncertainty we have to know how to seek and prioritize information . We need to fill gaps with assumptions . We need to know when to wait for the situation to evolve . We need the cleverness to act in a way that structures the situation . 11 Managing uncertainty also means managing the teams and organizations that exchange messages or suppress them . The examples of Pearl Harbor , 9 / 11 , and Enron show that we need to improve team sensemaking , because teams may ignore the weak signals that individuals notice . The Phoenix Memo , which preceded the 9 / 11 attack , illustrates how easy it is to suppress suspicions .

### Notiz - 9   More Is Less > Seite 145 · Position 2650

Als Student sollte man nicht zuviel lesen, sondern mehr sensemaking

### Markierung (gelb) - 9   More Is Less > Seite 145 · Position 2654

However , we had asked the participants to keep private diaries , and in every team at least one member noted the weak signals in his or her diary . Sometimes half the members of a team noted the weak signals in their diaries . In other words , each team had the potential to surface the weak signals . But not a single team talked about them . Somehow the climate of teamwork suppressed these cues . We need to find ways to encourage team members to voice suspicions and hunches without inviting ridicule or losing credibility .

### Markierung (gelb) - 9   More Is Less > Seite 146 · Position 2660

Claim 4 emphasizes the quest for more information , whereas in facing mysteries people need to focus on the ways they are interpreting and anticipating events . Managing information puts the emphasis on what we understand , not on how many signals we have collected . It’s usually more valuable to figure out how the data connect than to collect more data .

### Markierung (gelb) - 10   When Patience Is a Vice > Seite 148 · Position 2700

Feltovich , Coulson , and Spiro ( 2001 ) took this research out of the laboratory and studied pediatric cardiologists in a hospital setting . Their experiment used a garden - path scenario in which participants form an incorrect initial explanation , and then get messages that contradict the initial story . The researchers measure how long people stay on the garden path — how much contrary evidence they need before they come to their senses . In this study , the cardiologists read a description of a new case and tried to find a diagnosis while receiving more and more information about the fictitious patient . The initial description made the diagnosis seem fairly obvious . However , that obvious diagnosis was wrong . The subsequent information contradicted the obvious diagnosis . Feltovich et al . found that some cardiologists stayed on the garden path for a very long time . Some never got off . They remained fixated on the initial “ obvious ” diagnosis .

### Markierung (gelb) - 10   When Patience Is a Vice > Seite 149 · Position 2723

In the spring of 2000 , when the staff in the Army headquarters in the Presevo Valley saw eight men going in a direct path through the town , it matched the pattern they expected . From their experience , groups that big walking around town were usually up to no good . Fortunately , the Army unit had just gotten their Unmanned Aerial Vehicle to begin sending them visual feeds , which ran live in the update room , where everyone could see them . They oriented the UAV to stay in the area of the suspicious activity . The Brigade Commander came in to watch . The gang of eight men were moving quickly , jumping fences , and appearing to be wreaking havoc . The Brigade Commander gave soldiers on the ground orders to find the bad guys . Then the men who were being watched on the UAV feed began running around . The HQ staff speculated about why the men had started running ; perhaps they heard the UAV , or heard a helicopter , or heard the American soldiers moving toward them . Regardless , the staff still thought the behavior was consistent with being “ bad guys , ” so they weren’t really trying to figure out why the men were running . As seen on the UAV feed , the suspicious band of men would start running down an alley , then turn around and run in the other direction . Their movements became pretty erratic , but this didn’t send up a red flag . Because it was dark , it was hard to make out how the men were dressed . The Army staff knew the coordinates of the UAV and the general location of the thugs , but wasn’t sure . There were no distinguishing landmarks . The staff believed that the group was moving with a purpose . The cues were that there were eight of them , they were tightly knit , and they were looking into windows . The cohesiveness of the group seemed to be a cue ; it didn’t seem to be just a group out walking . Everyone seemed to know where the others were going , and they were moving fairly fast . When they would stop running , it seemed that they were trying to re - organize , that they didn’t feel that they were in any immediate danger anymore , and that they were trying to figure out what to do next . Toward the end of this incident , an intelligence analyst from another unit came into the update room to see what was going on . He never said anything or asked for information from anyone . He just said “ Hey , those are our guys . ” The soldiers on the ground were also the “ bad guys ” they were trying to catch ! When asked how he had figured it out , the intelligence analyst said he could hear the commander saying “ move right ” and then saw the people on the feed move right . The whole incident took approximately 15 minutes . The intelligence analyst only came into the room for the last 2 – 3 minutes . No one on the staff who had been there from the start of the incident realized that the men they observed were responding to the commander’s orders . Some people did notice a connection between the orders and the reactions of the “ bad guys , ” and explained it away : “ They must be intercepting our radio communications . ” This example and many others point to the importance of claim 5 . Yet our survey respondents gave this claim an average rating of only 5.06 , “ Tend to agree for most situations . ” They weren’t entirely convinced of it . Twenty out of 164 disagreed with it .

### Markierung (gelb) - 10   When Patience Is a Vice > Seite 152 · Position 2761

Example 10.2 : The plugged breathing tube Rudolph used a garden - path scenario to study the anesthesiologists . In the scenario , an anesthesiologist - in - training was called into a simulated but fully outfitted operating room to provide anesthesia for a woman ( actually , a very lifelike mannequin ) who was being prepared for an appendectomy . After getting the breathing tube into the mannequin’s airway and putting “ her ” to sleep , somehow the ventilation stopped working very well . Why could that be ? ( The actual reason is that the “ woman ” exhaled some mucous into the tube and this mucous plug hardened inside the breathing tube . ) The anesthesiologists struggled to what was going wrong because the timing of the ventilation problem and the patient’s history of mild asthma suggested that the cause might be a bronchospasm ( an asthma attack ) . When treating the bronchospasm didn’t work , another common reaction to the ventilation problem was to suction the breathing tube to remove any blockages . This treatment , however , also had no effect because the mucous plug had hardened . This is a rare development . Worse , the surgical team was under time pressure to remove the inflamed appendix . Rudolph divided her subjects into four categories based on their reactions to this scenario : Stalled , Fixated , Diagnostic Vagabonds , and Adaptive Problem Solvers . Two of the anesthesiologists fit the “ stalled ” category . Neither of them could find any pattern that showed them how to proceed . They couldn’t generate diagnoses , and they didn’t try different treatments . Neither figured out the problem . The eleven physicians categorized as fixated ( including one chief resident ) usually jumped to the obvious diagnosis of bronchospasm . This diagnosis fits perfectly with the timing of the ventilation problem , which began after the breathing tube was inserted . These physicians tended to repeat one treatment for bronchospasm over and over rather than experimenting with different treatments . They rarely reconsidered whether the ventilation problem was in the tube rather than in the patient . Six of these fixated physicians did wonder about secretions in the breathing tube , and two of them tested for a blocked tube . The test is to see if secretions come out when the tube is suctioned . Because the mucous plug had hardened , no secretions came out , so they mistakenly concluded that the tube itself was clear . The physicians also erroneously interpreted distant breath sounds as wheezes , a sign of bronchospasm . None of the eleven anesthesiologists in the fixated group diagnosed the problem . The 17 open - minded anesthesiologists fared no better . Rudolph called their pattern “ diagnostic vagabonding , ” because these anesthesiologists wouldn’t commit to any diagnosis but instead treated all possibilities as tentative . Was the problem bronchospasm , too little muscle relaxant , or a wrongly placed tube ? These physicians would consider each possibility but quickly jump to the others , and never engaged in a course of treatment that would let them probe more deeply . None of them figured out the problem . Last , we have the nine physicians who jumped to conclusions but tested those beliefs . Rudolph called them “ adaptive problem solvers . ” Like the fixated problem solvers , most of them immediately identified the bronchospasm as the most likely cause . But when their treatment didn’t work , they turned to other diagnoses ( e.g . , allergic reactions , pulmonary embolisms ) , testing and rejecting each , eventually speculating about an obstructed breathing tube . Their active exploration style let them use initial diagnoses as springboards for conducting subsequent tests and treatments . Seven of these nine physicians discovered the hardened mucous plug . They tested in different ways — with fiber optic scopes , by the feel of the suction catheter , by the dry sound as they did the suctioning , by comparing how far they could insert the suction catheter versus the length of the breathing tube . Four different strategies that all led to the same diagnosis . No physician in any of the other categories got there . Rudolph expected that the anesthesiologists who jumped to a conclusion and held on to it would be unsuccessful , and she was right . None of them ever figured out the problem . Rudolph also expected that the anesthesiologists who kept an open mind while receiving the stream of information would be successful at making the right diagnosis , and here she was wrong . The ones who kept an open mind , absorbing data like sponges , also failed . The only ones who succeeded had jumped to conclusions and tested them . They hadn’t fixated on their first explanation . Instead , they had used that explanation to guide the tests they had performed and the way they had searched for new information . They exemplified the strategy of “ strong ideas , weakly held . ” Now we have a conflict . Rudolph found that the physicians who kept an open mind didn’t make the diagnosis , but Bruner and Potter found that the subjects who speculated too soon , when the image was too blurry , did the worst . Where does that leave us ? Maybe it depends on whether people are active or passive . Bruner and Potter’s subjects , college students , couldn’t direct their own search ; they just had to sit there , watching a fuzzy image , describing what they thought they saw . In contrast , Rudolph let her subjects — anes - thesiologists — actively gather information . Perhaps that’s why their early speculations became a basis for testing and inquiring .

### Markierung (orange) - 10   When Patience Is a Vice > Seite 154 · Position 2807

doesn’t work under conditions of complexity . Replacement The replacement for claim 5 is to speculate actively , but to test those speculations instead of getting committed to them . Rather than advising people to keep an open mind , we can encourage them to engage in a speculate - and - test strategy . 6 Cohen , Freeman , and Thompson ( 1998 ) have developed a training approach to support the speculate - and - test strategy , and have demonstrated its effectiveness .

### Markierung (gelb) - 10   When Patience Is a Vice > Seite 158 · Position 2870

I think that in complex and ambiguous settings people should actively speculate , instead of passively absorbing data . This advice is just the opposite of claim 5 . Experts distinguish themselves by their ability to anticipate what might happen next . Even while doing their work , they are positioning themselves for the next task . Their transitions from one task to the next are smooth instead of abrupt . By forming sharp expectancies , experts can notice surprises more readily . They notice novel events and the absence of expected events . 9 As Weick and Sutcliffe ( 2007 , p . 45 ) observed , experts “ don’t necessarily see discrepancies any more quickly , but when they do spot discrepancies , they understand their meaning more fully and can deal with them more confidently . ” When we anticipate , we get ourselves ready for what may come next . We redirect our attention . We also can prepare for a few different possibilities . We don’t know for sure what the future will bring , but we aren’t going to stand idly by . And we can’t engage in anticipatory thinking by forcing ourselves to keep an open mind , waiting until we receive enough information before we begin to explore ways to react .

### Markierung (gelb) - 11   The Limits of Feedback > Seite 171 · Position 3068

Aside from the difficulty of giving feedback about tacit knowledge , instructors who give us rapid and accurate feedback can actually interfere with our learning . Schmidt and Wulf ( 1997 ) found that continuous , concurrent feedback does increase our learning curve while we are being trained but that it reduces any performance gains once the training is over . The reason is that we never have to learn how to get and interpret our own feedback as long as the instructor is doing all that work . The performance goes up nicely in the training environment , but then we are at a loss once we move into our work environment . We would be better off struggling to get our own feedback than having it spoon - fed to us by the instructors . The Schmidt - Wulf research involved motor skills rather than cognitive skills , but I suspect that there are times when aggressive feedback gets in the way of learning cognitive skills . Consider the way we would want someone to coach us to become a better cook . Rapid feedback from a coach ( e.g . , “ no , add more salt ” ) can interfere with our ability to develop a sense of taste that can dictate how to alter the seasoning . We need to learn for ourselves how to evaluate the saltiness of the food . The very idea of giving feedback suggests a passive learner . Learners are better off seeking their own feedback and asking for advice when they need it . 5

### Markierung (gelb) - 11   The Limits of Feedback > Seite 171 · Position 3083

Feedback applies more to some kinds of learning than to others . 6 It applies to learning about the actions we take .

### Markierung (gelb) - 11   The Limits of Feedback > Seite 172 · Position 3095

Sensemaking is at the heart of learning cognitive skills . We aren’t just acquiring new knowledge . We are changing the way we see things and think about them . We are making sense of conflicting and confusing data .

### Markierung (orange) - 11   The Limits of Feedback > Seite 173 · Position 3104

described in his novel Walden Two . The Walden Two society didn’t sound very inviting in 1948 , when the novel was published . It doesn’t sound any better now .

### Markierung (gelb) - 11   The Limits of Feedback > Seite 175 · Position 3142

Therefore , the replacement for claim 6 is “ We can’t just give feedback ; we have to find ways to make it understandable . ”

### Markierung (gelb) - 11   The Limits of Feedback > Seite 175 · Position 3150

In my book on intuition ( 2004 ) I suggested ways to make metrics more understandable and useful by blending them with stories . Story formats can help people make sense of feedback .

### Markierung (orange) - 11   The Limits of Feedback > Seite 176 · Position 3159

learner into account . 8 The learning relationship is just that — a relationship . Coaches have to diagnose what is going wrong ; they also have to find ways to get their message across .

### Markierung (gelb) - 12   Correcting the Dots > Seite 179 · Position 3191

ambiguous settings where you aren’t sure what to count as a dot . It isn’t that claim 7 is wrong . Like all the other claims , it tells only part of the story . It leaves out what counts as a dot , where the dots come from , how you know that you have finished , and how you suspect that a story is wrong .

### Markierung (gelb) - 12   Correcting the Dots > Seite 179 · Position 3196

Our expertise is as much about recognizing legitimate dots as about connecting them . Similarly , the metaphor of sensemaking as putting together the pieces of a puzzle is also misleading . When we assemble a puzzle , we have seen the box cover and know what we are trying to create . The job is much more difficult if we mix five puzzles together , and don’t see the cover picture for any of the boxes . Or if we try to solve a puzzle that doesn’t have a scene ( such as an all - black puzzle ) or a shape ( such as a puzzle with ragged edges instead of smooth ones ) . The metaphors of connecting the dots and assembling the puzzle don’t do justice to the need to notice and identify the cues in the first place .

### Markierung (gelb) - 12   Correcting the Dots > Seite 179 · Position 3207

In a 1919 book titled Fighting the Flying Circus , Eddie Rickenbacker , America’s World War I flying ace , described one of his early flights over Germany . When the group returned to their airport , the flight leader asked Rickenbacker what he had seen . Rickenbacker said it had gone very smoothly — he hadn’t seen any other airplanes except the ones in their formation . Rickenbacker had seen some German antiaircraft batteries and found it amusing to watch them waste their ammunition . The flight leader corrected him . A formation of five British Spads had passed under them just before they crossed into enemy lines and another formation of five Spads went by them soon after , neither more than 500 yards away . Plus four German Albatros airplanes two miles ahead of them when they turned back , and another German airplane , a two - seater , later on . Then the flight leader walked Ricken - backer over to his airplane and showed him the shrapnel holes from the German anti - aircraft fire that Rickenbacker found so amusing , including one piece of shrapnel that passed through both wings a foot from his body . Rickenbacker hadn’t realized that his plane had been hit . That was the beginning of his real education . The data were there — to the flight leader , but not to Rickenbacker . To survive , pilots had to learn quickly where to look and what to look for .

### Markierung (orange) - 12   Correcting the Dots > Seite 182 · Position 3253

see . They kept looking until they felt that they understood how the atmospheric forces were playing out . They knew which dots were relevant to the different possible stories . The data elements that we think are dots may turn out to be irrelevant . The next incident illustrates

### Markierung (gelb) - 12   Correcting the Dots > Seite 184 · Position 3286

Bill Duggan’s 2007 book Strategic Intuition is about the way people notice connections . For instance , Duggan explains how Bill Gates and Paul Allen made their initial discoveries . In high school they had programmed BASIC ( a simple computer language ) onto a PDP - 8 minicomputer . Later they noted that the new Intel 8082 chip had enough capacity to contain their BASIC program . Gates and Allen contacted the major computer companies to see if they would be interested in having a BASIC program for this new 8082 chip . None of them expressed any interest . Then , in December 1974 , as Allen was walking over to Gates’s dorm room at Harvard , he saw the cover of a issue of the magazine Popular Mechanics which featured the new Altair personal computer . The Altair , according to Popular Mechanics , would run on the 8082 chip and would cost only $ 397 . Instantly , all the connections fell into place . Gates and Allen contacted the manufacturer of the Altair , arranged a demonstration , and were soon on their way toward starting Microsoft . They had a new vision of a software company for microcomputers . Because of their backgrounds , the relevant dots jumped out at them , but not to others . The immediate dot was the cover of Popular Mechanics , which linked to their BASIC program and to their enthusiasm for the 8082 chip . Others looked at the magazine and thought that the price of personal computers was coming down . They didn’t know about the other dots . You can’t connect dots that you aren’t able to see . 5

### Markierung (gelb) - 12   Correcting the Dots > Seite 190 · Position 3398

Example 12.6 : Watching a baby develop an infection “ This baby was my primary ; I knew the baby and I knew how she normally acted . Generally she was very alert , was on feedings , and was off IVs . Her lab work on that particular morning looked very good . She was progressing extremely well and hadn’t had any of the setbacks that many other preemies have . She typically had numerous apnea \[ suspension of breathing \] episodes and then bradys \[ bradycardia episodes — spells of low heart rate \] , but we could easily stimulate her to end these episodes . At 2 : 30 , her mother came in to hold her and I noticed that she wasn’t as responsive to her mother as she normally was . She just lay there and half looked at her . When we lifted her arm it fell right back down in the bed and she had no resistance to being handled . This limpness was very unusual for her . “ On this day , the monitors were fine , her blood pressure was fine , and she was tolerating feedings all right . There was nothing to suggest that anything was wrong except that I knew the baby and I knew that she wasn’t acting normally . At about 3 : 30 her color started to change . Her skin was not its normal pink color and she had blue rings around her eyes . During the shift she seemed to get progressively grayer . “ Then at about 4 : 00 , when I was turning her feeding back on , I found that there was a large residual of food in her stomach . I thought maybe it was because her mother had been holding her and the feeding just hadn’t settled as well . “ By 5 : 00 I had a baby who was gray and had blue rings around her eyes . She was having more and more episodes of apnea and bradys ; normally she wouldn’t have any bradys when her mom was holding her . Still , her blood pressure hung in there . Her temperature was just a little bit cooler than normal . Her abdomen was a little more distended , up 2 cm from early in the morning , and there was more residual in her stomach . This was a baby who usually had no residual and all of a sudden she had 5 – 9 cc . We gave her suppositories thinking maybe she just needed to stool . Although having a stool reduced her girth , she still looked gray and was continuing to have more apnea and bradys . At this point , her blood gas wasn’t good so we hooked her back up to the oxygen . On the doctor’s orders , we repeated the lab work . The results confirmed that this baby had an infection , but we knew she was in trouble even before we got the lab work back . ”

### Markierung (gelb) - 12   Correcting the Dots > Seite 192 · Position 3435

These incidents illustrate how people question the frame despite data that appear convincing . Dave Malek , the nurse in the NICU example , and Lieutenant Colonel Petrov each made sense of events by rejecting the obvious story . In some ways their judgments weren’t any different from those of the officer at Pearl Harbor who dismissed the message of unknown airplanes spotted on radar . The mere act of explaining away data doesn’t merit our applause . I think the difference from the Pearl Harbor example is that Malek , the NICU nurse , and Petrov , as well as the intelligence officer who explained away the ominous airplanes that circled nuclear power plants after the 9 / 11 attack , all worried that they were wrong , and worried about the consequences of a mistake . They worried about the quality of the data .

### Markierung (gelb) - 12   Correcting the Dots > Seite 193 · Position 3451

The assembly - line model obscures the context in which data were collected . This loss of context may not matter to mediocre workers , but experts appreciate the implications of data collection methods . They know that each method has its own limits . Meteorologists know the limits of instruments for sampling wind direction and velocity , temperature , humidity , and so forth . They can take these limits into account . If you give weather forecasters a summary sheet , they can’t sort out where the forecast came from . The assembly - line model builds from data elements — dots — that have been collected . But what about events that didn’t happen ? These can also be informative . Experts appreciate the significance of these negative cues . But when they are given a set of recommendations and summaries , skilled decision makers don’t have any way to notice what didn’t happen .

### Markierung (gelb) - 12   Correcting the Dots > Seite 194 · Position 3468

The notion that data somehow appear doesn’t do justice to these kinds of active searches for meaningful cues . 9 Sensemaking isn’t just receiving data and inferences . It also involves knowing how to shake the system to find what you’re looking for .

### Notiz - 12   Correcting the Dots > Seite 194 · Position 3470

Ooda

### Markierung (gelb) - 12   Correcting the Dots > Seite 194 · Position 3471

Example 12.8 : Japan’s next target During World War II , the Japanese followed their attack on Pearl Harbor with additional victories , such as in the Battle of the Coral Sea . Yamamoto , their top naval commander , was obviously getting ready for yet another attack , but the Americans didn’t know where it might come . Perhaps Yamamoto would re - attack Pearl Harbor ; perhaps he would even attack California . As Japanese coded messages increased in volume , indicating that an attack was imminent , a US Navy cryptologist had a hunch . He was Captain Joseph Rochefort , an intelligence officer and a cryptologist who had also trained in the Japanese language . Rochefort was the officer in charge at Station HYPO , the cryptoanalysis unit in Pearl Harbor . He and others noticed that more and more of the Japanese messages they could partially decode used the phrase “ AF , ” which seemed to be the code for the next US target . But where was AF ? Rochefort studied a map of the Pacific and decided that if he was Yamamoto , getting ready to stage raids on Pearl Harbor or on the western United States , he would go after Midway Atoll . To test his theory , Rochefort arranged for the small unit on Midway to send a radio message describing a malfunction in their water - distilling plant . Two days later a Japanese cable said that AF was running low on drinking water and directed the AF force to bring additional water desalinization equipment . When Rochefort read this intercepted message , he knew Japan’s next target . Admiral Nimitz then reinforced Midway and sent three aircraft carriers to the island . Nimitz also knew the Japanese order of battle ; now that “ AF ” had been clarified all the previous messages about AF came into focus . Instead of an easy victory over a small garrison in an isolated island , the Japanese lost four of their six primary aircraft carriers and more than 250 airplanes . In one day at Midway they lost twice as many skilled pilots as their training programs produced in a year . The war in the Pacific theatre turned completely around . 10 Rochefort wasn’t waiting for the data to come to him . He wasn’t seeing his job as simply deciphering the Japanese messages . His job was to figure out what Yamamoto was planning .

### Markierung (gelb) - 12   Correcting the Dots > Seite 195 · Position 3489

Example 12.9 : Refusing to take any scrap In a factory that produced molds , the workers knew that excessive scrap pushed their expenses up and made them less competitive . The industry standard was about 6 percent , and , at their worst , this factory had a scrap rate of 14 percent . The workers hadn’t worried about their scrap rate until the owner lost the company to the bank and the workers took it over . They had to become profitable or they would lose their jobs . They succeeded in bringing their scrap rate down to 6 percent . The workers wondered if they could get the scrap rate even lower . They questioned what was behind this 6 percent figure and discovered that only a few of the molds accounted for most of the scrap . By redesigning these mold patterns , or by charging more for them , they could become more competitive for the rest of their products . They reduced their scrap rate dramatically , from 6 percent to 2.9 percent . And this reduction took place because they didn’t take the scrap rate as firm data ; instead they pushed further to investigate where that number came from . 11

### Markierung (gelb) - 12   Correcting the Dots > Seite 196 · Position 3508

In example 12.10 , the young sergeant was studying the terrain and reporting on what he saw . The general was speculating about what was out there , and was looking for things that he expected . We make sense of cues and data by fitting them into frames such as stories . An interesting cue , such as the increased use of “ AF ” in Japanese messages , may remind us of a frame — a preparation for the next attack . The reverse also happens . The frames we have learned guide our attention and shape what we notice . If we go for a walk in a park , a landscape architect is aware of the way the paths provide interesting views , a tree specialist is distinguishing the different species and how well each is growing , and a maintenance worker is getting a feel for whether heavy lawnmowing equipment might get stuck in the muddy ground . Same park , three different perspectives , three different experiences . 12 We make sense of cues and data by organizing them into frames such as stories , scripts , maps , and strategies . But the reverse also happens — our frames determine what we use as data . Both processes happen simultaneously , as shown in figure 12.3 . This reciprocal action between data and frames is the core of sensemaking . That’s the replacement for claim 7 . We make sense of data elements by fitting them into frames such as stories , but the reverse also happens — our frames determine what we use as data . Figure 12.3 The data / frame model of sensemaking ( the process of fitting data into a frame and fitting a frame around the data ) .

## Part III   Adapting

### Markierung (gelb) - 14   Moving Targets > Seite 212 · Position 3708

What happens when we don’t have clear goals ? This is the most serious limitation of claim 8 . Here we are facing what Rittel and Webber ( 1973 , 1984 ) described as “ wicked problems ” in which the goals are incomplete and keep changing , as well as occasionally conflicting . Solutions to wicked problems aren’t true or false . Instead , they are judged as good or bad because there is no way to test a solution to a wicked problem . Wicked problems epitomize the world of shadows . When we are faced with a wicked problem — when the goals just aren’t clear — there aren’t any objective ways to gauge success .

### Markierung (gelb) - 14   Moving Targets > Seite 213 · Position 3716

Claim 8 runs into trouble in complex situations in which the wicked problems prevent us from clarifying the goals at the start . Many of the problems we face are wicked problems in which we may have to reassess our original understanding of the goals . The goals will become clearer as we learn more . That’s why I am calling them emergent goals . Let’s look at some examples .

### Markierung (gelb) - 14   Moving Targets > Seite 215 · Position 3760

Bill Duggan , in Strategic Intuition ( 2007 ) , describes a number of other instances in which leaders changed their goals on the basis of what they learned . Napoleon went into battle without any firm plans . He maneuvered his forces until he found a match - up that suited him ; then he attacked . Bill Gates and Paul Allen , the founders of Microsoft , didn’t set out to start a software company . That came only after they failed with their initial ventures and realized the potential for standard software programs . Sergey Brin and Larry Page , the founders of Google , were just trying to find good dissertation topics in graduate school . They tried to sell a patent for their search algorithm for $ 1 million , but when that was turned down they decided to try to commercialize it themselves . Duggan argues that such breakthroughs depend on the ability to learn by making new connections , not on doggedly pursuing the original goal .

### Markierung (gelb) - 14   Moving Targets > Seite 221 · Position 3859

And Gantt charts discourage a team from modifying its goals . In the large project I was describing , once the Gantt charts were constructed and accepted , the customer approved funding for the next phase only if the milestone for the previous phase was reached . Bonuses were linked to the milestones . So no one wanted to move the milestones . And no one dared to think about changing any of the goals of the program . That’s another limitation of objectives - based tools such as Gantt charts : they contribute to goal fixation .

### Markierung (gelb) - 14   Moving Targets > Seite 223 · Position 3907

Jay Rothman and I have suggested an alternative approach , which we call Management by Discovery ( Klein and Rothman 2008 ) . In contrast to Management by Objectives , Management by Discovery ( MBD )

### Markierung (gelb) - 15   The Risks of Risk Management > Seite 235 · Position 4097

Authentic dissenters may be disliked even when they have helped the group do a better job .

### Notiz - 16   The Cognitive Wavelength > Seite 262 · Position 4578

Di rosso

- we can rest easy once we have completed the preparation. As the examples illustrate, common ground is never perfect and is always eroding.14 ([Location 4589](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4589))
    - **Tags:** [[orange]]
- replacement for claim 10 is that all team members are responsible for continually monitoring common ground for breakdowns and repairing common ground when necessary. Once we have finished preparing the team we should still be on the lookout for breakdowns. We should continually ([Location 4599](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4599))
- In complex situations, the original plans, goals, and roles are likely to change, degrading the team’s common ground. Instead of trying to increase control, we should expect common ground to erode and then repair it on the fly. ([Location 4602](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4602))
- Studies of highly reliable organizations, such as power plants and aircraft carriers, show that good teams spot potential confusions and repair them.16 They also take advantage of slow times to re-calibrate common ground, because they know from experience that once they are in the middle of a crisis it may be too late.17 One of the clues that common ground is breaking down is when we say “How can they be so stupid?” in regard to some of the actions taken by our teammates. People usually aren’t stupid. If they seem stupid, then maybe we don’t understand what is going on. ([Location 4643](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4643))
- Complex settings, on the other hand, wouldn’t benefit from routine handoff scripts. Emily Patterson (2008) studied the handoff process in a hospital setting and found that the people on the shift that was ending started their handoff by going over the most important ([Location 4656](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4656))
    - **Tags:** [[orange]]
- Example 17.1: Students and scientists Chinn and Brewer (1993) examined the way people reacted to anomalous data that contradicted some of their beliefs. They compared science students with actual scientists, and found that both used six common strategies to explain away discrepancies: •   They ignored the data. Scientists don’t pay attention to new claims for perpetual-motion machines or extra-sensory perception. •   They found something wrong that allowed them to reject the data. When Galileo, using telescopes, made findings that contradicted Aristotle, rival scientists argued about the reliability of the telescopes. •   They found a reason why the data didn’t really apply. It wasn’t clear if the phenomenon of Brownian motion (the random movement of particles suspended in a liquid or gas) was in the realm of biology, in the realm of chemistry, or in the realm of heat theory in physics, so scientists whose ideas were challenged by Brownian motion simply explained that one of the other fields would have to figure it out. •   They came up with a reason to hold the data in abeyance until some time in the future. When astronomers found that the orbit of Mercury was inconsistent with the Newtonian view, they just expected that someone eventually would reconcile the anomaly. •   They reinterpreted the data to make them less problematic. When a scientist speculated that mass extinctions in the Cretaceous era were caused by a meteor or comet, citing layers of iridium at a site in Italy, rivals argued that the iridium might have seeped down from layers of limestone above it. •   They made a small, peripheral change in their theories and models that seemed to handle the data without having to re-conceptualize anything. Galileo’s opponents believed that the moon and other heavenly bodies were perfect spheres. When Galileo persuaded one critic to look through a telescope and see mountains on the moon, the opponent countered that these mountains must be embedded in a transparent crystal sphere. When Copernicus suggested that Earth was rotating around the sun, astronomers who disagreed pointed out that the position of the stars stayed the same throughout Earth’s putative orbit around the sun each year. Surely during a six-month period when Earth would be moving from one side of the sun to the other, the stars should change their positions. Copernicus responded, not by giving up his ideas, but by making a peripheral change in his theory. He suggested that the stars must actually be very far away, and thus Earth’s orbit wouldn’t make much difference. This ad hoc explanation seemed feeble when Copernicus first voiced it, but we now know it was correct. So fixation isn’t always a weakness. It can help people mature their ideas. Physicians exhibit cognitive rigidity. Feltovich, Coulson, and Spiro (2001) showed that pediatric cardiologists had difficulty getting off the garden path.4 Once they formed an impression of what was wrong with a child, contrary evidence… ([Location 4702](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4702))
- flying at 1,000 feet or higher. At the very low altitudes they had to anticipate where they would have the time to perform tasks other than flying their aircraft. The new training concept dramatically reduced the number of crashes. The preceding example ([Location 4749](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4749))
    - **Tags:** [[orange]]
- Example 17.3: Explaining the Monty Hall problem We are going to make three passes at the problem. The first pass uses an exercise that relies on experience, the second exercise relies on a question, and the third exercise uses a new frame. The experience Place three similar objects in front of you, as shown in figure 17.1. They can be coins, ashtrays, sticky notes, or whatever is handy. These objects represent the three doors on Let’s Make a Deal. (Please do the exercise instead of just reading about it. It makes a difference.) Figure 17.1 The Monty Hall Problem. We’re going to go through this two times. The first time, you will stick with your first choice and we’ll see what happens when the prize is behind each of the doors. The second time you will shift from your first choice, and again we’ll see what happens when the prize is behind each of the doors. To save time, throughout this exercise you are always going to pick door 1 on the left. (See figure 17.1.) It’s easier to explain this way rather than doing all the permutations. Let’s start the first set of three trials, where you stick with your initial choice, door 1. For the first trial, find another object, perhaps a small packet of sweetener, to represent the prize, and put it on door 1. Now, you picked door 1 and the prize is behind door 1. As a stand-in for Monty Hall, I open one of the other doors (it doesn’t matter which, because neither has a prize behind it), you stick with your original choice, and you win. Congratulations. In the second trial, the prize is behind door 2. Move the marker behind door 2. You pick door 1, I open door 3 (showing that there isn’t a prize behind it), you stick with door 1, and you lose. Too bad. In the third trial, the prize is behind door 3. Move the marker over there. You pick door 1, I open door 2 (no prize), you stick with door 1, and again you lose. So by sticking with your original choice you win one out of three times. Those are the odds you would expect. Now let’s go through the drill a second time. This time, you are always going to switch. In the first trial, the prize is behind door 1. Move the marker back. You pick door 1, I open one of the others. It still doesn’t matter, but suppose I open door 2. You switch. You aren’t going to switch to door 2, because I’ve already shown that the prize wasn’t there, so you switch to door 3. I open all the doors, and the prize is behind door 1, your original choice. You lose, and you feel like a chucklehead for switching. In the second trial, the prize is behind door 2. Move the marker over. You pick door 1. I open door 3, which doesn’t have a prize. You switch. Obviously you won’t switch to door 3, because I’ve already shown that the prize wasn’t there. So you switch to door 2 and you win. In the third trial, the prize is behind door 3. Move the marker over (this is the last time, if you’re getting tired). You pick door 1. I open door 2. You switch to door 3, and you win again. Thus, when you stuck with… ([Location 4774](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4774))
- How can we get people to abandon old mental models so they can grow into new ones? To overcome fixation, one thing we can do is spot when it is happening to us, or to others. To do that, we can ask this question: What evidence would it take to change your mind? (It is easier to spot fixation in others than in ourselves.) If someone can’t think of any evidence, that is a sign that he or she may be fixating. A second suggestion is to keep an eye on how much contradictory evidence we need to explain away in order to hold on to our beliefs. If we are mistaken about what is happening, the contradictions should mount and we should be explaining away more and more discrepancies. We will reach a point where we realize that we are wrong. The sooner we get to that point, the better. Third, we can look at some comparable cases to see what typically happens; if our estimates are much different we better have a good explanation.8 Fourth, we can bring in a person who doesn’t have much history with the issues at hand and who thus will have a fresh perspective. That’s what happened in the example of the UAVs in Kosovo (chapter 12). The officer who figured out that the staff members were watching their own team and not a gang came in after the staff had been working the problem. He wasn’t in the room when they got on the garden path or when they became increasingly committed to it. Doug Harrington was fortunate. If the LSO hadn’t visited him on the fateful night, he would have ended his aviation career in the Navy without understanding what went wrong. Instead of waiting for failures, as in Doug’s case, we can manufacture them, maybe using exercises and simulations that prepare learners to absorb feedback. That’s a fifth suggestion. We can use these exercises to create a conflict and show people that their old beliefs don’t work very well. Example 17.4: Going against the flow My colleagues Danyele Harris-Thompson, Dave Malek, and Sterling Wiggins once did a research project to train operators who control the movement of petroleum products in pipelines that can be hundreds of miles long. The operators had been trained on the various pumping stations along the route and on the ways to control each pump to keep the product flowing. Danyele, Dave, and Sterling found that the highly skilled controllers had developed a feel for the movement of product inside the pipeline. They had learned to “play” the pumping stations almost as one would play a musical instrument. Danyele, Dave, and Sterling built a set of decision exercises to present the operators with dilemmas. In one exercise, the technicians at one of the pumping stations had asked permission to shut their station down for 20 minutes for routine maintenance. This simple request, had the operators honored it immediately, would have reduced pressure down the line, triggering a chain reaction of pump shutdowns due to low pressure. In fact, the entire line was going to get shut down if that pump was turned off for… ([Location 4830](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4830))
- All these methods have some value in helping people shed outmoded mental models and learn new ways to think and to transform their mental models. Notice that each type of method for breaking free from fixation involves an activity. The pilots unlearn by actually flying differently in simulators or airplanes. The physics students construct a new understanding through various activities. Similarly, activities can help us understand the mental models of subordinates and teammates. We can watch them as they engage in activities. Many years ago, my colleagues and I prepared some decision-making games to help young Marine squad leaders get ready for an upcoming exercise. The company commander selected one of his three platoon leaders to run the squad leaders through these games. He left the other two platoon leaders free to work on other things. The platoon training officer assigned to run the games had a chance to watch how all the squad leaders thought about tough dilemmas. He observed the squad leaders in his own platoon as well as the ones from the other two platoons. Pretty soon the other two platoon leaders started showing up for the training. They noticed that these exercises were a rare opportunity to watch the thinking processes of their squad leaders. They could note the kinds of cues they noticed and the cues they missed, the kinds of actions they considered and the kinds they ignored. The decision exercises gave all the platoon leaders a unique opportunity to take stock of their squad leaders’ mental models. The storehouse metaphor isn’t wrong. It is useful for teaching ([Location 4891](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4891))
- By ‘myth’ I mean “a belief given uncritical acceptance by the members of a group, especially in support of existing or traditional practices.”1 For example, many organizations ([Location 5001](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5001))
    - **Tags:** [[orange]]
- processes to transform the initial conditions into the desired outcomes. “A hallmark of the process,” Norman and Kuras write, “is the ability to justify everything built in terms of the original requirements. If requirements change, it dislodges the careful scaffolding upon which the system rests. . . . The specific desired outcome must be known a priori, and it must ([Location 5045](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5045))
    - **Tags:** [[orange]]
- Contrary to the popular view of experts as walking encyclopedias, we would do better to regard experts as detectors. They have spent many hours tuning themselves to notice cues that are invisible to the rest of us.1 They can make discriminations that most of us can’t make. Just as we need special devices to detect radon in our homes, or radioactivity levels near a nuclear power plant, or oxygen levels in a stream, experts pick up cues, patterns, and trends that otherwise go undetected. ([Location 5115](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5115))
- as much as possible. This control-oriented mindset is best suited for well-ordered situations. In ambiguous situations, in the world of shadows, we aren’t going to be able to control everything ([Location 5135](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5135))
    - **Tags:** [[orange]]
- “People who are lost,” Syrotuck writes, “may experience different types of reactions. They may panic, become depressed, or suffer from ‘woods shock.’ Panic usually implies tearing around or thrashing through the brush, but in its earlier stages it is less frantic. Most lost people go through some of the stages. It all starts when they look about and find that a supposedly familiar location now appears strange, or when it seems to be taking longer to reach a particular place than they had expected. There is a tendency to hurry to ‘find the right place. . . . Maybe it’s just over that little ridge.’ If things get progressively more unfamiliar and mixed up, they may then develop a feeling of vertigo, the trees and slopes seem to be closing in and claustrophobia compels them to try to ‘break out.’ This is the point at which running or frantic scrambling may occur and indeed outright panic has occurred. Running is panic!” (p. 11) ([Location 5154](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5154))
- Now let’s take a different perspective on navigation, a recovery-oriented mindset rather than a follow-the-steps mindset. In traveling over complicated terrain, whether in the woods or in a city, we can assume that there is a reasonable chance that we’ll make some kind of error. Our recovery-oriented mindset is to look at a map to see where we are likely to get confused. How will we know that we have gotten off track? How can we find our way? Even if we don’t get lost, we may at least get bewildered. Think back to all the times you wondered if you had driven too far and wanted to turn around, only to come to the next landmark shortly thereafter. A map looks different when we adopt a recovery-oriented perspective. New features draw our attention—roads that will show when we have gone too far, streets that resemble the ones onto which we want to turn, confusion zones where we’re likely to go the wrong way. By reviewing a map this way, we may prevent some of the mistakes. But the point of the recovery-perspective exercise isn’t to eliminate mistakes. That’s the control mentality at work. Rather, the point is to assume that we will get disoriented. When disorientation happens, we want to understand the map to more easily recover from our mistakes. The notion of a recovery perspective grew out of a project I did in 1990 at Fort Campbell, in Kentucky, with Steve Wolf and Marvin Thordsen. We watched UH-60 Blackhawk helicopter teams in a high-fidelity simulator. Their mission for the exercise was to convey soldiers into drop zones deep in enemy territory. The helicopter pilots had to deliver the soldiers during a pre-determined time; artillery barrages would be suppressed during this time window so that the helicopters could safely land and then take off. The mission sounded simple when it was briefed to the teams. During their planning sessions, most of the teams concentrated on the checkpoints listed on their maps. They would start out flying north, following a road past the first checkpoint, which was where the road crossed a stream. The second checkpoint was at a T intersection where another road dead-ended into the road they were following. Then they would come to a range of hills and turn left. In all there were ten checkpoints on their way to the drop zone. The helicopter crews figured out the average flying speed they had to sustain. They even jotted down the time they would have to hit each of the checkpoints in order to get to the drop zone. It was like watching dispatchers file a schedule for a bus route. None of the missions followed the script. Only one of the ten crews made it to the drop zone during the period when artillery was suppressed. The others were usually too late but sometimes too early. All the helicopters were detected by enemy radar at some point and had missiles fired at them. The helicopters immediately went into evasive maneuvers and dropped below the tree lines to break the radar lock. Then they had to figure out where… ([Location 5179](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5179))
- Ford and Kraiger (1995) compiled a set of synonyms for mental models: knowledge structures, cognitive maps, and task schemata. ([Location 5313](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5313))
- Doyle and Ford 1998. ([Location 5315](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5315))
- Here are some of the major types of learning: Recognizing and applying analogies, reinforcing responses, classical conditioning to create associations, deduction, induction, imitation learning, episodic learning, and implicit learning. And here are some of the things we learn: Skills, connections (e.g., perceptual-motor), mental models, patterns, typicality, habituation, sensitization, categories, concepts, object recognition, language, metacognition (learning about oneself), instances (episodic learning), facts (declarative knowledge), attention management, spatial mapping, generalization and discrimination, tool learning, decentering (taking the perspective of someone else), emotional control, statistical and analytical methods, and sequencing of tasks. Thus, the term “learning” covers a lot of ground. People trying to create learning organizations might want to pin down what kind of learning they want to foster. ([Location 5501](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5501))



[Streetlights and Shadows\_ Searching for the Keys t.pdf](./resources/201705310826.5_Streetlights_and_Shadows__Searching_for_the_Keys_to_Adaptive_Decision_Making_(MIT_Press)_-_Gary_Klein.resources/Streetlights and Shadows_ Searching for the Keys t.pdf)[Streetlights and Shadows\_ Searching for the Keys t.1.epub](./resources/201705310826.5_Streetlights_and_Shadows__Searching_for_the_Keys_to_Adaptive_Decision_Making_(MIT_Press)_-_Gary_Klein.resources/Streetlights and Shadows_ Searching for the Keys t.1.epub)

---

_Created at 20170531._
_Last updated at 20170622._



References:

 
https://anystyle.io/  um Einträge zu parsen
Bibliography


- Anderson, R., and Smith, B. 2005. Deaths: Leading Causes for 2002. National Vital Statistics Reports 53, no. 17.

- Ariely, D. 2008. Predictably Irrational: The Hidden Forces That Shape Our Decisions. Harper-Collins.

- Aven, T. 2003. Foundations of Risk Analysis: A Knowledge and Decision-Oriented Perspective. Wiley.

- Babler, T., and Dannemiller, J. 1993. Role of Image Acceleration in Judging Landing Location of Free-Falling Projectiles. Journal of Experimental Psychology: Human Perception and Performance 19: 15–31.

- Bach, P., Jett, J., Pastorino, U., Tockman, M., Swensen, S., and Begg, C. 2007. Computed Tomography Screening and Lung Cancer Outcomes. JAMA 297: 953–961.

- Baker, N., Green, S., and Bean, A. 1986. Why R&D Projects Succeed or Fail. Research Management 29, no. 6: 29–34.

- Balzer, W., Doherty, M., and O’Connor, R. 1989. The Effects of Cognitive Feedback on Performance. Psychological Bulletin 106: 410– 433.

- Baxter, H., Harris-Thompson, D., and Phillips, J. 2004. Evaluating a Scenario-Based Training Approach for Enhancing Situation Awareness Skills. In Proceedings of Interservice/ Industry Training, Simulation, and Education Conference, Orlando.

- Bazerman, M. 2006. Judgment in Managerial Decision Making, sixth edition. Wiley.

- Beach, L. 1993. Broadening the Definition of Decision Making: The Role of Prechoice Screening of Options. Psychological Science 4,

- no. 4: 215–220.

- Beach, L., and Mitchell, T. 1978. A Contingency Model for the Selection of Decision Strategies. Academy of Management Review 3: 439–449.

- Bechara, A., Damasio, H., Tranel, D., and Damasio, A. 1997. Deciding Advantageously before Knowing the Advantageous Strategy. Science 275: 1293–1295.

- Bernstein, N. 1996. On Dexterity and Its Development. In Dexterity and Its Development, ed. M. Latash and M. Turvey. Erlbaum.

- Berri, D., Schmidt, M., and Brook, S. 2006. The Wages of Wins: Taking Measure of the Many Myths in Modern Sport. Stanford University Press.

- Blickensderfer, E., Cannon-Bowers, J., and Salas, E. 1998. Assessing Team Shared Knowledge: A Field Test and Implications for Team Training. Presented at 42nd Annual Meeting of Human Factors and Ergonomics Society, Chicago.

- Bonabeau, E. 2003. Don’t Trust Your Gut. Harvard Business Review, May: 116–123.

- Borges, J. 1962. Funes the Memorius. In Ficciones. Grove.

- Brafman, O., and Brafman, R. 2008. Sway: The Irresistible Pull of Irrational Behavior. Doubleday.

- Bransford, J., Brown, A., and Cocking, R. 2000. How People Learn: Brain, Mind, Experience, and School. National Academy Press.

- Brennan, S. 1998. The Grounding Problem in Conversations With and Through Computers. In Social and Cognitive Psychological Approaches to Interpersonal Communication, ed. S. Fussel and R. Kreuz. Erlbaum.

- Brown, D., and Clement, J. 1989. Overcoming Misconceptions via Analogical Reasoning: Abstract Transfer Versus Explanatory Model Construction. Instructional Science 18: 237–261.

- Brown, D., and Clement, J. 1992. Classroom Teaching Experiments in Mechanics. In Research in Physics Learning: Theoretical Issues and Empirical Studies, ed. R. Duit et al. Institut für die Pädagogik der Naturwissenschaftern an der Universitatät Kiel.

- Bruner, J., and Potter, M. 1964. Interference in Visual Recognition. Science 144: 424–425.

- Burns, B. 2004. The Effects of Speed on Skilled Chess Performance. Psychological Science 15: 442–447.

- Calderwood, R., Klein, G., and Crandall, B. 1988. American Journal of Psychology 101: 481–493.

- Campitelli, G., and Gobet, R. 2004. Adaptive Expert Decision Making: Skilled Chess Players Search More and Deeper. ICGA Journal 27, no. 4: 209–216.

- Canham, M., Hegarty, M., and Smallman, H. 2007. Using Complex Visual Displays: When Users Want More Than Is Good for Them. Paper presented at 8th International Conference on Naturalistic Decision Making, Pacific Grove, California.

- Ceci, S., and Liker, J. 1986. Academic and Nonacademic Intelligence: An Experimental

- Separation. In Everyday Intelligence: Origins of Competence, ed. R. Sternberg and R. Wagner. Cambridge University Press.

- Cesna, M., and Mosier, K. 2005. Using a Prediction Paradigm to Compare Levels of Expertise and Decision Making Among Critical Care Nurses. In How Professionals Make Decisions, ed. H. Montgomery et al. Erlbaum.

- Chase, W., and Simon, H. 1973. The Mind’s Eye in Chess. In Visual Information Processing, ed. W. Chase. Academic.

- Chi, M. 2006. Two Approaches to the Study of Experts’ Characteristics. In The Cambridge Handbook of Expertise and Expert Performance, ed. K. Ericsson et al. Cambridge University Press.

- Chi, M., Siler, S., Jeong, H., Yamauchi, T., and Hausmann, R. 2001. Learning from Human Tutoring. Cognitive Science 18: 439–477.

- Chinn, C., and Brewer, W. 1993. The Role of Anomalous Data in Knowledge Acquisition: A Theoretical Framework and Implications for Science Instruction. Review of Educational Research 63: 1–49.

- Clark, H. 1996. Using Language. Cambridge University Press.

- Cohen, L. 1981. Can Human Irrationality Be Experimentally Demonstrated? Behavioral and Brain Sciences 4: 317–370.

- Cohen, M. 1993. The Naturalistic Basis for Decision Biases. In Decision Making in Action: Models and Methods, ed. G. Klein et al. Ablex.

- Cohen, M., Freeman, J., and Thompson, B. 1997. Training the Naturalistic Decision Maker. In Naturalistic Decision Making, ed. C. Zsambok and G. Klein. Erlbaum.

- Cohen, M., Freeman, J. and Thompson, B. 1998. Critical Thinking Skills in Tactical Decision Making: A Model and a Training Method. In Decision-Making Under Stress: Implications for Training and Simulation, ed. J. Cannon-Bowers and E. Salas. American Psychological Association.

- Cohen, M., March, J., and Olsen, J. 1972. A Garbage Can Model of Organizational Choice. Administrative Science Quarterly 17, no. 1: 1–25.

- Cooke, N., and Durso, F. 2008. Stories of Modern Technology Failures and Cognitive Engineering Successes. Taylor and Francis.

- Cosmides, L., and Tooby, J. 1996. Are Humans Good Intuitive Statisticians After All? Rethinking Some Conclusions from the Literature on Judgment under Uncertainty. Cognition 58: 1–73.

- Cox, D., Long, W., Wiggins, S., Miller, T., and Stevens, L. 2006. Cognitive Evaluation of NWS FSI Prototype. Constraints on the Cognitive Performance of the Warning Weather Forecaster. Evaluation Report prepared for National Weather Service, Norman, Oklahoma. Klein Associates Division, Applied Research Associates.

- Crandall, B., and Gamblian, V. 1991. Guide to Early Sepsis Assessment in the NICU. Instruction manual prepared for Ohio Department of Development. Klein Associates.

- Crandall, B., Klein, G., and Hoffman, R. 2006. Working Minds: A Practitioner’s Guide to Cognitive Task Analysis. MIT Press.

- Damasio, A. 1994. Descartes’ Error. Putnam.

- Dawes, R. 2001. Everyday Irrationality: How Pseudo-Scientists, Lunatics, and the Rest of Us Systematically Fail to Think Rationally. Westview.

- Deakin, J., and Cobley, S. 2003. A Search for Deliberate Practice: An Examination of the Practice Environments in Figure Skating and Volleyball. In Expert Performance in Sport, ed. J. Starkes and K. Ericsson. Human Kinetics.

- deGroot, A. D. 1978. Thought and Choice in Chess. Mouton. Originally published in 1946.

- De Keyser, V., and Woods, D. 1993. Fixation Errors: Failures to Revise Situation Assessment in Dynamic and Risky Systems. In Advanced Systems in Reliability Modeling, ed. A. Colombo and A. Saiz de Bustamente. Kluwer.

- Dekker, S. 2003. When Human Error Becomes a

- Crime. Human Factors and Aerospace Safety 3, no. 1: 83–92.

- DiBello, L. 2001. Solving the Problem of Employee Resistance to Technology by Reframing the Problem as One of Experts and Their Tools. In Linking Expertise and Naturalistic Decision Making, ed. E. Salas and G. Klein. Erlbaum.

- Dijksterhuis, A., Bos, M., Nordgren, L., and van Baaren, R. 2006. On Making the Right Choice: The Deliberation-Without-Attention Effect. Science 311: 1005–1007.

- Dobbs, M. 2008. One Minute to Midnight: Kennedy, Khrushchev, and Castro on the Brink of Nuclear War. Knopf.

- Dörner, D. 1996. The Logic of Failure. Perseus.

- [^21]: Doyle, J., and Ford, D. 1998. Mental Models Concepts for System Dynamics Research. System Dynamics Review 14, no. 1: 3–29.

- Dreyfus, H. 1972. What Computers Can’t Do: A Critique of Artificial Intelligence. MIT Press.

- Dreyfus, H. 1997. Intuitive, Deliberative, and Calculative Models of Expert Performance. In Naturalistic Decision Making, ed. C. Zsambok and G. Klein. Erlbaum.

- Dreyfus, H., and Dreyfus, S. 1986. Mind over Machine: The Powers of Human Intuition and Expertise in the Era of the Computer. Free Press.

- Duggan, W. 2007. Strategic Intuition: The Creative Spark in Human Achievement. Columbia Business School Publishing.

- Early, C., Northcraft, G., Lee, C., and Lituchy, T. 1990. Impact of Process and Outcome Feedback on the Relation of Goal Setting to Task Performance. Academy of Management Journal 33: 87–105.

- Edwards, W., and Fasolo, B. 2001. Decision Technology. Annual Review of Psychology 52: 581–606.

- Ellis, C. 2006. Joe Wilson and the Creation of Xerox. Wiley.

- Elstein, A., Shulman, L., and Sprafka, S. 1978. Medical Problem Solving: An Analysis of Clinical Reasoning. Harvard University Press.

- Endsley, M. 1995. Toward a Theory of Situation Awareness in Dynamic Systems. Human Factors 37, no. 1: 32–64.

- Endsley, M., Bolte, B., and Jones, D. 2003. Designing for Situation Awareness: An Approach to Human-Centered Design. Taylor and Francis.

- Epstein, S. 1994. Integration of the Cognitive and Psychodynamic Unconscious. American Psychologist 49: 709–724.

- Ettenson, R., Shanteau, J., and Krogstad, J. 1987. Expert Judgment: Is More Information Better? Psychological Reports 60: 227–238.

- Euler, E., Jolly, S., and Curtis, H. 2001. The Failures of the Mars Climate Orbiter and Mars Polar Lander: A Perspective from the People Involved. In Proceedings of Guidance and Control 2001, American Astronautical Society.

- Evans, J. 2008. Dual-Processing Accounts of Reasoning, Judgment and Social Cognition. Annual Review of Psychology 59: 255–278. Faragher, J. 1992. Daniel Boone: The Life and Legend of an American Pioneer. Holt.

- Feltovich, P., Coulson, R., and Spiro, R. 2001. Learners’ (Mis)Understanding of Important and Difficult Concepts: A Challenge to Smart Machines in Education. In Smart Machines in Education, ed. K. Forbus and P. Feltovich. AAAI/MIT Press.

- Feltovich, P., Johnson, P., Moller, J., and Swanson, D. 1984. LCS: The Role and Development of Medical Knowledge in Diagnostic Expertise. In Readings in Medical Artificial Intelligence: The First Decade, ed. W. Clancey and E. Shortliffe. Addison-Wesley.

- Feltovich, P., Spiro, R., and Coulson, R. 1993. The Nature of Conceptual Understanding in Biomedicine: The Deep Structure of Complex Ideas and the Development of Misconceptions. In Cognitive Science in Medicine: Biomedical Modeling, ed. D. Evans and V. Patel. MIT Press.

- Feynman, R. 1988. An Outsider’s Inside View of the Challenger Inquiry. Physics Today, February: 26–37.

- Fischhoff, B. 1982. Debiasing. In Judgment under Uncertainty: Heuristics and Biases, ed. D. Kahneman et al. Cambridge University Press.

- Fischhoff, B. 2005. Risk Perception and Communication. In McGraw-Hill Handbook of Terrorism and Counter-Terrorism, ed. D. Kamien. McGraw-Hill.

- Flin, R., and Maran, N. 2004. Identifying and Training Non-Technical Skills for Teams in Acute Medicine. Quality and Safety in Health Care 13: 80–84.

- [^20]: Ford, J., and Kraiger, K. 1995. The Application of Cognitive Constructs to the Instructional Systems Model of Training: Implications for Needs Assessment, Design, and Transfer. International Review of Industrial and Organizational Psychology 10: 1–48.

- Foushee, H., and Helmreich, R. 1988. Group Interaction and Flight Crew Performance. In Human Factors in Aviation, ed. E. Wiener and D. Nagel. Academic.

- Frederick, S. 2005. Cognitive Reflection and Decision Making. Journal of Economic Perspectives 19: 25–42.

- Friedman, E., Treadwell, M., and Beal, E. 2007. A Failure of Nerve: Leadership in the Age of the Quick Fix. Seabury.

- Galdi, S., Arcuri, L., and Gawronski, B. 2008. Automatic Mental Associates Predict Future Choices of Undecided Decision-Makers. Science 321: 1100–1102.

- Gallwey, W. 1974. The Inner Game of Tennis. Random House.

- Garg, A., Adhikari, N., McDonald, H., Rosas- Arellano, M., Devereaux, P., Beyene, J., Sam, J., and Haynes, R. 2005. Effects of Computerized Clinical Decision Support Systems on Practitioner Performance and Patient Outcomes: A Systematic review. JAMA 293: 1197–1203.

- Gawande, A. 2007a. The Way We Age Now. The New Yorker, April 30: 50–59.

- Gawande, A. 2007b. The Checklist. The New Yorker, December 10: 86–95.

- Gawande, A. 2007c. A Lifesaving Checklist. New York Times, December 30, 2007.

- George, A., and Stern, E. 2002. Harnessing Conflict in Foreign Policy Making: From Devil’s to Multiple Advocacy. Presidential Studies Quarterly, September: 484–508.

- Gigerenzer, G. 1991. How to Make Cognitive Illusions Disappear: Beyond “Heuristics and Biases.” In European Review of Social Psychology, volume 2, ed. W. Stroebe and M. Hewstone. Wiley.

- Gigerenzer, G. 2005. I Think, Therefore I Err. Social Research 72: 195–218.

- Gigerenzer, G., Hoffrage, U., and Ebert, A. 1998. AIDS Counseling for Low Risk Clients. AIDS Care 10, no. 2: 197–211.

- Gilovich, T. 1991. How We Know What Isn’t So: The Fallibility of Human Reasoning in Everyday Life. Free Press.

- Gilovich, T., Griffin, D., and Kahneman, D. 2002. Heuristics and Biases: The Psychology of Intuitive Judgment. Cambridge University Press.

- Gladwell, M. 2005. Blink. Little, Brown.

- Gladwell, M. 2007. Open secrets: Enron, Intelligence, and the Perils of Too Much Information. The New Yorker, January 8: 44–53.

- Gladwell, M. 2008. Outliers. Little, Brown.

- Goldstein, D., and Gigerenzer, G. 1999. The Recognition Heuristic: How Ignorance Makes Us Smart. In Simple Heuristics That Make Us Smart, ed. G. Gigerenzer et al. Oxford University Press.

- Greenwood, R. 1981. Management by Objectives: As Developed by Peter Drucker, Assisted by Harold Smiddy. Academy of Management Review 6, no. 2: 225–230.

- Grove, W., Zald, D., Lebvow, B., Snitz, B., and Nelson, C. 2000. Clinical versus Mechanical Prediction: A Meta-Analysis. Psychological Assessment 12: 19–30.

- Halberda, J., Mazzocco, M., and Feigenson, L. 2008. Individual Differences in Non-Verbal Number Acuity Correlate with Maths Achievement. Nature 455, no. 2: 665–669.

- Hall, C., Ariss, L., and Todorov, A. 2007. The Illusion of Knowledge: When More Information Reduces Accuracy and Increases Confidence. Organizational Behavior and Human Decision Processes 103: 277–290.

- Harris-Thompson, D., and Wiggins, S. 2007 When SOP Is Not Enough. Law Officer Magazine, May: 24–26.

- Hawkins, J., and Blakeslee, S. 2004. On Intelligence. Henry Holt.

- Haynes, A., Weiser, T., Berry, W., Lipsitz, S., Breizat, A., Dellinger, E., Herbosa, T., Joseph, S., Kibatala, P., Lapitan, M., Merry, A., Moorthy, K., Reznick, R., Taylor, B., and Gawande, A. 2009. A Surgical Safety Checklist to Reduce Morbidity and Mortality in a Global Population. New England Journal of Medicine 360, no. 5: 491–499.

- Hersh, S. 2002. Missed Messages: Why the Government Didn’t Know What It Knew. The New Yorker, June 3: 40–48.

- Hertwig, R., and Gigerenzer, G. 1999. The “Conjunction Fallacy” Revisited: How Intelligent Inferences Look Like Reasoning Errors. Journal of Behavioral Decision Making 12, no. 4: 275– 305.

- Heuer, R. 1999. Psychology of Intelligence Analysis. Central Intelligence Agency.

- Hockey, G., Sauer, J., and Wastell, D. 2007. Adaptability of Training in Simulated Process Control: Knowledge Versus Rule-Based Guidance under Task Changes and Environmental Stress. Human Factors 49: 158–174. ^a3a220
	- [](zotero://select/library/items/WH3FXQ4R)


- Hoffer, W., and Hoffer, M. 1989. Freefall: 41,000 Feet and Out of Fuel—A True Story. St. Martin’s.

- Hoffman, D. 1999. I Had a Funny Feeling in My Gut. Washington Post Foreign Service, February 10.

- Hoffman, R., and Nead, J. 1983. General Contextualism, Ecological Science and Cognitive Research. Journal of Mind and Behavior 4: 507– 560.

- Hoffrage, U., Lindsey, S., Hertwig, R., and Gigerenzer, G. 2000. Communicating Statistical Information. Science 290: 2261–2262.

- Jacobson, G., and Hillkirk, J. 1986. Xerox: American Samurai. MacMillan

- James, W. 1936. The Varieties of Religious Experience. First Modern Library edition. Original copyright 1902.

- Jamieson, G., and Miller, C. 2000. Exploring the “Culture of Procedures.” In Proceedings of 5th International Conference on Human Interaction with Complex Systems, Urbana, Illinois.

- Johnson, D., Perlow, R., and Piper, K. 1993. Differences in Team Performance as a Function of Type of Feedback: Learning Oriented Versus Performance Oriented Feedback. Journal of Applied Psychology 23: 303–320.

- Johnson, J., Driskell, J., and Salas, E. 1997. Vigilant and Hypervigilant Decision Making. Journal of Applied Psychology 82, no. 4: 614– 622.

- Johnson, J., and Raab, M. 2003. Take the First: Option-Generation and Resulting Choices. Organizational Behavior and Human Decision Processes 91, no. 2: 215–229.

- Johnston, N. The Paradox of Rules: Procedural Drift in Commercial aviation. Unpublished manuscript.

- Kahn, D. 1991–92. The Intelligence Failure of Pearl Harbor. Foreign Affairs, winter: 138–152.

- Kahneman, D., and Klein, G. In press. Conditions for Intuitive Expertise: A Failure to Disagree. American Psychologist.

- Kahneman, D., Slovic, P., and Tversky, A. 1982. Judgment under Uncertainty: Heuristics and Biases. Cambridge University Press.

- Kahneman, D., and Tversky, A. 1982. On the Study of Statistical Intuitions. Cognition 11: 123–141.

- Kaiser, R., Hogan, R., and Craig, S. 2008. Leadership and the Fate of Organizations. American Psychologist 63: 96–110.

- Kauffmann, B. 2005. Unsung Hero Saves the Day at Midway. Dayton Daily News, June 4.

- Kelley, T., and Littman, J. 2005. The Ten Faces of Innovation: IDEO’s Strategies for Defeating the Devil’s Advocate and Driving Creativity Throughout Your Organization. Doubleday Currency.

- Klein, D., and Murphy, G. 2001. The Representation of Polysemous Words. Journal of Memory and Language 45, no. 2: 259–282.

- Klein, G. 1979. User Guides: Theoretical Guidelines for their Use. Paper presented at American Psychological Association Meeting, New York.

- Klein, G. 1989. Do Decision Biases Explain Too Much? Human Factors Society Bulletin 23, no. 5: 1–3.

- Klein, G. 1998. Sources of Power: How People Make Decisions. MIT Press.

- Klein, G. 2001. Features of Team Coordination. In New Trends in Cooperative Activities, ed. M. McNeese et al. Human Factors and Ergonomics Society.

- Klein, G. 2004. The Power of Intuition. Doubleday/Currency.

- Klein, G. 2007a. Performing a Project Premortem. Harvard Business Review, September: 18–19.

- Klein, G. 2007b. Flexecution as a Paradigm for Replanning, Part 1. IEEE Intelligent Systems 22, no. 5: 79–83.

- Klein, G. 2007c. Flexecution, Part 2: Understanding and Supporting Flexible Execution. IEEE Intelligent Systems 22, no. 6: 108–112.

- Klein, G. 2007d. Corruption and Recovery of Sensemaking During Navigation. In Decision Making in Complex Environments, ed. M. Cook et al. Ashgate.

- Klein, G., Armstrong, A., Woods, D., Gokulachandra, M., and Klein, H. A. 2000. Cognitive Wavelength: The Role of Common Ground in Distributed Replanning. Technical Report AFRL-HE-WP-TR-2001-0029, US Air Force Research Laboratory, Wright-Patterson Air Force Base, Ohio.

- Klein, G., and Baxter, H. 2009. Cognitive Transformation Theory: Contrasting Cognitive and Behavioral Learning. In The PSI Handbook of Virtual Environments for Training and Education, volume 1, ed. D. Schmorrow et al. Praeger Security International.

- Klein, G., Calderwood, R., and Clinton-Cirocco, A. 1986. Rapid Decision Making on the Fireground. In Proceedings of the Human Factors and Ergonomics Society 30th Annual Meeting.

- Klein, G., Feltovich, P., Bradshaw, J., and Woods, D. 2004. Common Ground and Coordination in Joint Activity. In Organizational Simulation, ed. W. Rouse and K. Boff. Wiley.

- Klein, G., and Hoffman, R. 1993. Seeing the Invisible: Perceptual/Cognitive Aspects of Expertise. In Cognitive Science Foundations of Instruction, ed. M. Rabinowitz. Erlbaum.

- Klein, G., Phillips, J., Rall, E., and Peluso, D. 2007. A Data/Frame Theory of Sensemaking. In Expertise Out of Context, ed. R. Hoffman. Erlbaum.

- Klein, G., Pliske, R., Crandall, B., and Woods, D. 2005. Problem Detection. Cognition, Technology & Work 7: 14–28.

- Klein, G., Ross, K., Moon, B., Klein, D. E., Hoffman, R., and Hollnagel, E. 2003. Macrocognition. IEEE Intelligent Systems 18, no. 3: 81–85.

- Klein, G., and Rothman, J. 2008. Staying on Course When Your Destination Keeps Changing. Conference Board Review, November-December: 24–27.

- Klein, G., and Weitzenfeld, J. 1978. Improvement of Skills for Solving Ill-Defined Problems. Educational Psychologist 13: 31–41.

- Klein, G., and Weitzenfeld, J. 1982. The Use of Analogues in Comparability Analysis. Applied Ergonomics 13: 99–104.

- Klein, H. A., and Lippa, K. 2008. Type II Diabetes Self-Management: Controlling a Dynamic System. Journal of Cognitive Engineering and Decision Making 2: 48–62.

- Klein, H. A., and McHugh, A. 2005. National Differences in Teamwork. In Organizational Simulation, ed. W. Rouse and K. Boff. Wiley.

- Klein, H. A., and Meininger, A. 2004. Self Management of Medication and Diabetes: Cognitive Control. IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans 34: 718–725.

- Kontogiannis, T. 2000. The Effect of Training Systemic Information on the Acquisition and Transfer of Fault-Finding Skills. International Journal of Cognitive Ergonomics 4: 243–267.

- Kruger, J., Wirtz, D., and Miller, D. 2005. Counterfactual Thinking and the First Instinct Fallacy. Journal of Personality and Social Psychology 88: 725–735.

- Kurtz, C. F., and Snowden, D. J. 2003. The New Dynamics of Strategy: Sensemaking in a Complex and Complicated World. e-Business Management 42, no. 3.

- Lambe, P. 2007. Organising Knowledge: Taxonomies, Knowledge and Organizational Effectiveness. Chandos.

- Langer, E. 1989. Mindfulness. Addison-Wesley.

- Lanir, Z. 1983. Fundamental Surprise: The National Intelligence Crisis. Center for Strategic Studies, Tel Aviv University.

- Latash, M. 1996. The Bernstein Problem: How Does the Central Nervous System Make Its Choices? In Dexterity and Its Development, ed. M. Latash and M. Turvey. Erlbaum.

- Laufer, A. 2009. Breaking the Code of Project Management. Palgrave Macmillan.

- Lewis, M. 2003. Moneyball: The Art of Winning an Unfair Game. Norton.

- Lippa, K., and Klein, H. 2008. Portraits of Patient Cognition: How Patients Understand Diabetes Self-Care. Canadian Journal of Nursing Research 40: 80–95.

- Lipshitz, R. 1997. Coping with Uncertainty: Beyond the Reduce, Quantify and Plug Heuristic. In Decision Making under Stress: Emerging Themes and Applications, ed. R. Flin et al. Ashgate.

- Lipshitz, R., and Ben Shaul, O. 1997. Schemata and Mental Models in Recognition-Primed Decision Making. In Naturalistic Decision Making, ed. C. Zsambok and G. Klein. Erlbaum.

- Lipshitz, R., and Strauss, O. 1997. Coping with Uncertainty: A Naturalistic Decision-Making Analysis. Organizational Behavior and Human Decision Processes 69: 149–163.

- Lopes, L. 1991. The Rhetoric of Irrationality. Theory and Psychology 1, no. 1: 65–82.

- Lovallo, D., and Kahneman, D. 2003. Delusions of Success. How Optimism Undermines

- Executives’ Decisions. Harvard Business Review 81, no. 7: 56–63.

- Lowenstein, R. 2000. When Genius Failed: The Rise and Fall of Long-Term Capital Management. Random House.

- LuperFoy, S. 1995. Naturalistic Dialogs with Artificial Agents: Why I Lie to My Toaster. Paper presented at MIT Language, Cognition, and Computation Seminar.

- Macey, J. 2003. A Pox on Both Your Houses: Enron, Sarbanes-Oxley and the Debate Concerning the Relative Efficiency of Mandatory Versus Enabling Rules. Washington University Law Quarterly 81: 329–355.

- MacGregor, J., Ormerod, T., and Chronicle, E. 2000. A Model of Human Performance on the Traveling Salesperson Problem. Memory and Cognition 28: 1183–1190.

- Manos, D. 2004. Majority of Diabetics Do Not Follow Guidelines for Maintaining Health, Despite a Decade of Warning, Study Shows.

- Medical Guidelines and Outcomes Research 15: 1–6.

- Mauboussin, M. 2007. More Than You Know: Finding Financial Wisdom in Unconventional Places. Columbia University Press.

- McCarthy, P., Ball, D., and Purcell, W. 2007. Project Phoenix—Optimizing the Machine-Person Mix in High-Impact Weather Forecasting. Presented at 22nd American Meteorological Society Conference on Weather Analysis and Forecasting, Park City, Utah.

- [^16]:  McLeod, P., and Dienes, Z. 1996. Do Fielders Know Where to Go to Catch the Ball or Only How to Get There? Journal of Experimental Psychology: Human Perception and Performance 22: 531–543.

- McNeil, B., Pauker, S., Sox, H., and Tversky, A. 1982. On the Elicitation of Preferences for Alternative Therapies. New England Journal of Medicine 306, no. 21: 1259–1262.

- Medin, D., Lynch, E., Coley, J., and Atran, S. 1997. Categorization and Reasoning Among Tree Experts: Do All Roads Lead to Rome? Cognitive Psychology 32: 49–96.

- Meehl, P. 1954. Clinical vs. Statistical Prediction: A Theoretical Analysis and a Review of the Evidence. University of Minnesota Press.

- Minsky, M. 1986. The Society of Mind. Simon and Schuster.

- Mintzberg, H. 1994. The Rise and Fall of Strategic Planning. Free Press.

- Mitchell, D., Russo, J., and Pennington, N. 1989. Back to the Future: Temporal Perspective in the Explanation of Events. Journal of Behavioral Decision Making 2: 25–38.

- Mlodinow, L. 2008. The Drunkard’s Walk: How Randomness Rules Our Lives. Pantheon.

- Morris, N., and Rouse, W. 1985. Review and Evaluation of Empirical Research in Troubleshooting. Human Factors 27: 503–530.

- Mueller, S., and Minnery, B. 2008. Adapting the Turing Test for Embodied Neurocognitive Evaluation of Biologically Inspired Cognitive Agents. Presented at AAAI symposium.



- National Commission on Terrorist Attacks Upon the United States. 2004. The 9/11 Commission Report.

- Neace, W., Michaud, S., Bolling, L., Deer, K., and Zecevic, L. 2008. Frequency Formats, Probability Formats, or Problem Structure? A Test of the Nest-Sets Hypothesis in an Extensional Reasoning Task. Judgment and Decision Making 3, no. 2: 140–152.

- Nemeth, C., Brown, K., and Rogers, J. 2001. Devil’s Advocate Versus Authentic Dissent: Stimulating Quantity and Quality. European Journal of Social Psychology 31: 707–720.

- Nemeth, C., Connell, J., Rogers, J., and Brown, K. 2001. Improving Decision Making by Means of Dissent. Journal of Applied Social Psychology 31: 48–58.

- Newell, B., Wong, K., Cheung, J., and Rakow, T. 2009. Think, Blink or Sleep on It? The Impact of

- Modes of Thought on Complex Decision Making. Quarterly Journal of Experimental Psychology 62, no. 4: 707–732.

- Nocera, J. 2009. Risk Mismanagement. New York Times Magazine, January 4.

- Nonaka, I., and Takeuchi, H. 1995. The Knowledge Creating Company. Oxford University Press.

- Norman, D. 1988. The Design of Everyday Things. Doubleday.

- Norman, D. 1993. Things That Make Us Smart: Defending Human Attributes in the Age of the Machine. Addison-Wesley.

- Norman, D., and Kuras, M. 2006. Engineering Complex Systems. In Complex Engineered Systems: Science Meets Technology, ed. D. Braha et al. Springer.

- Northcraft, G., and Neale, M. 1987. Experts, Amateurs, and Real Estate: An Anchoring-and- Adjustment Perspective on Property Pricing Decisions. Organizational Behavior and Human Decision Processes 39: 84–97.

- Nygren, T., and White, R. 2002. Assessing Individual Differences in Decision Making Styles: Analytical vs. Intuitive. In Proceedings of 46th Annual Meeting of Human Factors and Ergonomics Society.

- O’Brian, P . 1970. Master and Commander . William Collins.

- O’Brian, P. 1972. Post Captain. William Collins. O’Brian, P. 1973. H.M.S. Surprise. William

- Collins.

- O’Brian, P . 1977. The Mauritius Command. William Collins.

- O’Brian, P. 1983. Treason’s Harbour. William Collins.

- O’Conner, A., Rostom, A., Fiset, V., Tetroe, J., Entwistle, V ., Llewellyn-Thomas, H., Holmes- Rovner, M., Barry, M., and Jones, J. 1999. Decision Aids for Patients Facing Health Treatment or Screening Decisions: Systematic Review. British Medical Journal 319: 731–734.

- Olby, R. 1974. The Path to the Double Helix: The Discovery of DNA. University of Washington Press.

- Omodei, M., McLennan, J., Elliott, G., Wearing, A., and Clancy, J. 2005. “More Is Better?”: A Bias toward Overuse of Resources in Naturalistic Decision-Making Settings. In How Professionals Make Decisions, ed. H. Montgomery et al. Erlbaum.

- Orasanu, J. 1994. Shared Problem Models and Flight Crew Performance. In Aviation Psychology in Practice, ed. N. Johnston et al. Ashgate.

- Orasanu, J., and Connolly, T. 1993. The Reinvention of Decision Making. In Decision Making in Action: Models and Methods, ed. G. Klein et al. Ablex.

- Orr, J. 1990. Sharing Knowledge, Celebrating Identity: Community Memory in a Service Culture. In Collective Remembering, ed. D. Middleston and D. Edwards. Sage.
 ^8b5d9b
- Oskamp, S. 1965. Overconfidence in Case Study Judgments. Journal of Consulting Psychology 29: 61–265.

- Parker, M. 2007. Panama Fever. Doubleday.

- Patrick, J., and Haines, B. 1988. Training and Transfer of Fault-Finding Skill. Ergonomics 31: 193–210.

- Patterson, E. 2008. Editorial: Structuring flexibility: The Potential Good, Bad, and Ugly in Standardisation of Handovers. Quality and Safety in Healthcare 17, no. 1: 4–5.

- Patterson, E., Watts-Perotti, J., and Woods, D. 1999. Voice Loops as Coordination Aids in Space Shuttle Mission Control. Computer Supported Cooperative Work 8, no. 4: 3535–371.

- Patton, M. 1978. Utilization-Focused Evaluation. Sage.

- Perrow, C. 1984. Normal Accidents. Basic Books.

- Phelps, R., and Shanteau, J. 1978. Livestock Judges: How Much Information Can an Expert Use? Organizational Behavior and Human Performance 21: 209–219.

- Pizlo, Z., Rosenfeld, A., and Epelboim, J. 1995. An Exponential Pyramid Model of the Time- Course of Size Processing. Vision Research 35: 1089–1107.

- Plous, S. 1993. The Psychology of Judgment and Decision Making. McGraw-Hill.

- Polanyi, M. 1958. Personal Knowledge. University of Chicago Press.

- Polanyi, M. 1967. The Tacit Dimension. Doubleday.
 ^6f0d17
- Popper, K. 1959. The Logic of Scientific Discovery. Basic Books.

- Potchen, E. 2006. Measuring Observer Performance in Chest Radiology: Some Experiences. Journal of the American College of Radiology 3: 423–432.

- Pradhan, A., Hammel, K., DeRamus, R., Pollatsek, A., Noyce, D., and Fisher, D. 2005. Using Eye Movements to Evaluate Effects of Driver Age on Risk Perception in a Driving Simulator. Human Factors 47: 840–852.

- Prange, G. 1981. At Dawn We Slept: The Untold Story of Pearl Harbor. Penguin.

- Pronovost, P ., Needham, D., Berenholtz, S., Sinopoli, D., Chu, H., Cosgrove, S., Sexton, B., Hyzy, R., Welsh, R., Roth, G., Bander, J., Kepros, J., and Goeschel, C. 2006. An Intervention to Decrease Catheter-Related Bloodstream Infections in the ICU. New England Journal of Medicine 355, no. 26: 2725–2732.

- Raiffa, H. 1968. Decision Analysis: Introductory Lectures on Choices under Uncertainty. Addison- Wesley.

- Rasmussen, J. 1990. The Role of Error in Organizing Behavior. Ergonomics 33: 1185–1199.

- Reason, J. 1990. Human Error. Cambridge University Press.

- Rickenbacker, E. 1919. Fighting the Flying Circus. Lippincott.

- Rittel, H., and Webber, M. 1973. Dilemmas in a General Theory of Planning. Policy Sciences 4: 155–169.

- Rittel, H., and Webber, M. 1984. Planning Problems Are Wicked Problems. In Developments in Design Methodology, ed. N. Cross. Wiley.

- Ross, K., Klein, G., Thunholm, P., Schmitt, J., and Baxter, H. 2004. The Recognition-Primed Decision Model. Military Review 74, no. 4: 6–10.

- Rothman, J. 2006. Identity and Conflict: Collaboratively Addressing Police-Community Conflict in Cincinnati, Ohio. Ohio State Journal on Dispute Resolution 22, no. 1: 105–132.

- Routley, J., Jennings, C., and Chubb, M. Undated. High-Rise Office Building Fire: One Meridian Plaza, Philadelphia, Pennsylvania. National Fire Data Center, Federal Emergency Management Agency.

- Rudolph, J. 2003. Into the Big Muddy and Out Again: Error Persistence and Crisis Management in the Operating Room. Dissertation, Boston College.

- Rudolph, J., Morrison, J., and Carroll, J. In press. The Dynamics of Action-Oriented Problem

- Solving: Linking Interpretation and Choice. Academy of Management Review.

- Russo, J., and Schoemaker, P. 1989. Decision Traps: The Ten Barriers to Brilliant Decision- Making and How to Overcome Them. Doubleday.

- Ryle, G. 1949. The Concept of Mind. University of Chicago Press, 1984.

- Sauer, J., Hockey, G., and Wastell, D. 2000. Effects of Training on Short-and Long-Term Skill Retention in a Complex Multi-Task Environment. Ergonomics 43: 2043–2064.

- Sawyer, K. 1999. Mystery of Orbiter Crash Solved. Washington Post, October 1.

- Saxberg, B. 1987. Projected Free Fall Trajectories: I. Theory and Simulation. Biological Cybernetics 56: 159–175.

- Schacter, D. 2001. The Seven Sins of Memory: How the Mind Forgets and Remembers. Houghton Mifflin.

- Schmidt, R., and Wulf, G. 1997. Continuous Concurrent Feedback Degrades Skill Learning: Implications for Training and Simulation. Human Factors 39: 509–525.

- Schmitt, J., and Klein, G. 1996. Fighting in the Fog: Dealing with Battlefield Uncertainty. Marine Corps Gazette, August: 62–69.

- Schwartz, B. 2004. The Paradox of Choice: Why More Is Less. HarperCollins.

- Self, N. 2008. Two Wars: One Hero’s Flight on Two Fronts—Abroad and Within. Tyndale House.

- Seligman, M. 2002. Authentic Happiness: Using the New Positive Psychology to Realize Your Potential for Lasting Fulfillment. Simon and Schuster.

- Sengupta, K., Abdel-Hamed, T., and Van Wassenhove, L. 2007. The Experience Trap. Harvard Business Review, February: 94–101.

- Shafir, E. 1993. Choosing and Rejecting: Why Some Options Are Both Better and Worse. Memory and Cognition 21: 546–556.

- Shalin, V ., and V erdile, C. 2003. The Identification of Knowledge Content and Function in Manual Labor. Ergonomics 46, no. 7: 695–713.

- Shanteau, J. 1992. Competence in Experts: The Role of Task Characteristics. Organizational Behavior and Human Decision Processes 53: 252–262.

- Shapira, Z. 1995. Risk Taking: A Managerial Perspective. Russell Sage Foundation.

- Silberberg, E., and Suen, W. 2001. The Structure of Economics: A Mathematical Analysis, third edition. Irwin/McGraw-Hill.

- Sills, D. 2009. On the MSC Forecaster Forums and the Future Role of the Human Forecaster. Bulletin of the American Meteoreological Society.

- Simon, H. A. 1957. Models of Man: Social and Rational. Wiley.

- Simon, H. 1992. What Is an Explanation of Behavior? Psychological Science 3: 150–161.

- Skinner, B. F. 1948. Walden Two. Macmillan.

- Skitka, L., Mosier, K., and Burdick, M. 2000. Accountability and Automation Bias. International Journal of Human-Computer Studies 52: 701–717.

- Sloman, S. 1996. The Empirical Case for Two Systems of Reasoning. Psychological Bulletin 119: 3–22.

- Sloman, S., Over, D., Slovak, L., and Stibel, J. 2003. Frequency Illusions and Other Fallacies. Organizational Behavior and Human Decision Processes 91: 296–309.

- Slovic, P. 1995. The Construction of Preference. American Psychologist 50, no. 5: 364–371.

- Smallman, H., and Hegarty, M. 2007. Expertise, Spatial Ability and Intuition in the Use of Complex Displays. Paper presented at 51st Annual Conference of Human Factors and Ergonomics Society, Baltimore.

- Smith, J., and Kida, T. 1991. Heuristics and Biases: Expertise and Task Realism in Auditing. Psychological Bulletin 109: 472–489.

- Smith, P., Woods, D., McCoy, E., Billings, C., Sarter, N., and Dekker, S. 1998. Using Forecasts of Future Incidents to Evaluate Future ATM System Designs. Air Traffic Control Quarterly 6, no. 1: 71–85.

- Snellman, L. 1977. Operational Forecasting Using Automated Guidance. Bulletin of the American Meteorological Society 58: 1036–1044.

- Snook, S. 2000. Friendly Fire: The Accidental Shootdown of US Black Hawks over Northern Iraq. Princeton University Press.

- Snowden, D., Klein, G., Chew, L., and Teh, C. 2007. A Sensemaking Experiment: Techniques to Achieve Cognitive Precision. In Proceedings of 12th International Command and Control Research and Technology Symposium.

- Staszewski, J. 2008. Cognitive Engineering Based on Expert Skill: Notes on Success and Surprises. In Naturalistic Decision Making and Macrocognition, ed. J. Schraagen et al. Ashgate.

- Stewart, S. 1992. Emergency: Crisis on the Flight Deck. Airlife.

- Stewart, T., Moninger, W., Heideman, K., and

- Reagan-Cirincione, P. 1993. Effects of Improved Information on the Components of Skill in Weather Forecasting. Organizational Behavior and Human Decision Processes 53: 107–134.

- Stewart, T., Roebber, P., and Bosart, L. 1997. The Importance of the Task in Analyzing Expert Judgment. Organizational Behavior and Human Decision Processes 69: 205–219.

- Strack, F., and Mussweiler, T. 1997. Explaining the Enigmatic Anchoring Effect: Mechanisms of Selective Accessibility. Journal of Personality and Social Psychology 73: 437–446.

- Sutcliffe, K., and Weick, K. 2008. Information Overload Revisited. In The Oxford Handbook of Organizational Decision Making, ed. G. Hodgkinson and W. Starbuck. Oxford University Press.

- Syrotuck, W. 1976. Analysis of Lost Person Behavior: An Aid to Search Planning. Barkleigh Productions.

- Taleb, N. 2007. The Black Swan: The Impact of the Highly Improbable. Random House.

- Tetlock, P. 2005. Expert Political Judgment: How Good Is It? How Can We Know? Princeton University Press.

- Thaler, R., and Sunstein, C. 2008. Nudge: Improving Decisions about Health, Wealth, and Happiness. Yale University Press.

- Thunholm, P. 2005. Planning Under Time Pressure: An Attempt Toward a Prescriptive Model of Military Tactical Decision Making. In How Professionals Make Decisions, ed. H. Montgomery et al.. Erlbaum.

- Thunholm, P. 2007. Militär genomförandeledning —Vad händer när det oväntade inträffar? [Commanding Execution—What happens when the unexpected occurs?]. Publication 1525/7:1, Swedish National Defence College.

- Treverton, G. 2001. Reshaping National Intelligence for an Age of Information. Cambridge University Press.

- Tversky, A., and Kahneman, D. 1974. Judgment under Uncertainty: Heuristics and Biases. Science 185: 1124–1130.

- Tversky, A., and Kahneman, D. 1982. Judgments of and by Representativeness. In Judgment under Uncertainty: Heuristics and Biases, ed. D. Kahneman et al. Cambridge University Press.

- Tversky, A., and Kahneman, D. 1983. Extensional versus Intuitive Reasoning: The Conjunction Fallacy in Probability Judgment. Psychological Review 90: 293–315.

- Tversky, A., and Shafir, E. 1992. Decision under Conflict: An Analysis of Choice Aversion. Psychological Science 6: 358–361.

- Vanderbilt, T. 2008. Traffic: Why We Drive the Way We Do (and What It Says about Us). Knopf.

- Van Hecke, M. 2007. Blind Spots: Why Smart People Do Dumb Things. Prometheus.

- Vicente, K. 1999. Cognitive Work Analysis: Toward Safe, Productive, and Healthy Computer- Based Work. Erlbaum.

- Vicente, K. 2002. Work Domain Analysis and Task Analysis: A Difference That Matters. In Cognitive Task Analysis, ed. J. Schraagen et al. Erlbaum.

- Vos Savant, M. 1990. Ask Marilyn. Parade, September 9.

- Waldmann, M.R. 1996. Knowledge-Based Causal Induction. In The Psychology of Learning and Motivation, volume 34: Causal Learning, ed. D. Shanks et al. Academic.

- Wallsten, T. 2000. A Letter from the President.

- Society for Judgment and Decision Making Newsletter, June: 4–5.

- Watson, J. 1968. The Double Helix. Atheneum

- Wears, R., and Berg, M. 2005. Computer Technology and Clinical Work: Still Waiting for Godot. JAMA 293: 1261–1263.

- Weick, K., and Sutcliffe, K. 2001. Managing the Unexpected: Assuring High Performance in an Age of Complexity. Jossey-Bass.

- Weick, K., and Sutcliffe, K. 2006. Mindfulness and the Quality of Attention. Organization Science 17, no. 4: 514–525.

- Weick, K., Sutcliffe, K., and Obstfeld, D. 1999. Organizing for High Reliability: Processes of Collective Mindfulness. Research in Organizational Behavior 21: 81–123.

- Westin, D., and Weinberger, J. 2004. When Clinical Description Becomes Statistical Prediction. American Psychologist 59: 595–613.

- Wilson, T. 2002. Strangers to Ourselves: Discovering the Adaptive Unconscious. Harvard University Press.

- Wohlstetter, R. 1962. Pearl Harbor: Warning and Decision. Stanford University Press.

- Woods, D. 2006. Essential Characteristics of Resilience for Organizations. In Resilience Engineering: Concepts and Precepts, ed. E. Hollnagel et al. Ashgate.

- Woods, D. 2007. Creating Safety by Engineering Resilience. Invited talk, Almaden Institute, IBM Almaden Research Center.

- Woods, D. 2009. Escaping Failures of Foresight. Safety Science 47: 498–501.

- Woods, D., and Hollnagel, E. 2006. Joint Cognitive Systems: Patterns in Cognitive Systems Engineering. Taylor and Francis.

- Woods, D., Patterson, E., and Roth, E. 2002. Can We Ever Escape from Data Overload? A Cognitive Systems Diagnosis. Cognition, Technology and Work 4: 22–36.

- Woods, D., and Wreathall, J. 2008. Stress-Strain Plot as a Basis for Assessing System Resilience. In Resilience Engineering: Remaining Sensitive to the Possibility of Failure, ed. E. Hollnagel et al. Ashgate.

- Wright, L. 2006. The Agent: Did the CIA Stop an FBI Detective from Preventing 9/11? The New Yorker, July 10 and 17.

- Wu, G., Zhang, J., and Gonzalez, R. 2004. Decision under Risk. In Blackwell Handbook of Judgment and Decision Making, ed. D. Koehler and N. Harvey. Blackwell.

- Xiao, Y., Seagull, F., Nieves-Khouw, F., Barczak,

- N., and Perkins, S. 2004. Organizational- Historical Analysis of the “Failure to Respond to Alarm” Problems. IEEE Transactions on Systems, Man, and Cybernetics, Part A 34, no. 6: 772–778.

- Yamagishi, K. 2003. Facilitating Normative Judgments of Conditional Probability: Frequency or Nested Sets? Experimental Psychology 50: 97– 106.

- Yates, J., Veinott, E., and Patalano, A. 2003. Hard Decisions, Bad Decisions: On Decision Quality and Decision Aiding. In Emerging Perspectives on Judgment and Decision Research, ed. S. Schneider and J. Shanteau. Cambridge University Press.

- Yergin, D. 1991. The Prize: The Epic Quest for Oil, Money, and Power. Free Press.

- Zakay, D., and Wooler, S. 1984. Time Pressure, Training, and Decision Effectiveness. Ergonomics 27: 273–284.

- Zhu, L., and Gigerenzer, G. 2006. Children Can Solve Bayesian Problems: The Role of in Mental Computation. Cognition 98: 287–308.

