# Streetlights and Shadows


[[20211041526 Dreyfus Model of Expertise]]


[[201705310826.5 Streetlights and Shadows_ Searching for the Keys to Adaptive Decision Making (MIT Press) - Gary Klein]]
P5: 
> We can’t treat every situation as an emergency; that’s why we depend on standard strategies to let us reach our everyday goals. However, we can become vulnerable if we are too rigid, too locked into our routines to adjust to changing conditions. **We need both mental “gears”: one for using the standard procedures and the other for improvising when situations become unsettled. **

P5
> Our eyes are built for two perspectives. During the daytime we rely on our cone cells, which depend on lots of light and let us see details. At night the cone cells become useless and we depend on rod cells, which are much more sensitive. The rod cells in our eyes are connected together to detect stray light; as a result they don’t register fine details. If we want to see something in bright light, we focus the image on the center of our retina (the fovea), where the cone cells are tightly packed. To see something at night, we must look off to the side of it, because staring directly at it will focus the object on the useless cone cells in the fovea.

---

Notebookexport

Streetlights and Shadows: Searching for the Keys to Adaptive Decision Making (English Edition)

Klein, Gary A.

---
___

## 1   Ten Surprises about How We Handle Ambiguous Situations

### Markierung (gelb) - Seite 5 · Position 265

we rely on our cone cells , which depend on lots of light and let us see details . At night the cone cells become useless and we depend on rod cells , which are much more sensitive . The rod cells in our eyes are connected together to detect stray light ; as a result they don’t register fine details . If we want to see something in bright light , we focus the image on the center of our retina ( the fovea ) , where the cone cells are tightly packed . To see something

### Markierung (gelb) - Seite 12 · Position 397

make progress when we find regularities in situations that appeared to be highly complex . We should encourage those researchers who look for order in complex situations . Many hygiene and public health procedures are examples of initially complex domains which , after painstaking study , analysis , data gathering , and assessments , evolved over many years to orderly understandings . SARS was complex and initially required complex responses , but over time we have figured out how it works and now have a repertoire of very structured responses to it . The boundaries

### Notiz - Seite 12 · Position 401

Rki corona was wenn noch keine daten verfügbar

## Part I   Making Decisions

### Markierung (orange) - 2   A Passion for Procedures > Seite 17 · Position 459

Instructional Systems Design ,

### Markierung (gelb) - 2   A Passion for Procedures > Seite 17 · Position 461

1974 book The Inner Game of Tennis , Timothy Gallwey argued that in tennis following procedures is the opposite of skill . Instead of engaging in the sport , players worry about their form — whether their feet are too far apart , if one elbow is bent at the correct angle , and so forth . Those kinds of rules and procedures are more likely to interfere with performance than to improve it . Putting Gallwey’s ideas together with my discussion with Scotty and

### Markierung (gelb) - 2   A Passion for Procedures > Seite 17 · Position 467

Hubert and Stuart Dreyfus to provide an alternative view . Both of them were Berkeley professors , Bert in philosophy and Stuart in operations research .

### Markierung (gelb) - 2   A Passion for Procedures > Seite 17 · Position 469

how people develop expertise . According to their model , novices are given simple procedures that don’t depend on context — on what else might be going on . Thus , beginning chess players might be taught numerical values of the pieces and advised to be careful not to lose exchanges . For example , it’s a bad idea to trade a queen for a pawn . 6 Of course , these numerical values are fictions ; the real value of a chess piece depends on what is happening in a position , and will change as the game changes .

### Markierung (gelb) - 2   A Passion for Procedures > Seite 18 · Position 476

Dreyfus model of expertise emphasizes intuition and tacit knowledge that can’t be captured in rules and procedures . People might need some rules in order to get started , but they have to move beyond rules in order to achieve mastery . 7 Procedures , including checklists , are tools . Every tool has limitations , and I am not arguing that we should do away with procedures . For example , I admire Peter Pronovost , who advocated for

### Markierung (gelb) - 2   A Passion for Procedures > Seite 18 · Position 485

accident , there is a good chance that “ procedural violation ” will be trumpeted as one of the contributing factors . I once participated in an aviation accident investigation . The flight data recordings showed pretty clearly that the pilots hadn’t done everything exactly by the book . The senior pilot next to me pointed out that pilots violate some procedures on almost every flight . There are so many procedures that pilots are bound to

### Markierung (gelb) - 2   A Passion for Procedures > Seite 22 · Position 557

when turning the data into forecasts . They took the “ low road ” illustrated in figure 2.1 . In contrast , the highly skilled forecasters tried to understand what was going on . They foraged for data that helped them build a better understanding , and used their understanding to make predictions . When I presented figure 2.1 at the 21st American Meteorological Society Conference on Weather Analysis and Forecasting , in Washington , in 2005 , an executive from the national forecasting service Accuweather commented that his company was increasingly reliant on the procedures its forecasters needed to take the low road in figure 2.1 . The senior staff referred to these procedural guidelines as “ the great equalizer . ” They permitted the mediocre forecasters to just follow some rules and still put out adequate forecasts . But they tied the hands of the skilled meteorologists and degraded their performance to the point that they were just putting out adequate forecasts , no better than that . The procedures mandated which data to collect and what types of analyses

### Markierung (gelb) - 2   A Passion for Procedures > Seite 23 · Position 576

Research supports this idea of eroding expertise . A number of studies have shown that procedures help people handle typical tasks , but people do best in novel situations when they understand the system they need to control . 11 People taught to understand the system develop

### Markierung (gelb) - 2   A Passion for Procedures > Seite 28 · Position 655

change behavior even though there may be simpler and more effective strategies . For example , public officials in Taiwan grew frustrated by merchants who failed to pay sales taxes . The merchants handled cash transactions off the cash registers , leaving no trail for inspectors to follow . Instead of increasing penalties and warnings , Taiwan set up a lottery in which every entry was required to be accompanied by a sales slip . Suddenly , in that lottery - crazed country , people were demanding

### Markierung (orange) - 2   A Passion for Procedures > Seite 30 · Position 699

To put procedures into perspective , consider the difference between directions and maps ( Vicente 2002 ) . When we have to travel to an unfamiliar destination , we sometimes get directions — a sequence of actions ( e.g . , turn right , go straight for two blocks , then turn left ) . Other times we get a map showing where we are , where we want to be , and the terrain in between . The directions are easier to follow , but if anything goes wrong ( say , a street is blocked off ) we are stuck . A map demands more of us but makes it easier for us to adapt and can be used for other routes in the same area . For many types of complex work we need both procedures and the judgment to interpret and work around the procedures . Hockey , Sauer , and Wastell ( 2007 ) used a laboratory process control task to compare the value of training rules and procedures against the value of training people to understand the system they had to control . As was expected , people trained to understand the way the system worked were more flexible , and did a better job of spotting and fixing unfamiliar and complex malfunctions , than people trained to follow rules and procedures . However , they also took longer to do the work , and they were more affected by a stressor — noise — than people who had merely been trained to follow procedures .

### Notiz - 2   A Passion for Procedures > Seite 30 · Position 704

\*\*Linux vs mac\*\*

### Markierung (gelb) - 2   A Passion for Procedures > Seite 31 · Position 709

Here is another way to teach procedures : Set up scenarios for various kinds of challenges and let the new workers go through the scenarios . If the procedures make sense , then workers should get to see what happens when they depart from the optimal procedures . When procedures are taught in a scenario format , people can appreciate why the procedures were put into place and can also gain a sense of the limitations of the procedures . This scenario format seems to work better than having people memorize the details of each step . The scenarios provide a good counterpoint for learning the steps of complicated tasks . Moreover , the scenarios can help people acquire some of the tacit knowledge they need in order to apply procedures effectively . ( The topic of tacit knowledge will be taken up in the next chapter . )

### Markierung (orange) - 3   Seeing the Invisible > Seite 33 · Position 732

Tacit knowledge is being able to do things without being able to explain how . We can’t learn tacit knowledge from a textbook . 2 We know more than we can tell . 3

### Markierung (gelb) - 3   Seeing the Invisible > Seite 44 · Position 930

The interplay between noticing typical cases and anomalous ones is a type of tacit knowledge found in many fields . For example , nurses in intensive - care units build up a sense of typicality that lets them notice when a patient looks atypical . The nurses become early warning systems to catch weak signals that a patient is starting to deteriorate .

### Markierung (gelb) - 3   Seeing the Invisible > Seite 44 · Position 935

Mental models20 are the stories we construct to understand how things work . They mirror the events or system they are modeling , but they capture only a limited aspect of those events or that system . 21 We form our mental models from the way we understand causes .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 49 · Position 999

The anchoring - and - adjustment heuristic When we have to make an estimate and we don’t know the answer , one strategy we use is to find a plausible answer and then adjust it up or down . This mental strategy is called anchoring and adjustment .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 50 · Position 1015

That’s what is meant by biasing people’s judgments .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 52 · Position 1053

When given the ‘ deny ’ frame , subjects looked for reasons to deny custody , and parent B had stronger reasons

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 52 · Position 1054

As in the preceding example , the frame we use will affect which types of data we notice . We can influence people’s judgments just by the way we frame the problem .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 52 · Position 1058

“ Would you like to have me give a short talk about some of my findings ? ” But I was pretty sure he would have said no , because he wouldn’t have heard any reasons to grant my request . Instead , I asked “ Do you have any objection if I take a few minutes to describe some findings that bear directly on the issues we’ve been discussing ? ” He thought about it , he couldn’t come up with any strong reasons to turn me down ( we were slightly ahead of schedule ) , and he carved out a slot for me . It’s all in the framing .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 53 · Position 1070

Tversky and Kahneman ( 1982 ) provided the classical demonstration of this heuristic : Linda is 31 years old , single , outspoken and very bright . She majored in philosophy . As a student , she was deeply concerned with issues of discrimination and social justice , and also participated in antinuclear demonstrations . Please check off the most likely alternative : — Linda is a bank teller . — Linda is a bank teller and is active in the feminist movement . In the original experiment , about 85 percent of the subjects picked the second alternative . But the first alternative is statistically more likely , because it includes the possibility that Linda is active in the feminist movement as well as the possibility that Linda is a bank teller but is not active in the feminist movement . Most subjects pick the second alternative because it seems like a fuller and more accurate description of Linda . It seems like a better representation of the kind of person Linda is . The pull of the second alternative shows the representativeness heuristic at work . Here it prevented subjects from judging which alternative is more likely .

### Notiz - 4   How Biased Is Our Thinking? > Seite 53 · Position 1079

Gigerenzer wird doch auch beinsönke ahrens erwähnt

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 55 · Position 1109

Fortunately , the fears of decision biases are overblown . The research doesn’t really demonstrate that we are irrational . Rather , we use effective strategies in our thinking but these have some limitations that researchers can exploit .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 60 · Position 1197

Instead , we learn to use something called the gaze heuristic . If you are running in at just the right speed , the angle of your gaze — directed at the ball — will stay constant .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 63 · Position 1258

Lovallo and Kahneman suggest a strategy of taking an “ outside view ” — that is , using previous projects to suggest how long tasks will take and how many resources they will consume .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 63 · Position 1261

PreMortem technique .

### Markierung (gelb) - 4   How Biased Is Our Thinking? > Seite 66 · Position 1310

claim 2 matters Claim 2 ( that decision biases distort our thinking ) matters because the concept of bias encourages organizations to overreact to failures by enacting excessive procedures and counter - productive restrictions . Biases often get invoked whenever a decision turns out wrong . But preventing these “ biases ” would likely do more harm than good . It could eradicate

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 67 · Position 1322

1954 , Paul Meehl published Clinical vs . Statistical Prediction , a very influential book describing 18 studies that showed the limitations of human judgment . These studies compared the judgments of professionals against statistical rule - based predictions about parole violations , success in pilot training , and academic success . In each study , the professionals had access to the data used by the statistical procedures and to additional data that might not have been included in the algorithm . Despite all their experience , the professionals outperformed the algorithms in only one of the 18 cases . In a few other cases the professionals and the formulas gave similar results , but in most of the cases the statistical rules were superior to the expert judgments . In one example , academic counselors had access to all the data on a school’s incoming freshmen plus a 45 - minute interview of each one , and still were less accurate in predicting their first - year grades than a statistical analysis based only on the students ’ high school grades and their scores on a standardized test .

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 68 · Position 1341

Tetlock studied a set of 284 experts , most with PhDs and postgraduate training in fields such as political science , economics , international law and diplomacy , public policy , and journalism . Averaging 12 years of relevant work experience , they came from universities , think tanks , government service , and international institutions . Tetlock’s criterion for experts was that they be professionals earning their living by commenting or offering advice on political and economic trends . Their average age was 43 . Approximately 61 percent had been interviewed by a major media outlet ; 21 percent had been interviewed more than ten times . Tetlock collected most of his data between 1988 and 1992 . He presented these experts with sets of questions and asked them to rate the probabilities . The questions dealt with ( among other things ) the likelihood of various countries acquiring the ability to produce weapons of mass destruction ; the possibility of states or terrorist groups using such weapons ; predictions over 3 , 6 , or 12 years about economic reforms in the former Soviet Bloc countries ; adoption of the Euro ; the prospects of former Soviet Bloc countries and Turkey joining the European Union ; the winners of the American presidential elections in 1992 and 2000 and the margins of victory ; the performance of the NASDAQ ; the revenues , earnings , and share prices of Internet and information - technology companies such as CISCO , Oracle , Microsoft , Enron , and IBM ; and whether Quebec would secede from the rest of Canada . Tetlock waited a few years to see whether the world events actually occurred within the time frame in the questions . Then he compiled the actual results , to see how well the experts performed in comparison with a control — the performance of a hypothetical chimpanzee who simply assigned equal probabilities to the different events . The experts barely beat the hypothetical chimp . They were slightly more accurate than chance . It got worse . Tetlock had tested the experts with some questions from their fields of study and some from unfamiliar fields . The experts didn’t do any better on questions from their fields of study than questions on unfamiliar topics . ( They did outperform Berkeley undergraduates who received short descriptions of each topic of interest . ) When Tetlock confronted the experts with evidence for their inaccurate predictions , they were unfazed . They explained away the prediction failures rather than trying to improve their mental models . They showed the typical symptoms of fixation . Experts attributed their successes to their own skilled judgment , whereas they blamed their failures on bad luck or task difficulty .

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 72 · Position 1411

To make predictions we have to do two things : collect the data and then combine the data . Some of the confusion about logic and statistics and intuition arises from blurring this distinction . Meehl and his followers haven’t shown that expert intuitions are useless . They have just shown that statistical formulas for combining the data can be more accurate than the estimates made by experts .

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 72 · Position 1416

Clinicians make these judgments from interviews and observations with prisoners , judging typicality in reference to hundreds of other interviews they have done , many with similar prisoners . Clinicians use their judgment to identify variables that might be important , something that mechanical systems aren’t particularly good at . Meehl never doubted the importance of intuition and expertise in making these judgments . 3

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 73 · Position 1442

Most of what differentiates skilled from unskilled chess players is their tacit knowledge , not their ability to calculate move quality . Systematic analysis is important in chess , but intuitive skills and tacit knowledge seem to be more critical . As players get stronger , their ability to do deliberate search and analysis doesn’t seem to get any better . 5,6 It is hard to reconcile such findings with the claim that successful decision makers rely on logic and statistics rather than on intuition .

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 76 · Position 1491

The answer seems obvious — we should rely on conscious thought . That’s what a team of Dutch researchers led by Ap Dijksterhuis found when they asked subjects to make simple choices , such as selecting towels or oven mitts ( Dijksterhuis et al . 2006 ) . But when the researchers turned to complex8 choices , such as selecting an automobile or an apartment , the subjects did better with unconscious thought . In one study , participants read about four apartments that differed in desirability , then either chose immediately , chose after thinking hard about the alternatives , or chose after being distracted for the same period of time . This last group , the unconscious thinkers , made the best choices , picking the apartments that got the highest ratings from judges . In another study , participants tried to select a car from among four different models . 9 The option that was considered the best was the one with the highest proportion of positive features . In the simple condition the participants had to consider only four attributes of the cars ; in the complex condition they had to consider twelve attributes . Some of the participants had a chance to think deliberately about the options for four minutes ; others were distracted by another task for four minutes . Then they made their choice . The conscious thinkers made the right choice in the simple condition ( only four attributes to think about ) but not in the complex conditions . The people who were distracted — the unconscious thinkers — made the best choice in both the simple and the complex conditions .

### Markierung (gelb) - 5   Intuition versus Analysis > Seite 77 · Position 1508

We can’t easily explain these findings away , because other researchers , including Timothy Wilson , Jonathan Schooler , and John Bargh , have all reported the same results — that conscious deliberation seems to make people less satisfied with their decisions . 11 Conscious thought doesn’t necessarily help us make better choices . Any systematic strategy exposes us to the risks of overthinking . 12 Radiologists worry about overthinking . If they look at a film too long , they start seeing things that aren’t there . After about 38 seconds , they begin to overreact to slight irregularities in normal structures and identify non - existent malformations . 13 What causes overthinking ? One possibility is that we can think of only a few things at a time , so our cognitive bandwidth prevents us from considering all the variables when making choices . Unconscious thought lets us integrate lots of information . Another possibility is that when we consciously deliberate we may over - emphasize some of the features — the features that are easier to verbalize . We may ignore or downplay important features that don’t have convenient verbal tags . Conscious deliberation seems to impose serious penalties on us . Conscious deliberation presses us to view the world through a keyhole — the limitation of attention . And what gets missed by this keyhole view ? Often we fail to notice the context . We happily flag three or four variables and think systematically about these , ignoring the background of what is going on . But when we tell stories , we tend to capture much of the context — that’s the whole idea of a story , to embed facts within contexts . If we try to think about facts without the story ( the context ) , we risk overthinking . It is very easy to miss what is really going on by attending just to the facts . Example 5.5 shows that the objective data may give us a distorted picture of what is really going on .

### Markierung (orange) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 86 · Position 1680

The more skilled one is , the fewer options one thinks about . The only population I studied that compared multiple options for most decisions was the least experienced — tank platoon leaders in their initial training . Novices have to compare options because they don’t have the experience base to recognize what to do . However , comparing options doesn’t substitute for experience . A final weakness in these analytical methods is the Zone of Indifference . 6 When one option is clearly better than the others , we need not do any analysis . We immediately know what to choose . The closer the options become , the more the strengths and weaknesses are balanced , the harder the choice . The hardest decisions are those that must be made when the options are just about perfectly balanced . Paradoxically , if the options are perfectly balanced it doesn’t much matter which one we choose . We agonize the most , spend the most time and effort , making choices that are inside this Zone of Indifference , when we might as well flip a coin . The analytical methods are designed to help us make the toughest choices , but once we realize we are inside the Zone of Indifference we should stop right there , make an arbitrary choice , and move on . What if we don’t move on ? Herbert Simon argued that any company that attempted to optimize its returns and make the best decisions would fall into a never - ending quest to find the best decision . Simon ( 1957 ) coined the term satisficing to describe what we all do just about all the time — pick the first option that seems to get the job done and not worry about whether it is the best .

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 90 · Position 1752

continuing on until they found an adequate course of action . Klein , Calderwood , and Clinton - Cirocco ( 1986 ) called this strategy a Recognition - Primed Decision ( RPD ) model . ( See figure 6.1 . ) The pattern recognition suggested an effective course of action and then the firefighters used a mental simulation to make sure it would work . This RPD strategy combines intuition with analysis . The pattern matching is the intuitive part , and the mental simulation is the

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 91 · Position 1768

Example 6.3 : Miracle on the Hudson On January 15 , 2009 , at 3 : 25 p.m . , US Airways Flight 1529 , an Airbus 320 , took off from LaGuardia Airport in New York on its way to Charlotte , North Carolina . Two minutes after the takeoff the airplane hit a flock of Canada geese and lost thrust in both of its engines . The captain , Chesley B . “ Sully ” Sullen - berger III , and the first officer , Jeffrey Skiles , safely landed the airplane in the Hudson River at 3 : 31 p.m . All 150 passengers plus the five crew members were rescued . Media reports and interviews with Sullenberger allow us to describe his decision strategy after he lost thrust in both engines . Option 1 was to return to LaGuardia Airport . Sullenberger’s initial message to the air traffic controllers was “ Hit birds . We lost thrust in both engines . We’re turning back toward LaGuardia . ” But he quickly realized that the airplane was too low and slow to make it back , so he abandoned that plan . Option 2 was to find another airport . Sullenberger was headed west and thought he might be able to reach Teterboro Airport in New Jersey . The air traffic controllers quickly gained permission for him to land at Teterboro , but Sullenberger judged he wouldn’t get that far . “ We can’t do it , ” he stated . “ We’re gonna be in the Hudson . ” Option 3 was to land in the Hudson River .

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 95 · Position 1835

Research by Raanan Lipshitz and Orit Ben Shaul ( 1997 ) is consistent with what numerous other studies have found : novices tend to deliberate about which option to select , whereas experts deliberate about what is going on in the situation .

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 95 · Position 1848

may have a choice between a few different job applicants , or between a few different jobs , or different colleges that have accepted us , or we may have to figure out which computer to buy . We may have to decide whether to move to another state to be with a spouse .

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 96 · Position 1850

We can learn from skilled chess players . 17 They don’t settle for the first satisfactory option . They really do want to play the best move possible . Their strategy is to conduct mental simulations of each one of the promising moves , imagining how the option would play out deeper and deeper into the future . Then they take stock of their mental and emotional reactions to what they see in this mental review . If they feel that a line of play is going to get them into trouble , they reject that move . Other moves may have potential and are worth thinking about , and some just feel right — they just seem to be more promising than the others .

### Markierung (gelb) - 6   Blending Intuition and Analysis to Make Rapid Decisions > Seite 98 · Position 1895

Because of misplaced faith in claim 3 , and because of its advantages for coordinating planning teams , some organizations mandate these methods . For example , the US Army has a Military Decision Making Process that centers around generating three courses of action and evaluating each option on a common set of dimensions , as illustrated in table 6.1 . The Marine Corps has its own version of this process .

### Markierung (gelb) - 7   Experts and Errors > Seite 101 · Position 1934

7 Experts and Errors According to legend , a famous composer was asked how long it took to write one of his operas and replied “ One week , and all my life before that . ” Many of the claims discussed in this book relate to the nature of expertise . These claims rest on

### Markierung (orange) - 7   Experts and Errors > Seite 104 · Position 1998

The experience and the patterns enable us to judge what to pay attention to and what to ignore . That way , we usually reserve our attention for the most important cues and aspects of a situation . However , if the situation is deceptive or is different from what we expect , we may focus our attention on the wrong things and ignore important cues . That’s why the concept of “ mindsets ” creates so much controversy . Our mindsets frame the cues in front of us and the events that are unfolding so we can make sense of everything . Experience and patterns produce mindsets . The more experience we have , the more patterns we have learned , the larger and more varied our mindsets and the more accurate they are . We depend heavily on our mindsets . Yet our mindsets aren’t perfect and can mislead us . With more expertise , we may become more confident in our mindsets , and therefore more easily misled . 2

### Markierung (orange) - 7   Experts and Errors > Seite 105 · Position 2006

Mindsets aren’t good or bad . Their value depends on how well they fit the situation in which we find ourselves . Mindsets help us frame situations and provide anchors for making estimates . With more experience , our frames will be effective and our anchors will permit accurate estimates . When we are in unfamiliar situations , this same use of mindsets won’t work as well . Our frames may distort what is going on , and we may be susceptible to irrelevant anchors .

### Markierung (gelb) - 7   Experts and Errors > Seite 107 · Position 2045

The fear of human error also influences health care . Hospitals and insurance companies are increasingly turning to evidence - based medicine , the process of relying on statistical analyses of which treatments work most effectively . Hospitals and health insurance companies expect physicians to comply with the best practices .

### Markierung (orange) - 7   Experts and Errors > Seite 108 · Position 2069

Think of the task of controlling a nuclear power plant . If you insert the graphite rods too far into the core , you slow the reaction and the plant doesn’t produce much energy . If you withdraw the graphite rods too far , the reaction will speed up too much and the uranium in the core may overheat and melt . The technicians in the control room have an array of sensors and computer aids with which to determine what is going on inside the reactor .

### Markierung (gelb) - 7   Experts and Errors > Seite 109 · Position 2093

own blood glucose works . Everyone has a different metabolism and physiology , so you can’t rely on standard rules . You can’t boil diabetes management into one - size - fits - all procedures . The successful patients learned offsetting strategies , such as going for a walk , to get the levels down . One Air Force pilot explained that controlling his diabetes was like flying an airplane when you couldn’t use autopilot anymore and had to take over manual control . That illustrates his attitude as well as the strategy of learning the handling characteristics of his condition . The successful diabetics had learned what caused their blood glucose to go up and down , and what the time lags were . Diabetics

### Markierung (orange) - 7   Experts and Errors > Seite 110 · Position 2112

Kahneman and I concluded that two conditions are necessary for reliable intuitions to develop : the situation must be reasonably predictable and people must have opportunities to learn . Low - predictability situations make it unlikely that people can develop expertise because it is so hard to identify reliable cues .

### Markierung (orange) - 7   Experts and Errors > Seite 112 · Position 2140

In many cases , our aversion to mistakes may be counter - productive . We must make mistakes in order to learn . 10 Deakin and Cobley ( 2003 ) found that the most elite figure skaters fall more often than others during practice sessions because they spent more time attempting jumps they hadn’t mastered . If we discouraged the skaters from falling , they wouldn’t learn as quickly .

### Markierung (orange) - 7   Experts and Errors > Seite 112 · Position 2144

R . Buckminster Fuller once said “ If I ran a school , I’d give the average grade to the ones who gave me all the right answers , for being good parrots . I’d give the top grades to those who made a lot of mistakes and told me about them , and then told me what they learned from them . ” 11

### Markierung (orange) - 7   Experts and Errors > Seite 114 · Position 2149

as Sidney Dekker ( 2003 ) has complained , in his article “ When does human error become a crime , ” these sanctions reduce safety rather than increasing it . We learn from mistakes by diagnosing why they happened , but if an organization is overzealous about reducing mistakes then workers may spend more time covering up their mistakes than they spend trying to figure out what caused those mistakes and how to do a better job in the future .

### Markierung (gelb) - 8   Automating Decisions > Seite 115 · Position 2162

And yet many decision - support systems are rejected or fail . The developers think that people will welcome decision aids that make it easier to conduct decision analyses . The developers often can’t believe that decision makers are too stupid to appreciate all the benefits of such decision aids . But decision makers are not just rejecting the aids — they are rejecting the mindset that keeps churning out this kind of system . Decision makers are rejecting the mentality that idealizes reflective , analytical thinking and marginalizes automatic , intuitive thinking , instead of blending the two kinds of thinking .

### Markierung (gelb) - 8   Automating Decisions > Seite 116 · Position 2177

The decision maker alone . Kahneman and I think that most judgments and decisions fall into the first category . We don’t see much to be gained by incorporating information technology when people can attain a reasonable level of expertise . Information technology makes even less sense in the face of unstable , shadowy conditions that are heavily dependent on context . We are most comfortable relying on decision makers when they can develop tacit knowledge as a basis for their intuitions . For that to happen , the environment must have some predictability and decision makers must be able to get feedback on their choices and to gain some level of proficiency .

### Markierung (gelb) - 8   Automating Decisions > Seite 116 · Position 2182

The decision maker is helped by a support system or an algorithm . How hard should I exercise ? Many gyms have diagrams showing heart - rate guidelines . By looking at the column for my age , I can see the level I need to get a good workout and the suggested maximum heart rate . What price should I set for my car when I list it for sale ? By entering the year , model , and condition of the car and the geographical area , I can find out what price the car is likely to fetch . Should we drill for oil in a certain region ? Geologists have powerful analytical tools that can provide useful advice , even though the tools aren’t good enough to replace the seasoned veterans .

### Markierung (gelb) - 8   Automating Decisions > Seite 117 · Position 2187

The decision - support system has the final say , with inputs from the operators . Should a bank lend me money ? Intelligent systems now can do a more reliable and unbiased job than most of the people who have this authority . Drivers can tell their GPS systems if they want the fastest or the shortest route , and get a recommendation . Let the humans feed data into the program , but let the program make the final decision or recommendation . This approach means giving up authority to the decision - support system . However , in cases where our judgment is not particularly accurate ( for example , selecting job applicants or finding our way in unfamiliar cities , or guessing future revenues ) , we’ll probably get better decisions by relying on a formula than by using our judgment .

### Markierung (gelb) - 8   Automating Decisions > Seite 118 · Position 2204

The decision - support system makes the entire decision on its own . There is no point in letting experts do a job that a machine can do better . If I’m driving on an icy road , my car’s anti - lock braking system can figure out when to kick in and how to pump the brakes better than I can . My car’s traction - control system can figure out better than I can how to transfer power from wheels that are starting to slip to those that are solidly contacting the road .

### Markierung (gelb) - 8   Automating Decisions > Seite 121 · Position 2273

Example 8.1 : Racking and stacking4 Software developers have to decide what features to include in the release of the next system . To help with this challenge , Carlshamre ( 2002 ) built a prototype planning aid that balanced the costs of the new features and their value to the client . His system would “ rack and stack ” the proposed features , to help the developers decide which ones to select for the next version . But the simple tradeoffs that Carlshamre built into his planning aid didn’t work . Software developers rejected this system . They weren’t just trading off costs and benefits . For instance , a given requirement might depend on an employee who was going on maternity leave , so the requirement had to be delayed until a subsequent version . Carlshamre also discovered that the concept of “ value to the client ” combined the strategic business value for the customer , long - term and short - term value for a range of consumers with differing importance in different markets , compliance with laws and regulations , compatibility with new computing platforms , and internal cost savings . In estimating the resource demands for a new feature , the developers considered factors such as individual employee’s workload and vacation schedules and their company’s recruitment plans . Carlshamre’s system required users to assign values to the requirements but to the software developers these values were arbitrary and unconvincing . The criteria couldn’t be defined in advance , because many essential parameters are never quantified . The developers were always discovering properties as they planned — some criteria were only realized after solutions were presented . And as they worked the developers were continually gaining new insights about the relationship between features that were treated separately by Carlshamre’s program . Carlshamre concluded that a simple tradeoff — calculating the value of a feature against the resource required to field it — was actually a “ wicked ” problem ( Rittel and Webber 1984 ) that doesn’t permit optimal solutions .

### Markierung (gelb) - 8   Automating Decisions > Seite 123 · Position 2311

However , the errors that the system makes usually involve high - impact weather , such as severe storms and extreme temperature events .

### Markierung (gelb) - 8   Automating Decisions > Seite 124 · Position 2317

Decision aids aren’t doing very well in the field of medicine , either . One review of 100 studies of clinical decision - support systems over a six - year period6 found that if we discount evaluations done by the people who built the systems , fewer than 50 percent showed an improvement in performance . Wears and Berg ( 2005 ) argue that we can’t blame the problems on bad programming or poor implementation .

## Part II   Making Sense of Situations

### Markierung (orange) - Seite 128 · Position 2348

Sensemaking is not just a matter of connecting the dots . Sensemaking determines what counts as a dot . Jumping to conclusions is sometimes the right thing to do even before all the dots have been collected . Feedback depends on sensemaking . Our minds are not computers — they don’t connect dots

### Markierung (gelb) - 9   More Is Less > Seite 130 · Position 2369

A useful way to think about uncertainty is to distinguish between puzzles and mysteries . 3 A puzzle is easily solved with the addition of a critical data point . For example , as I write this ( in 2008 ) we don’t know exactly where Osama bin Laden is hiding . That is knowable . He is somewhere . We just don’t know where he is , or even if he is alive . 4 But if an informer were to provide bin Laden’s current location , the puzzle would be solved . A mystery isn’t solved by critical data . It requires more analysis , not more data . If we want to know what the future will bring to Iraq , no data point will give us the answer . No amount of data will eliminate our uncertainties about whether China is a potential business partner of the United States or an inevitable military , political , and commercial threat .

### Markierung (gelb) - 9   More Is Less > Seite 132 · Position 2403

Most experts use fewer than five cues when making judgments . That doesn’t mean we should stop gathering data after we get five cues . Experts know which five cues will matter . However , even experts ask for more than they need . Gathering the extra information is the easy part . Thinking about what the extra information means takes real work . We would be better off thinking more about what we have learned instead of continuing our pursuit of data . Omodei et al . ( 2005 ) came to the same conclusion in a study of firefighters . The researchers presented experienced commanders a set of decision scenarios of simulated forest fires . The teams with incomplete information performed better than the teams with detailed information .

### Markierung (gelb) - 9   More Is Less > Seite 133 · Position 2418

Oskamp ( 1965 ) gave experienced clinical psychologists more information to use in diagnosing a patient’s condition . The additional information didn’t improve the accuracy of their judgments , but their confidence ratings got higher as they received more information . Therefore , their confidence was misplaced . It reflected the amount of data the judges had , not their accuracy . We have to be careful not to overplay these kinds of studies in which the experimenters control which data to feed to their subjects . In practice , we usually decide for ourselves which data we will examine . We decide when we will stop seeking more data . Look at Mauboussin’s ( 2007 ) study of horse - racing handicappers . The more information the handicappers got , the more confident they were , just as Oskamp found with clinicians . Further , the handicappers ’ predictions were less accurate when they got 40 pieces of information than when they got five pieces . Once again , the more information , the worse the performance . But Ceci and Liker’s racing handicappers ( discussed in chapter 7 ) were able to integrate lots and lots of information . In a natural setting , the handicappers were deciding for themselves what information to use , rather than having the information thrust on them . It makes a difference . Therefore , even though I like the studies showing that people reach a saturation point and their performance gets worse when we drown them in too much data , in real - world settings experts usually can protect themselves . They self - select which types of data to seek . For example , in a study of professional auditors and accounting students , the experts primarily relied on a single type of information , whereas the novices tried to use all the data ( Ettenson et al . 1987 ) . The experts , using their single data source , were more accurate and showed greater reliability and consensus than the students .

### Markierung (gelb) - 9   More Is Less > Seite 133 · Position 2431

The biggest danger of claim 4 ( that we can reduce uncertainty by gathering more information ) is that it invites a mindless search for more data . It invites a search for anything that might be relevant . It invites abuse , encouraging us to move past our saturation point . Even when we control our own searches for information , we tend to gather more than we need , and the extra information can get in our way . We do this to ourselves . How many times have we gathered information we didn’t use ? The real reason for gathering extra information isn’t to make a better decision ; usually it is to stall for time .

### Markierung (gelb) - 9   More Is Less > Seite 144 · Position 2624

We will gather more data . The more the uncertainty , the more strenuous the data gathering . We won’t stop until our uncertainty disappears or we become exhausted , whichever comes first . Sometimes we will notice that the information we need is already in our “ in ” boxes . Therefore , we’ll decree that every message must be read in its entirety , to make sure we don’t miss anything . If we can’t personally read every message , we’ll have to hire additional staff members , and somehow work out how all those involved will collaborate . Because of the potential for overload , we will have to invest in information technology to collect and categorize and analyze all the data . And we’ll have to store the data in a way that will let us retrieve what we need instantaneously ( as long as we remember it and the label we used to store it ) . We will also have to develop intelligent filters to figure out what is relevant and what we can ignore . To make sure this all works efficiently , we’ll develop measures and standards for the right amount of data with which to make any type of decision . Does that fill anyone with confidence ?

### Markierung (gelb) - 9   More Is Less > Seite 144 · Position 2638

When dealing with a mystery , instead of a puzzle , we enter the realm of complexity . We have to pick our way through the shadows . What is the future of the relationship between Islamic and Western civilizations ? No data point will solve this mystery . The more books we read , the more lectures we attend , the more we have to think about , the more complicated it all gets . Information will not cure these kinds of uncertainty . And a non - directed search for more information can just add to the confusion . Richards Heuer wrote in Psychology of Intelligence Analysis ( 1999 ) that the intelligence community needs more analysis , not more data . His recommendation applies to many other communities as well . The replacement for that claim is that in complex environments , what we need isn’t the right information but the right way to understand the information we have . Further , under complex conditions , we need to manage uncertainty more than we need to reduce it . To manage uncertainty we have to know how to seek and prioritize information . We need to fill gaps with assumptions . We need to know when to wait for the situation to evolve . We need the cleverness to act in a way that structures the situation . 11 Managing uncertainty also means managing the teams and organizations that exchange messages or suppress them . The examples of Pearl Harbor , 9 / 11 , and Enron show that we need to improve team sensemaking , because teams may ignore the weak signals that individuals notice . The Phoenix Memo , which preceded the 9 / 11 attack , illustrates how easy it is to suppress suspicions .

### Notiz - 9   More Is Less > Seite 145 · Position 2650

Als Student sollte man nicht zuviel lesen, sondern mehr sensemaking

### Markierung (gelb) - 9   More Is Less > Seite 145 · Position 2654

However , we had asked the participants to keep private diaries , and in every team at least one member noted the weak signals in his or her diary . Sometimes half the members of a team noted the weak signals in their diaries . In other words , each team had the potential to surface the weak signals . But not a single team talked about them . Somehow the climate of teamwork suppressed these cues . We need to find ways to encourage team members to voice suspicions and hunches without inviting ridicule or losing credibility .

### Markierung (gelb) - 9   More Is Less > Seite 146 · Position 2660

Claim 4 emphasizes the quest for more information , whereas in facing mysteries people need to focus on the ways they are interpreting and anticipating events . Managing information puts the emphasis on what we understand , not on how many signals we have collected . It’s usually more valuable to figure out how the data connect than to collect more data .

### Markierung (gelb) - 10   When Patience Is a Vice > Seite 148 · Position 2700

Feltovich , Coulson , and Spiro ( 2001 ) took this research out of the laboratory and studied pediatric cardiologists in a hospital setting . Their experiment used a garden - path scenario in which participants form an incorrect initial explanation , and then get messages that contradict the initial story . The researchers measure how long people stay on the garden path — how much contrary evidence they need before they come to their senses . In this study , the cardiologists read a description of a new case and tried to find a diagnosis while receiving more and more information about the fictitious patient . The initial description made the diagnosis seem fairly obvious . However , that obvious diagnosis was wrong . The subsequent information contradicted the obvious diagnosis . Feltovich et al . found that some cardiologists stayed on the garden path for a very long time . Some never got off . They remained fixated on the initial “ obvious ” diagnosis .

### Markierung (gelb) - 10   When Patience Is a Vice > Seite 149 · Position 2723

In the spring of 2000 , when the staff in the Army headquarters in the Presevo Valley saw eight men going in a direct path through the town , it matched the pattern they expected . From their experience , groups that big walking around town were usually up to no good . Fortunately , the Army unit had just gotten their Unmanned Aerial Vehicle to begin sending them visual feeds , which ran live in the update room , where everyone could see them . They oriented the UAV to stay in the area of the suspicious activity . The Brigade Commander came in to watch . The gang of eight men were moving quickly , jumping fences , and appearing to be wreaking havoc . The Brigade Commander gave soldiers on the ground orders to find the bad guys . Then the men who were being watched on the UAV feed began running around . The HQ staff speculated about why the men had started running ; perhaps they heard the UAV , or heard a helicopter , or heard the American soldiers moving toward them . Regardless , the staff still thought the behavior was consistent with being “ bad guys , ” so they weren’t really trying to figure out why the men were running . As seen on the UAV feed , the suspicious band of men would start running down an alley , then turn around and run in the other direction . Their movements became pretty erratic , but this didn’t send up a red flag . Because it was dark , it was hard to make out how the men were dressed . The Army staff knew the coordinates of the UAV and the general location of the thugs , but wasn’t sure . There were no distinguishing landmarks . The staff believed that the group was moving with a purpose . The cues were that there were eight of them , they were tightly knit , and they were looking into windows . The cohesiveness of the group seemed to be a cue ; it didn’t seem to be just a group out walking . Everyone seemed to know where the others were going , and they were moving fairly fast . When they would stop running , it seemed that they were trying to re - organize , that they didn’t feel that they were in any immediate danger anymore , and that they were trying to figure out what to do next . Toward the end of this incident , an intelligence analyst from another unit came into the update room to see what was going on . He never said anything or asked for information from anyone . He just said “ Hey , those are our guys . ” The soldiers on the ground were also the “ bad guys ” they were trying to catch ! When asked how he had figured it out , the intelligence analyst said he could hear the commander saying “ move right ” and then saw the people on the feed move right . The whole incident took approximately 15 minutes . The intelligence analyst only came into the room for the last 2 – 3 minutes . No one on the staff who had been there from the start of the incident realized that the men they observed were responding to the commander’s orders . Some people did notice a connection between the orders and the reactions of the “ bad guys , ” and explained it away : “ They must be intercepting our radio communications . ” This example and many others point to the importance of claim 5 . Yet our survey respondents gave this claim an average rating of only 5.06 , “ Tend to agree for most situations . ” They weren’t entirely convinced of it . Twenty out of 164 disagreed with it .

### Markierung (gelb) - 10   When Patience Is a Vice > Seite 152 · Position 2761

Example 10.2 : The plugged breathing tube Rudolph used a garden - path scenario to study the anesthesiologists . In the scenario , an anesthesiologist - in - training was called into a simulated but fully outfitted operating room to provide anesthesia for a woman ( actually , a very lifelike mannequin ) who was being prepared for an appendectomy . After getting the breathing tube into the mannequin’s airway and putting “ her ” to sleep , somehow the ventilation stopped working very well . Why could that be ? ( The actual reason is that the “ woman ” exhaled some mucous into the tube and this mucous plug hardened inside the breathing tube . ) The anesthesiologists struggled to what was going wrong because the timing of the ventilation problem and the patient’s history of mild asthma suggested that the cause might be a bronchospasm ( an asthma attack ) . When treating the bronchospasm didn’t work , another common reaction to the ventilation problem was to suction the breathing tube to remove any blockages . This treatment , however , also had no effect because the mucous plug had hardened . This is a rare development . Worse , the surgical team was under time pressure to remove the inflamed appendix . Rudolph divided her subjects into four categories based on their reactions to this scenario : Stalled , Fixated , Diagnostic Vagabonds , and Adaptive Problem Solvers . Two of the anesthesiologists fit the “ stalled ” category . Neither of them could find any pattern that showed them how to proceed . They couldn’t generate diagnoses , and they didn’t try different treatments . Neither figured out the problem . The eleven physicians categorized as fixated ( including one chief resident ) usually jumped to the obvious diagnosis of bronchospasm . This diagnosis fits perfectly with the timing of the ventilation problem , which began after the breathing tube was inserted . These physicians tended to repeat one treatment for bronchospasm over and over rather than experimenting with different treatments . They rarely reconsidered whether the ventilation problem was in the tube rather than in the patient . Six of these fixated physicians did wonder about secretions in the breathing tube , and two of them tested for a blocked tube . The test is to see if secretions come out when the tube is suctioned . Because the mucous plug had hardened , no secretions came out , so they mistakenly concluded that the tube itself was clear . The physicians also erroneously interpreted distant breath sounds as wheezes , a sign of bronchospasm . None of the eleven anesthesiologists in the fixated group diagnosed the problem . The 17 open - minded anesthesiologists fared no better . Rudolph called their pattern “ diagnostic vagabonding , ” because these anesthesiologists wouldn’t commit to any diagnosis but instead treated all possibilities as tentative . Was the problem bronchospasm , too little muscle relaxant , or a wrongly placed tube ? These physicians would consider each possibility but quickly jump to the others , and never engaged in a course of treatment that would let them probe more deeply . None of them figured out the problem . Last , we have the nine physicians who jumped to conclusions but tested those beliefs . Rudolph called them “ adaptive problem solvers . ” Like the fixated problem solvers , most of them immediately identified the bronchospasm as the most likely cause . But when their treatment didn’t work , they turned to other diagnoses ( e.g . , allergic reactions , pulmonary embolisms ) , testing and rejecting each , eventually speculating about an obstructed breathing tube . Their active exploration style let them use initial diagnoses as springboards for conducting subsequent tests and treatments . Seven of these nine physicians discovered the hardened mucous plug . They tested in different ways — with fiber optic scopes , by the feel of the suction catheter , by the dry sound as they did the suctioning , by comparing how far they could insert the suction catheter versus the length of the breathing tube . Four different strategies that all led to the same diagnosis . No physician in any of the other categories got there . Rudolph expected that the anesthesiologists who jumped to a conclusion and held on to it would be unsuccessful , and she was right . None of them ever figured out the problem . Rudolph also expected that the anesthesiologists who kept an open mind while receiving the stream of information would be successful at making the right diagnosis , and here she was wrong . The ones who kept an open mind , absorbing data like sponges , also failed . The only ones who succeeded had jumped to conclusions and tested them . They hadn’t fixated on their first explanation . Instead , they had used that explanation to guide the tests they had performed and the way they had searched for new information . They exemplified the strategy of “ strong ideas , weakly held . ” Now we have a conflict . Rudolph found that the physicians who kept an open mind didn’t make the diagnosis , but Bruner and Potter found that the subjects who speculated too soon , when the image was too blurry , did the worst . Where does that leave us ? Maybe it depends on whether people are active or passive . Bruner and Potter’s subjects , college students , couldn’t direct their own search ; they just had to sit there , watching a fuzzy image , describing what they thought they saw . In contrast , Rudolph let her subjects — anes - thesiologists — actively gather information . Perhaps that’s why their early speculations became a basis for testing and inquiring .

### Markierung (orange) - 10   When Patience Is a Vice > Seite 154 · Position 2807

doesn’t work under conditions of complexity . Replacement The replacement for claim 5 is to speculate actively , but to test those speculations instead of getting committed to them . Rather than advising people to keep an open mind , we can encourage them to engage in a speculate - and - test strategy . 6 Cohen , Freeman , and Thompson ( 1998 ) have developed a training approach to support the speculate - and - test strategy , and have demonstrated its effectiveness .

### Markierung (gelb) - 10   When Patience Is a Vice > Seite 158 · Position 2870

I think that in complex and ambiguous settings people should actively speculate , instead of passively absorbing data . This advice is just the opposite of claim 5 . Experts distinguish themselves by their ability to anticipate what might happen next . Even while doing their work , they are positioning themselves for the next task . Their transitions from one task to the next are smooth instead of abrupt . By forming sharp expectancies , experts can notice surprises more readily . They notice novel events and the absence of expected events . 9 As Weick and Sutcliffe ( 2007 , p . 45 ) observed , experts “ don’t necessarily see discrepancies any more quickly , but when they do spot discrepancies , they understand their meaning more fully and can deal with them more confidently . ” When we anticipate , we get ourselves ready for what may come next . We redirect our attention . We also can prepare for a few different possibilities . We don’t know for sure what the future will bring , but we aren’t going to stand idly by . And we can’t engage in anticipatory thinking by forcing ourselves to keep an open mind , waiting until we receive enough information before we begin to explore ways to react .

### Markierung (gelb) - 11   The Limits of Feedback > Seite 171 · Position 3068

Aside from the difficulty of giving feedback about tacit knowledge , instructors who give us rapid and accurate feedback can actually interfere with our learning . Schmidt and Wulf ( 1997 ) found that continuous , concurrent feedback does increase our learning curve while we are being trained but that it reduces any performance gains once the training is over . The reason is that we never have to learn how to get and interpret our own feedback as long as the instructor is doing all that work . The performance goes up nicely in the training environment , but then we are at a loss once we move into our work environment . We would be better off struggling to get our own feedback than having it spoon - fed to us by the instructors . The Schmidt - Wulf research involved motor skills rather than cognitive skills , but I suspect that there are times when aggressive feedback gets in the way of learning cognitive skills . Consider the way we would want someone to coach us to become a better cook . Rapid feedback from a coach ( e.g . , “ no , add more salt ” ) can interfere with our ability to develop a sense of taste that can dictate how to alter the seasoning . We need to learn for ourselves how to evaluate the saltiness of the food . The very idea of giving feedback suggests a passive learner . Learners are better off seeking their own feedback and asking for advice when they need it . 5

### Markierung (gelb) - 11   The Limits of Feedback > Seite 171 · Position 3083

Feedback applies more to some kinds of learning than to others . 6 It applies to learning about the actions we take .

### Markierung (gelb) - 11   The Limits of Feedback > Seite 172 · Position 3095

Sensemaking is at the heart of learning cognitive skills . We aren’t just acquiring new knowledge . We are changing the way we see things and think about them . We are making sense of conflicting and confusing data .

### Markierung (orange) - 11   The Limits of Feedback > Seite 173 · Position 3104

described in his novel Walden Two . The Walden Two society didn’t sound very inviting in 1948 , when the novel was published . It doesn’t sound any better now .

### Markierung (gelb) - 11   The Limits of Feedback > Seite 175 · Position 3142

Therefore , the replacement for claim 6 is “ We can’t just give feedback ; we have to find ways to make it understandable . ”

### Markierung (gelb) - 11   The Limits of Feedback > Seite 175 · Position 3150

In my book on intuition ( 2004 ) I suggested ways to make metrics more understandable and useful by blending them with stories . Story formats can help people make sense of feedback .

### Markierung (orange) - 11   The Limits of Feedback > Seite 176 · Position 3159

learner into account . 8 The learning relationship is just that — a relationship . Coaches have to diagnose what is going wrong ; they also have to find ways to get their message across .

### Markierung (gelb) - 12   Correcting the Dots > Seite 179 · Position 3191

ambiguous settings where you aren’t sure what to count as a dot . It isn’t that claim 7 is wrong . Like all the other claims , it tells only part of the story . It leaves out what counts as a dot , where the dots come from , how you know that you have finished , and how you suspect that a story is wrong .

### Markierung (gelb) - 12   Correcting the Dots > Seite 179 · Position 3196

Our expertise is as much about recognizing legitimate dots as about connecting them . Similarly , the metaphor of sensemaking as putting together the pieces of a puzzle is also misleading . When we assemble a puzzle , we have seen the box cover and know what we are trying to create . The job is much more difficult if we mix five puzzles together , and don’t see the cover picture for any of the boxes . Or if we try to solve a puzzle that doesn’t have a scene ( such as an all - black puzzle ) or a shape ( such as a puzzle with ragged edges instead of smooth ones ) . The metaphors of connecting the dots and assembling the puzzle don’t do justice to the need to notice and identify the cues in the first place .

### Markierung (gelb) - 12   Correcting the Dots > Seite 179 · Position 3207

In a 1919 book titled Fighting the Flying Circus , Eddie Rickenbacker , America’s World War I flying ace , described one of his early flights over Germany . When the group returned to their airport , the flight leader asked Rickenbacker what he had seen . Rickenbacker said it had gone very smoothly — he hadn’t seen any other airplanes except the ones in their formation . Rickenbacker had seen some German antiaircraft batteries and found it amusing to watch them waste their ammunition . The flight leader corrected him . A formation of five British Spads had passed under them just before they crossed into enemy lines and another formation of five Spads went by them soon after , neither more than 500 yards away . Plus four German Albatros airplanes two miles ahead of them when they turned back , and another German airplane , a two - seater , later on . Then the flight leader walked Ricken - backer over to his airplane and showed him the shrapnel holes from the German anti - aircraft fire that Rickenbacker found so amusing , including one piece of shrapnel that passed through both wings a foot from his body . Rickenbacker hadn’t realized that his plane had been hit . That was the beginning of his real education . The data were there — to the flight leader , but not to Rickenbacker . To survive , pilots had to learn quickly where to look and what to look for .

### Markierung (orange) - 12   Correcting the Dots > Seite 182 · Position 3253

see . They kept looking until they felt that they understood how the atmospheric forces were playing out . They knew which dots were relevant to the different possible stories . The data elements that we think are dots may turn out to be irrelevant . The next incident illustrates

### Markierung (gelb) - 12   Correcting the Dots > Seite 184 · Position 3286

Bill Duggan’s 2007 book Strategic Intuition is about the way people notice connections . For instance , Duggan explains how Bill Gates and Paul Allen made their initial discoveries . In high school they had programmed BASIC ( a simple computer language ) onto a PDP - 8 minicomputer . Later they noted that the new Intel 8082 chip had enough capacity to contain their BASIC program . Gates and Allen contacted the major computer companies to see if they would be interested in having a BASIC program for this new 8082 chip . None of them expressed any interest . Then , in December 1974 , as Allen was walking over to Gates’s dorm room at Harvard , he saw the cover of a issue of the magazine Popular Mechanics which featured the new Altair personal computer . The Altair , according to Popular Mechanics , would run on the 8082 chip and would cost only $ 397 . Instantly , all the connections fell into place . Gates and Allen contacted the manufacturer of the Altair , arranged a demonstration , and were soon on their way toward starting Microsoft . They had a new vision of a software company for microcomputers . Because of their backgrounds , the relevant dots jumped out at them , but not to others . The immediate dot was the cover of Popular Mechanics , which linked to their BASIC program and to their enthusiasm for the 8082 chip . Others looked at the magazine and thought that the price of personal computers was coming down . They didn’t know about the other dots . You can’t connect dots that you aren’t able to see . 5

### Markierung (gelb) - 12   Correcting the Dots > Seite 190 · Position 3398

Example 12.6 : Watching a baby develop an infection “ This baby was my primary ; I knew the baby and I knew how she normally acted . Generally she was very alert , was on feedings , and was off IVs . Her lab work on that particular morning looked very good . She was progressing extremely well and hadn’t had any of the setbacks that many other preemies have . She typically had numerous apnea \[ suspension of breathing \] episodes and then bradys \[ bradycardia episodes — spells of low heart rate \] , but we could easily stimulate her to end these episodes . At 2 : 30 , her mother came in to hold her and I noticed that she wasn’t as responsive to her mother as she normally was . She just lay there and half looked at her . When we lifted her arm it fell right back down in the bed and she had no resistance to being handled . This limpness was very unusual for her . “ On this day , the monitors were fine , her blood pressure was fine , and she was tolerating feedings all right . There was nothing to suggest that anything was wrong except that I knew the baby and I knew that she wasn’t acting normally . At about 3 : 30 her color started to change . Her skin was not its normal pink color and she had blue rings around her eyes . During the shift she seemed to get progressively grayer . “ Then at about 4 : 00 , when I was turning her feeding back on , I found that there was a large residual of food in her stomach . I thought maybe it was because her mother had been holding her and the feeding just hadn’t settled as well . “ By 5 : 00 I had a baby who was gray and had blue rings around her eyes . She was having more and more episodes of apnea and bradys ; normally she wouldn’t have any bradys when her mom was holding her . Still , her blood pressure hung in there . Her temperature was just a little bit cooler than normal . Her abdomen was a little more distended , up 2 cm from early in the morning , and there was more residual in her stomach . This was a baby who usually had no residual and all of a sudden she had 5 – 9 cc . We gave her suppositories thinking maybe she just needed to stool . Although having a stool reduced her girth , she still looked gray and was continuing to have more apnea and bradys . At this point , her blood gas wasn’t good so we hooked her back up to the oxygen . On the doctor’s orders , we repeated the lab work . The results confirmed that this baby had an infection , but we knew she was in trouble even before we got the lab work back . ”

### Markierung (gelb) - 12   Correcting the Dots > Seite 192 · Position 3435

These incidents illustrate how people question the frame despite data that appear convincing . Dave Malek , the nurse in the NICU example , and Lieutenant Colonel Petrov each made sense of events by rejecting the obvious story . In some ways their judgments weren’t any different from those of the officer at Pearl Harbor who dismissed the message of unknown airplanes spotted on radar . The mere act of explaining away data doesn’t merit our applause . I think the difference from the Pearl Harbor example is that Malek , the NICU nurse , and Petrov , as well as the intelligence officer who explained away the ominous airplanes that circled nuclear power plants after the 9 / 11 attack , all worried that they were wrong , and worried about the consequences of a mistake . They worried about the quality of the data .

### Markierung (gelb) - 12   Correcting the Dots > Seite 193 · Position 3451

The assembly - line model obscures the context in which data were collected . This loss of context may not matter to mediocre workers , but experts appreciate the implications of data collection methods . They know that each method has its own limits . Meteorologists know the limits of instruments for sampling wind direction and velocity , temperature , humidity , and so forth . They can take these limits into account . If you give weather forecasters a summary sheet , they can’t sort out where the forecast came from . The assembly - line model builds from data elements — dots — that have been collected . But what about events that didn’t happen ? These can also be informative . Experts appreciate the significance of these negative cues . But when they are given a set of recommendations and summaries , skilled decision makers don’t have any way to notice what didn’t happen .

### Markierung (gelb) - 12   Correcting the Dots > Seite 194 · Position 3468

The notion that data somehow appear doesn’t do justice to these kinds of active searches for meaningful cues . 9 Sensemaking isn’t just receiving data and inferences . It also involves knowing how to shake the system to find what you’re looking for .

### Notiz - 12   Correcting the Dots > Seite 194 · Position 3470

Ooda

### Markierung (gelb) - 12   Correcting the Dots > Seite 194 · Position 3471

Example 12.8 : Japan’s next target During World War II , the Japanese followed their attack on Pearl Harbor with additional victories , such as in the Battle of the Coral Sea . Yamamoto , their top naval commander , was obviously getting ready for yet another attack , but the Americans didn’t know where it might come . Perhaps Yamamoto would re - attack Pearl Harbor ; perhaps he would even attack California . As Japanese coded messages increased in volume , indicating that an attack was imminent , a US Navy cryptologist had a hunch . He was Captain Joseph Rochefort , an intelligence officer and a cryptologist who had also trained in the Japanese language . Rochefort was the officer in charge at Station HYPO , the cryptoanalysis unit in Pearl Harbor . He and others noticed that more and more of the Japanese messages they could partially decode used the phrase “ AF , ” which seemed to be the code for the next US target . But where was AF ? Rochefort studied a map of the Pacific and decided that if he was Yamamoto , getting ready to stage raids on Pearl Harbor or on the western United States , he would go after Midway Atoll . To test his theory , Rochefort arranged for the small unit on Midway to send a radio message describing a malfunction in their water - distilling plant . Two days later a Japanese cable said that AF was running low on drinking water and directed the AF force to bring additional water desalinization equipment . When Rochefort read this intercepted message , he knew Japan’s next target . Admiral Nimitz then reinforced Midway and sent three aircraft carriers to the island . Nimitz also knew the Japanese order of battle ; now that “ AF ” had been clarified all the previous messages about AF came into focus . Instead of an easy victory over a small garrison in an isolated island , the Japanese lost four of their six primary aircraft carriers and more than 250 airplanes . In one day at Midway they lost twice as many skilled pilots as their training programs produced in a year . The war in the Pacific theatre turned completely around . 10 Rochefort wasn’t waiting for the data to come to him . He wasn’t seeing his job as simply deciphering the Japanese messages . His job was to figure out what Yamamoto was planning .

### Markierung (gelb) - 12   Correcting the Dots > Seite 195 · Position 3489

Example 12.9 : Refusing to take any scrap In a factory that produced molds , the workers knew that excessive scrap pushed their expenses up and made them less competitive . The industry standard was about 6 percent , and , at their worst , this factory had a scrap rate of 14 percent . The workers hadn’t worried about their scrap rate until the owner lost the company to the bank and the workers took it over . They had to become profitable or they would lose their jobs . They succeeded in bringing their scrap rate down to 6 percent . The workers wondered if they could get the scrap rate even lower . They questioned what was behind this 6 percent figure and discovered that only a few of the molds accounted for most of the scrap . By redesigning these mold patterns , or by charging more for them , they could become more competitive for the rest of their products . They reduced their scrap rate dramatically , from 6 percent to 2.9 percent . And this reduction took place because they didn’t take the scrap rate as firm data ; instead they pushed further to investigate where that number came from . 11

### Markierung (gelb) - 12   Correcting the Dots > Seite 196 · Position 3508

In example 12.10 , the young sergeant was studying the terrain and reporting on what he saw . The general was speculating about what was out there , and was looking for things that he expected . We make sense of cues and data by fitting them into frames such as stories . An interesting cue , such as the increased use of “ AF ” in Japanese messages , may remind us of a frame — a preparation for the next attack . The reverse also happens . The frames we have learned guide our attention and shape what we notice . If we go for a walk in a park , a landscape architect is aware of the way the paths provide interesting views , a tree specialist is distinguishing the different species and how well each is growing , and a maintenance worker is getting a feel for whether heavy lawnmowing equipment might get stuck in the muddy ground . Same park , three different perspectives , three different experiences . 12 We make sense of cues and data by organizing them into frames such as stories , scripts , maps , and strategies . But the reverse also happens — our frames determine what we use as data . Both processes happen simultaneously , as shown in figure 12.3 . This reciprocal action between data and frames is the core of sensemaking . That’s the replacement for claim 7 . We make sense of data elements by fitting them into frames such as stories , but the reverse also happens — our frames determine what we use as data . Figure 12.3 The data / frame model of sensemaking ( the process of fitting data into a frame and fitting a frame around the data ) .

## Part III   Adapting

### Markierung (gelb) - 14   Moving Targets > Seite 212 · Position 3708

What happens when we don’t have clear goals ? This is the most serious limitation of claim 8 . Here we are facing what Rittel and Webber ( 1973 , 1984 ) described as “ wicked problems ” in which the goals are incomplete and keep changing , as well as occasionally conflicting . Solutions to wicked problems aren’t true or false . Instead , they are judged as good or bad because there is no way to test a solution to a wicked problem . Wicked problems epitomize the world of shadows . When we are faced with a wicked problem — when the goals just aren’t clear — there aren’t any objective ways to gauge success .

### Markierung (gelb) - 14   Moving Targets > Seite 213 · Position 3716

Claim 8 runs into trouble in complex situations in which the wicked problems prevent us from clarifying the goals at the start . Many of the problems we face are wicked problems in which we may have to reassess our original understanding of the goals . The goals will become clearer as we learn more . That’s why I am calling them emergent goals . Let’s look at some examples .

### Markierung (gelb) - 14   Moving Targets > Seite 215 · Position 3760

Bill Duggan , in Strategic Intuition ( 2007 ) , describes a number of other instances in which leaders changed their goals on the basis of what they learned . Napoleon went into battle without any firm plans . He maneuvered his forces until he found a match - up that suited him ; then he attacked . Bill Gates and Paul Allen , the founders of Microsoft , didn’t set out to start a software company . That came only after they failed with their initial ventures and realized the potential for standard software programs . Sergey Brin and Larry Page , the founders of Google , were just trying to find good dissertation topics in graduate school . They tried to sell a patent for their search algorithm for $ 1 million , but when that was turned down they decided to try to commercialize it themselves . Duggan argues that such breakthroughs depend on the ability to learn by making new connections , not on doggedly pursuing the original goal .

### Markierung (gelb) - 14   Moving Targets > Seite 221 · Position 3859

And Gantt charts discourage a team from modifying its goals . In the large project I was describing , once the Gantt charts were constructed and accepted , the customer approved funding for the next phase only if the milestone for the previous phase was reached . Bonuses were linked to the milestones . So no one wanted to move the milestones . And no one dared to think about changing any of the goals of the program . That’s another limitation of objectives - based tools such as Gantt charts : they contribute to goal fixation .

### Markierung (gelb) - 14   Moving Targets > Seite 223 · Position 3907

Jay Rothman and I have suggested an alternative approach , which we call Management by Discovery ( Klein and Rothman 2008 ) . In contrast to Management by Objectives , Management by Discovery ( MBD )

### Markierung (gelb) - 15   The Risks of Risk Management > Seite 235 · Position 4097

Authentic dissenters may be disliked even when they have helped the group do a better job .

### Notiz - 16   The Cognitive Wavelength > Seite 262 · Position 4578

Di rosso

- we can rest easy once we have completed the preparation. As the examples illustrate, common ground is never perfect and is always eroding.14 ([Location 4589](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4589))
    - **Tags:** #orange
- replacement for claim 10 is that all team members are responsible for continually monitoring common ground for breakdowns and repairing common ground when necessary. Once we have finished preparing the team we should still be on the lookout for breakdowns. We should continually ([Location 4599](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4599))
    - **Tags:** #orange
- In complex situations, the original plans, goals, and roles are likely to change, degrading the team’s common ground. Instead of trying to increase control, we should expect common ground to erode and then repair it on the fly. ([Location 4602](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4602))
- Studies of highly reliable organizations, such as power plants and aircraft carriers, show that good teams spot potential confusions and repair them.16 They also take advantage of slow times to re-calibrate common ground, because they know from experience that once they are in the middle of a crisis it may be too late.17 One of the clues that common ground is breaking down is when we say “How can they be so stupid?” in regard to some of the actions taken by our teammates. People usually aren’t stupid. If they seem stupid, then maybe we don’t understand what is going on. ([Location 4643](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4643))
- Complex settings, on the other hand, wouldn’t benefit from routine handoff scripts. Emily Patterson (2008) studied the handoff process in a hospital setting and found that the people on the shift that was ending started their handoff by going over the most important ([Location 4656](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4656))
    - **Tags:** #orange
- Example 17.1: Students and scientists Chinn and Brewer (1993) examined the way people reacted to anomalous data that contradicted some of their beliefs. They compared science students with actual scientists, and found that both used six common strategies to explain away discrepancies: •   They ignored the data. Scientists don’t pay attention to new claims for perpetual-motion machines or extra-sensory perception. •   They found something wrong that allowed them to reject the data. When Galileo, using telescopes, made findings that contradicted Aristotle, rival scientists argued about the reliability of the telescopes. •   They found a reason why the data didn’t really apply. It wasn’t clear if the phenomenon of Brownian motion (the random movement of particles suspended in a liquid or gas) was in the realm of biology, in the realm of chemistry, or in the realm of heat theory in physics, so scientists whose ideas were challenged by Brownian motion simply explained that one of the other fields would have to figure it out. •   They came up with a reason to hold the data in abeyance until some time in the future. When astronomers found that the orbit of Mercury was inconsistent with the Newtonian view, they just expected that someone eventually would reconcile the anomaly. •   They reinterpreted the data to make them less problematic. When a scientist speculated that mass extinctions in the Cretaceous era were caused by a meteor or comet, citing layers of iridium at a site in Italy, rivals argued that the iridium might have seeped down from layers of limestone above it. •   They made a small, peripheral change in their theories and models that seemed to handle the data without having to re-conceptualize anything. Galileo’s opponents believed that the moon and other heavenly bodies were perfect spheres. When Galileo persuaded one critic to look through a telescope and see mountains on the moon, the opponent countered that these mountains must be embedded in a transparent crystal sphere. When Copernicus suggested that Earth was rotating around the sun, astronomers who disagreed pointed out that the position of the stars stayed the same throughout Earth’s putative orbit around the sun each year. Surely during a six-month period when Earth would be moving from one side of the sun to the other, the stars should change their positions. Copernicus responded, not by giving up his ideas, but by making a peripheral change in his theory. He suggested that the stars must actually be very far away, and thus Earth’s orbit wouldn’t make much difference. This ad hoc explanation seemed feeble when Copernicus first voiced it, but we now know it was correct. So fixation isn’t always a weakness. It can help people mature their ideas. Physicians exhibit cognitive rigidity. Feltovich, Coulson, and Spiro (2001) showed that pediatric cardiologists had difficulty getting off the garden path.4 Once they formed an impression of what was wrong with a child, contrary evidence… ([Location 4702](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4702))
- flying at 1,000 feet or higher. At the very low altitudes they had to anticipate where they would have the time to perform tasks other than flying their aircraft. The new training concept dramatically reduced the number of crashes. The preceding example ([Location 4749](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4749))
    - **Tags:** #orange
- Example 17.3: Explaining the Monty Hall problem We are going to make three passes at the problem. The first pass uses an exercise that relies on experience, the second exercise relies on a question, and the third exercise uses a new frame. The experience Place three similar objects in front of you, as shown in figure 17.1. They can be coins, ashtrays, sticky notes, or whatever is handy. These objects represent the three doors on Let’s Make a Deal. (Please do the exercise instead of just reading about it. It makes a difference.) Figure 17.1 The Monty Hall Problem. We’re going to go through this two times. The first time, you will stick with your first choice and we’ll see what happens when the prize is behind each of the doors. The second time you will shift from your first choice, and again we’ll see what happens when the prize is behind each of the doors. To save time, throughout this exercise you are always going to pick door 1 on the left. (See figure 17.1.) It’s easier to explain this way rather than doing all the permutations. Let’s start the first set of three trials, where you stick with your initial choice, door 1. For the first trial, find another object, perhaps a small packet of sweetener, to represent the prize, and put it on door 1. Now, you picked door 1 and the prize is behind door 1. As a stand-in for Monty Hall, I open one of the other doors (it doesn’t matter which, because neither has a prize behind it), you stick with your original choice, and you win. Congratulations. In the second trial, the prize is behind door 2. Move the marker behind door 2. You pick door 1, I open door 3 (showing that there isn’t a prize behind it), you stick with door 1, and you lose. Too bad. In the third trial, the prize is behind door 3. Move the marker over there. You pick door 1, I open door 2 (no prize), you stick with door 1, and again you lose. So by sticking with your original choice you win one out of three times. Those are the odds you would expect. Now let’s go through the drill a second time. This time, you are always going to switch. In the first trial, the prize is behind door 1. Move the marker back. You pick door 1, I open one of the others. It still doesn’t matter, but suppose I open door 2. You switch. You aren’t going to switch to door 2, because I’ve already shown that the prize wasn’t there, so you switch to door 3. I open all the doors, and the prize is behind door 1, your original choice. You lose, and you feel like a chucklehead for switching. In the second trial, the prize is behind door 2. Move the marker over. You pick door 1. I open door 3, which doesn’t have a prize. You switch. Obviously you won’t switch to door 3, because I’ve already shown that the prize wasn’t there. So you switch to door 2 and you win. In the third trial, the prize is behind door 3. Move the marker over (this is the last time, if you’re getting tired). You pick door 1. I open door 2. You switch to door 3, and you win again. Thus, when you stuck with… ([Location 4774](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4774))
- How can we get people to abandon old mental models so they can grow into new ones? To overcome fixation, one thing we can do is spot when it is happening to us, or to others. To do that, we can ask this question: What evidence would it take to change your mind? (It is easier to spot fixation in others than in ourselves.) If someone can’t think of any evidence, that is a sign that he or she may be fixating. A second suggestion is to keep an eye on how much contradictory evidence we need to explain away in order to hold on to our beliefs. If we are mistaken about what is happening, the contradictions should mount and we should be explaining away more and more discrepancies. We will reach a point where we realize that we are wrong. The sooner we get to that point, the better. Third, we can look at some comparable cases to see what typically happens; if our estimates are much different we better have a good explanation.8 Fourth, we can bring in a person who doesn’t have much history with the issues at hand and who thus will have a fresh perspective. That’s what happened in the example of the UAVs in Kosovo (chapter 12). The officer who figured out that the staff members were watching their own team and not a gang came in after the staff had been working the problem. He wasn’t in the room when they got on the garden path or when they became increasingly committed to it. Doug Harrington was fortunate. If the LSO hadn’t visited him on the fateful night, he would have ended his aviation career in the Navy without understanding what went wrong. Instead of waiting for failures, as in Doug’s case, we can manufacture them, maybe using exercises and simulations that prepare learners to absorb feedback. That’s a fifth suggestion. We can use these exercises to create a conflict and show people that their old beliefs don’t work very well. Example 17.4: Going against the flow My colleagues Danyele Harris-Thompson, Dave Malek, and Sterling Wiggins once did a research project to train operators who control the movement of petroleum products in pipelines that can be hundreds of miles long. The operators had been trained on the various pumping stations along the route and on the ways to control each pump to keep the product flowing. Danyele, Dave, and Sterling found that the highly skilled controllers had developed a feel for the movement of product inside the pipeline. They had learned to “play” the pumping stations almost as one would play a musical instrument. Danyele, Dave, and Sterling built a set of decision exercises to present the operators with dilemmas. In one exercise, the technicians at one of the pumping stations had asked permission to shut their station down for 20 minutes for routine maintenance. This simple request, had the operators honored it immediately, would have reduced pressure down the line, triggering a chain reaction of pump shutdowns due to low pressure. In fact, the entire line was going to get shut down if that pump was turned off for… ([Location 4830](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4830))
- All these methods have some value in helping people shed outmoded mental models and learn new ways to think and to transform their mental models. Notice that each type of method for breaking free from fixation involves an activity. The pilots unlearn by actually flying differently in simulators or airplanes. The physics students construct a new understanding through various activities. Similarly, activities can help us understand the mental models of subordinates and teammates. We can watch them as they engage in activities. Many years ago, my colleagues and I prepared some decision-making games to help young Marine squad leaders get ready for an upcoming exercise. The company commander selected one of his three platoon leaders to run the squad leaders through these games. He left the other two platoon leaders free to work on other things. The platoon training officer assigned to run the games had a chance to watch how all the squad leaders thought about tough dilemmas. He observed the squad leaders in his own platoon as well as the ones from the other two platoons. Pretty soon the other two platoon leaders started showing up for the training. They noticed that these exercises were a rare opportunity to watch the thinking processes of their squad leaders. They could note the kinds of cues they noticed and the cues they missed, the kinds of actions they considered and the kinds they ignored. The decision exercises gave all the platoon leaders a unique opportunity to take stock of their squad leaders’ mental models. The storehouse metaphor isn’t wrong. It is useful for teaching ([Location 4891](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=4891))
- By ‘myth’ I mean “a belief given uncritical acceptance by the members of a group, especially in support of existing or traditional practices.”1 For example, many organizations ([Location 5001](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5001))
    - **Tags:** #orange
- processes to transform the initial conditions into the desired outcomes. “A hallmark of the process,” Norman and Kuras write, “is the ability to justify everything built in terms of the original requirements. If requirements change, it dislodges the careful scaffolding upon which the system rests. . . . The specific desired outcome must be known a priori, and it must ([Location 5045](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5045))
    - **Tags:** #orange
- Contrary to the popular view of experts as walking encyclopedias, we would do better to regard experts as detectors. They have spent many hours tuning themselves to notice cues that are invisible to the rest of us.1 They can make discriminations that most of us can’t make. Just as we need special devices to detect radon in our homes, or radioactivity levels near a nuclear power plant, or oxygen levels in a stream, experts pick up cues, patterns, and trends that otherwise go undetected. ([Location 5115](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5115))
- as much as possible. This control-oriented mindset is best suited for well-ordered situations. In ambiguous situations, in the world of shadows, we aren’t going to be able to control everything ([Location 5135](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5135))
    - **Tags:** #orange
- “People who are lost,” Syrotuck writes, “may experience different types of reactions. They may panic, become depressed, or suffer from ‘woods shock.’ Panic usually implies tearing around or thrashing through the brush, but in its earlier stages it is less frantic. Most lost people go through some of the stages. It all starts when they look about and find that a supposedly familiar location now appears strange, or when it seems to be taking longer to reach a particular place than they had expected. There is a tendency to hurry to ‘find the right place. . . . Maybe it’s just over that little ridge.’ If things get progressively more unfamiliar and mixed up, they may then develop a feeling of vertigo, the trees and slopes seem to be closing in and claustrophobia compels them to try to ‘break out.’ This is the point at which running or frantic scrambling may occur and indeed outright panic has occurred. Running is panic!” (p. 11) ([Location 5154](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5154))
- Now let’s take a different perspective on navigation, a recovery-oriented mindset rather than a follow-the-steps mindset. In traveling over complicated terrain, whether in the woods or in a city, we can assume that there is a reasonable chance that we’ll make some kind of error. Our recovery-oriented mindset is to look at a map to see where we are likely to get confused. How will we know that we have gotten off track? How can we find our way? Even if we don’t get lost, we may at least get bewildered. Think back to all the times you wondered if you had driven too far and wanted to turn around, only to come to the next landmark shortly thereafter. A map looks different when we adopt a recovery-oriented perspective. New features draw our attention—roads that will show when we have gone too far, streets that resemble the ones onto which we want to turn, confusion zones where we’re likely to go the wrong way. By reviewing a map this way, we may prevent some of the mistakes. But the point of the recovery-perspective exercise isn’t to eliminate mistakes. That’s the control mentality at work. Rather, the point is to assume that we will get disoriented. When disorientation happens, we want to understand the map to more easily recover from our mistakes. The notion of a recovery perspective grew out of a project I did in 1990 at Fort Campbell, in Kentucky, with Steve Wolf and Marvin Thordsen. We watched UH-60 Blackhawk helicopter teams in a high-fidelity simulator. Their mission for the exercise was to convey soldiers into drop zones deep in enemy territory. The helicopter pilots had to deliver the soldiers during a pre-determined time; artillery barrages would be suppressed during this time window so that the helicopters could safely land and then take off. The mission sounded simple when it was briefed to the teams. During their planning sessions, most of the teams concentrated on the checkpoints listed on their maps. They would start out flying north, following a road past the first checkpoint, which was where the road crossed a stream. The second checkpoint was at a T intersection where another road dead-ended into the road they were following. Then they would come to a range of hills and turn left. In all there were ten checkpoints on their way to the drop zone. The helicopter crews figured out the average flying speed they had to sustain. They even jotted down the time they would have to hit each of the checkpoints in order to get to the drop zone. It was like watching dispatchers file a schedule for a bus route. None of the missions followed the script. Only one of the ten crews made it to the drop zone during the period when artillery was suppressed. The others were usually too late but sometimes too early. All the helicopters were detected by enemy radar at some point and had missiles fired at them. The helicopters immediately went into evasive maneuvers and dropped below the tree lines to break the radar lock. Then they had to figure out where… ([Location 5179](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5179))
- Ford and Kraiger (1995) compiled a set of synonyms for mental models: knowledge structures, cognitive maps, and task schemata. ([Location 5313](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5313))
- Doyle and Ford 1998. ([Location 5315](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5315))
- Here are some of the major types of learning: Recognizing and applying analogies, reinforcing responses, classical conditioning to create associations, deduction, induction, imitation learning, episodic learning, and implicit learning. And here are some of the things we learn: Skills, connections (e.g., perceptual-motor), mental models, patterns, typicality, habituation, sensitization, categories, concepts, object recognition, language, metacognition (learning about oneself), instances (episodic learning), facts (declarative knowledge), attention management, spatial mapping, generalization and discrimination, tool learning, decentering (taking the perspective of someone else), emotional control, statistical and analytical methods, and sequencing of tasks. Thus, the term “learning” covers a lot of ground. People trying to create learning organizations might want to pin down what kind of learning they want to foster. ([Location 5501](https://readwise.io/to_kindle?action=open&asin=B08BT39W94&location=5501))
